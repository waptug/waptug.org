---
date: 2020-06-10T01:01:01-08:00
title: "GeekZoneBooks.Com Guide to Wireless History"
description: "A guide to the wireless industry"
featured_image: "/images/bookcover.jpg"
tags: ["Wireless Technology"]
---
{{< figure src="/images/bookcover.jpg" title="GeekZoneBooks.Com Book Cover" >}}
GeekZoneBooks.Com: Guide to Wireless Networking

© 2007 GeekZoneBooks.Com

GeekZoneBooks.Com

Guide to Wireless Networking

![](media/e48212757d00dcc50843908c795ec1f8.jpg)

*Travel through the history of computer networking and learn how computers and
cell phones send data over the air in a secure manner from the moon and back.*

Forward
=======

**Convergence ...** In the very near future all media will be converged on
mobile wireless hand held devices.

On-Demand Video & Audio, Live Streaming Video & Audio, Voice, Picture Messaging,
E-Mail and Internet Access, Games, GPS Mapping Applications, Digital Key Lock,
Bar Code Processing, Bio-Metric Id. Security, Personal Information Managers,
payment processing settlement services and other yet to be developed
applications.

The FCC has set a goal for broadband coverage of speeds over 400Kbs on all
wireless devices that will allow the 3G and 4G revolution to happen. *(Updated
Info: It took 12Mbps to make this happen)*

So if you’re looking at this on a wireless device you are holding in your hands
the kind of device that has connected you to the future of convergence.

At the end of convergence all the world’s knowledge will be able to be accessed
from your data phone communicator.

The satellites will carry the information to all parts of the world and the
headset will be able to communicate with any other device on the network. Towers
will no longer be needed to carry radio wave based communications.

Each device will able to carry the person’s entire history and important
documentation and only be accessible by that person because of biometric
identification methods.

They will be able to do real time language conversion on all voice
communications and textual information.

Brew — (the language of cell phone applications) or its future incarnations will
morph into a ubiquitous open standard that the platform of applications will be
built on. *(Updated Info: Android won the open source platform and iOS runs on
Apple IPhones)*

Data compression will mature so full motion video and audio receive and transmit
will be possible.

The user of the devices will always be at a disadvantage always because of the
amount of research they would have to do to understand the foundations of the
operation of the industry so Informatics companies such as WDS Global will
always have a niche value to the networks to provide support for end users.

When will convergence be complete? When the global satellite communication
system is in place and tested and accepted by the populace as economically
affordable for service and device acquisition.

The system must support the bandwidth to provide the room for IPV6 to have every
possible IP number available assigned to a device and allow full motion
transmit/receive in high-def digital audio/video formats.

When this system is designed and in place then the device industry will be able
to design the hand held units that will allow the end user to interface with the
network.

Perhaps the interim phase will be a hybrid cell tower/satellite/fiber system
with the satellites carrying the back haul bandwidth and using the cell towers
and fiber to the door to distribute the communications to handset units.

This is what I see and what I prognosticate. Where will you be and what part
will you play in this inevitability? Be a part of history, don't just read about
it.

Michael Scott McGinn

March 21, 2007

Introduction: The history of networking
=======================================

Because networking is such a broad and complex subject, no single event
represents its point of origin. However, we can arbitrarily decide on the 1960’s
as the formative period for the field, since that is when the computer began to
affect significantly the lives of ordinary individuals and the day-to-day
operation of both business and government.

1960’s
------

In the 1960s computer networking was essentially synonymous with mainframe
computing, and the distinction between local and wide area networks did not yet
exist. Mainframes were typically "networked" to a series of dumb terminals with
serial connections running on RS—232 or some other electrical interface. If a
terminal in one city needed to connect with a mainframe in another city, a 300-
baud long—haul modem would use the existing analog Public Switched Telephone
Network (PSTN) to form the connection. The technology was primitive indeed, but
it was an exciting time nevertheless.

To continue the story, the quality and reliability of the PSTN increased
signiﬁcantly in 1962 with the introduction of pulse code modulation (PCM), which
converted analog voice signals into digital sequences of bits. A consequent
development was the ﬁrst commercial touch-tone phone, which was introduced in
1962. Before long, digital phone technology became the norm, and DS-O (Digital
Signal Zero) was chosen as the basic 64-kilobit—per-second (Kbps) channel upon
which the entire hierarchy of the digital telephone system was built. A later
development was a device called a channel bank, which took 24 separate DS-O
channels and combined them together using time-division multiplexing (TDM) into
a single 1.544—Mbps channel called DS-1 or T1. (In Europe, 30 DS-O channels were
combined to make E1.) When the backbone of the Bell telephone system finally
became fully digital years later, the transmission characteristics improved
signiﬁcantly for both voice and data transmission due to higher quality and less
noise associated with Integrated Services Digital Network (ISDN) digital lines,
though local loops have remained analog in many places. But that is getting a
little ahead of the story.

The first communication satellite, Telstar, was launched in 1962. This
technology did not immediately affect the networking world because of the high
latency of satellite links compared to undersea cable communications, but it
eventually surpassed transoceanic underwater telephone cables (which were first
deployed in 1965 and could carry 130 simultaneous conversations) in carrying
capacity. In fact, early in 1960 scientists at Bell Laboratories transmitted a
communication signal coast-to-coast across the United States by bouncing it off
the moon! By 1965 popular commercial communication satellites such as Early Bird
were being widely deployed and used.

The year 1969 witnessed an event whose full significance was not realized until
more than two decades later: namely, the development of the ARPANET
packet-switching network. ARPANET was a project of the US. Department of
Defense's Advanced Research Projects Agency (ARPA), which became DARPA in 1972.
Similar efforts were underway in France and the United Kingdom, but it was the
US project that eventually evolved into the present-day Internet. (France's
MlNTEL packet-switching system, which was based on the X25 protocol and which
aimed to bring data networking into every home, did take off in 1984 when the
French government started giving away MlNTEL terminals; by the early 1990s, more
than 20 percent of the country's population was using it.) The original ARPANET
network connected computers at Stanford University, the University of California
at Los Angeles (UCLA), the University of California at Santa Barbara (UCSB), and
the University of Utah, with the first node being installed at UCLA's Network
Measurement Center. A year later, Harvard University, the Massachusetts
Institute of Technology (MIT), and a few other prominent institutions were added
to the network, but few of those involved could imagine that this technical
experiment would someday profoundly affect modern society and the way we do
business.

That same year, Bell Laboratories developed the UNIX operating system, a
multitasking, multi-user network operating system (NOS) that became popular in
academic computing environments in the 1970s. A typical UNIX system in 1974 was
a PDP—11 minicomputer with dumb terminals attached. In a configuration with 768
kilobytes (KB) of magnetic core memory and a couple of ZOO-megabyte (MB) hard
disks, the cost of such a system would have been around \$40,000. Many important
standards for computer systems also evolved during the 1960s. In 1962, IBM
introduced the first 8-bit character encoding system, called Extended
Binary-Coded Decimal Interchange Code (EBCDIC). A year later the competing
American Standard Code for Information Interchange (ASCII) was introduced. ASCII
ultimately won out over EBCDIC even though EBCDIC was 8-bit and ASCII was only
7—bit. The American National Standards Institute (ANSI) formally standardized
ASCII in 1968. ASCII was first used in serial transmission between mainframe
hosts and dumb terminals in mainframe computing environments, but it was
eventually extended to all areas of computer and networking technologies.

Other developments in the 1960s included the development in 1964 of IBM's
powerful System/360 mainframe computing environment, which was widely
implemented in government, university, and corporate computing centers. In 1966,
IBM introduced the first disk storage system, which employed 50 metal platters,
each of which was 2 feet (0.6 meter) wide and had a storage capacity of 5 MB.
IBM created the ﬁrst ﬂoppy disk in 1967. In 1969, Intel Corporation released a
RAM chip that stored 1 KB of information, which at the time was an amazing feat
of engineering.

1970’s
------

Although the 1960’s were the decade of the mainframe, the 19703 gave birth to
Ethernet, which today is by far the most popular LAN technology. Ethernet was
born in 1973 in Xerox Corporation's research lab in Palo Alto, California. (An
earlier experimental network called ALOHAnet was developed in 1970 at the
University of Hawaii.) The original Xerox networking system was known as X-wire
and worked at 2.94 Mbps. X—wire was experimental and was not used commercially,
although a number of Xerox Palo Alto workstations used for word processing were
networked together in the White House using X-wire during the Carter
administration. In 1979, Digital Equipment Corporation (DEC), Intel, and Xerox
formed the DIX consortium and developed the specification for standard 10-Mbps
Ethernet, or thicknet, which was published in 1980. This standard was revised
and additional features were added in the following decade.

The conversion of the backbone of the Bell telephone system to digital circuitry
continued during the 1970s and included the deployment in 1974 of the first
digital data service (DDS) circuits (then called the Dataphone Digital Service).
DDS formed the basis of the later deployment of ISDN and T1 lines to customer
premises, and AT&T installed its first digital switch in 1976.

In wide area networking, a new telecommunications service called X.25 was
deployed toward the end of the decade. This new system was packet-switched, in
contrast to the circuit-switched PSTN, and later evolved into public X25
networks such as GTE's Telenet Public Packet Distribution Network (PDN), which
later became SprintNet. X.25 was widely deployed in Europe, where it still
maintains a large installed base, especially for communications in the banking
and ﬁnancial industry.

In 1970 the Federal Communications Commission (FCC) announced the regulation of
the ﬂedgling cable television industry. Cable TV remained primarily a broadcast
technology for delivering entertainment to residential homes until the
mid-1990s, when technologies began to be developed to enable it to carry
broadband services to residential subscribers. Cable modems now compete strongly
with Digital Subscriber Line (DSL) as the main two forms of broadband Internet
access technologies.

Despite all these technological advances, however, telecommunications services
in the 1970s remained unintegrated, with voice, data, and entertainment carried
on different media. Voice was carried by telephone, which was still analog at
the customer premises; entertainment was broadcast using radio and television
technologies; and data was usually carried over RS-232 or Binary Synchronous
Communication (BSC) serial connections between dumb terminals and mainframes
(or, for remote terminals, long-haul modem connections over analog telephone
lines).

The 1970s were also notable for the growth of ARPANET, which grew throughout the
decade as additional hosts were added at various universities and government
institutions. By 1971 the network had 19 nodes, mostly consisting of a mix of
PDP-8, PDP-11, IBM 8/360, DEC-10, Honeywell, and other mainframe and
minicomputer systems linked together. The initial design of ARPANET called for a
maximum of 265 nodes, which seemed like a distant target in the early 1970s. The
initial protocol used on this network was NCP, but this was replaced in 1982 by
the more powerful TCP/IP protocol suite. In 1975 the administration of ARPANET
came under the authority of the Defense Communications Agency.

ARPANET protocols and technologies continued to evolve using the informal RFC
process developed in 1969. In 1972 the Telnet protocol was defined in RFC 318,
followed by FTP in 1973 (RFC 454). ARPANET became an international network in
1973 when nodes were added at the University College of London in the United
Kingdom and at the Royal Radar Establishment in Norway. ARPANET even established
an experimental wireless packet-switching radio service in 1977, which two years
later became the Packet Radio Network (PRN ET).

Meanwhile, in 1974 the first specification for the Transmission Control Protocol
(TCP) was published. Progress on the TCP/IP protocols continued through several
iterations until the basic TCP/lP architecture was formalized in 1978, but it
was not until 1983 that ARPANET started using TCP/IP instead of NCP as its
primary networking protocol.

The year 1977 also saw the development of UNIX to UNIX Copy (UUCP), a protocol
and tool for sending messages and transferring files on UNIX-based networks. An
early version of the USENET news system using UUCP was developed in 1979. (The
Network News Transfer Protocol [NNTP] came much later, in 1987.)

In 1979 the first commercial cellular phone system began operation in Japan.
This system was analog in nature, used the 800-MHz and 900-MHz frequency bands,
and was based on a concept developed in 1947 at Bell Laboratories.

An important standard to emerge in the 1970s was the public—key cryptography
scheme developed in 1976 by Whitﬁeld Diffie and Martin Hellman. This scheme
underlies the Secure Sockets Layer (SSL) protocol developed by Netscape
Communications, which is still the predominant approach for ensuring privacy and
integrity of financial and other transactions over the World Wide Web (WWW).
Without SSL, popular e-business sites such as Amazon and eBay would have a hard
time attracting customers!

Among other miscellaneous developments during this decade, in 1970 IBM
researchers invented the relational database, a set of conceptual technologies
that has become the foundation of today's distributed application environments.
In 1971, IBM demonstrated the first speech recognition technologies--which have
since led to those annoying automated call handling systems found in customer
service centers! IBM also developed the concept of the virtual machine in 1972
and created the first sealed disk drive (the Winchester) in 1973. In 1974, IBM
introduced the Systems Networking Architecture (SNA) for networking its
mainframe computing environment. In 1971, Intel released its first
microprocessor, a 4-bit processor called the 4004 that ran at a clock speed of
108 kilohertz (kHz), a snail's pace by modern standards but a major development
at the time. Another significant event was the launching of the online service
CompuServe in 1979, which led to the development of the first online
communities.

The first personal computer, the Altair, went on the market as a kit in 1975.
The Altair was based on the Intel 8080, an 8-bit processor, and came with 256
bytes of memory, toggle switches, and light-emitting diode (LED) lights.
Although the Altair was basically for hobbyists, the Apple II from Apple
Computer, which was introduced in 1977, was much more. A typical Apple II
system, which was based on the Motorola 6502 8-bit processor, had 4 KB of RAM, a
keyboard, a motherboard with expansion slots, built-in BASIC in ROM, and color
graphics. The Apple II quickly became the standard desktop system in schools and
other educational institutions. However, it was not until the introduction of
the IBM Personal Computer (PC) in 1981 that the full potential of personal
computers began to be realized, especially in businesses.

In 1975, Bill Gates and Paul Allen licensed their BASIC computer programming
language to MITS, the Altair's manufacturer. BASIC was the first computer
language specifically written for a personal computer. Gates and Allen coined
the name "Microsoft" for their business partnership, and they officially
registered it as a trademark the following year. Microsoft Corporation went on
to license BASIC to other personal computing platforms such as the Commodore PET
and the TRS-80.

1980’s
------

In the 1980s the growth of client/server LAN architectures continued while that
of mainframe computing environments declined. The advent of the IBM PC in 1981
and the standardization and cloning of this architecture led to an explosion of
PC-based LANs in businesses and corporations around the world, particularly with
the release of the IBM PC AT hardware platform in 1984. The number of PCs in use
grew from 2 million in 1981 to 65 million in 1991. Novell, which appeared on the
scene in 1983, soon became a major player in file and print servers for LANs
with its Novell NetWare platform.

However, the biggest development in the area of LAN networking in the 1980s was
the continued evolution and standardization of Ethernet. While the DIX
consortium worked on Ethernet standards in the late 1970s, the lEEE with its
Project 802 initiative tried working toward a single unified LAN standard. When
it became clear that this goal was impossible, Project 802 was divided into a
number of separate working groups, with 802.3 focusing on Ethernet, 802.4 on
Token Bus, and 802.5 on Token Ring technologies and standards. The work of the
802.3 group resulted in the first Ethernet standard, called 10Base5 or thicknet,
which was almost identical to the version developed by DIX. 10Base5 was called
thicknet because it used thick coaxial cable, and in 1985 the 802.3 standard was
extended to include 10Base2 using thin coaxial cable, commonly called thinnet.

Through most of the 1980s, coaxial cable was the main form of cabling used for
implementing Ethernet. A company called SynOptics Communications, however,
developed a product called LattisNet that was designed for transmitting 10-Mbps
Ethernet over twisted—pair wiring using a star-wired topology that was connected
to a central hub or repeater. This wiring was cheaper than coaxial cable and was
similar to the wiring used in residential and business telephone wiring systems.
LattisNet was such a commercial success that in 1990 the 802.3 committee
approved a new standard called 10BaseT for Ethernet that ran over twisted-pair
wiring. 10BaseT soon superseded the coaxial forms of Ethernet because of its
ease of installation and because its hierarchical star-wired topology was a good
match for the architectural topology of multistory buildings.

In other Ethernet developments, fiber-optic cabling, first developed in the
early 1970s by Corning, found its first commercial networking application in
Ethernet networking in 1984. (The technology itself was standardized as 1OBaseFL
in the early 1990s.) In 1988 the first fiber—optic transatlantic undersea cable
was laid and greatly increased the capacity of transatlantic communication
systems.

Ethernet bridges became available in 1984 from DEC and were used both to connect
separate Ethernet LANs to make large networks and to reduce trafﬁc bottlenecks
on overloaded networks by splitting them into separate segments. Routers could
be used for similar purposes, but bridges generally offered better price and
performance, as well as less complexity, during the 1980s. Again, market
developments preceded standards, as the IEEE 802.1D Bridge Standard, which was
initiated in 1987, was not standardized until 1990.

In the UNIX arena, the development of the Network File System (NFS) by Sun
Microsystems in 1985 resulted in a proliferation of diskless UNIX workstations
having built—in Ethernet interfaces. This development helped drive the demand
for Ethernet and accelerated the evolution of Ethernet bridging technologies
into today's switched networks. By 1985 the rapidly increasing numbers of UNIX
hosts and LANs connected to the ARPANET began to transform it from what had been
mainly a network of mainframe and minicomputer systems into something

like what it is today. The first UNIX implementation of TCP/IP came in v4.2 of
Berkeley's BSD UNIX, from which other vendors such as Sun Microsystems quickly
ported their versions of TCP/IP. Although PC-based LANs rapidly grew in
popularity in business and corporate settings during the 1980s, UNIX continued
to dominate in academic and professional high-end computing environments as the
mainframe environment declined.

IBM introduced its Token Ring networking technology in 1985 as an alternative
LAN technology to Ethernet. IBM had submitted its technology to the IEEE in 1982
and the 802.5 committee standardized it in 1984. IBM soon supported the
integration of Token Ring with its existing SNA networking services and
protocols for IBM mainframe computing environments. The initial Token Ring
specifications delivered data at 1 Mbps and 4 Mbps, but it dropped the 1-Mbps
version in 1989 when it introduced a newer 16-Mbps version. Interestingly, no
formal IEEE specification exists for 16—Mbps Token Ring--vendors simply adopted
IBM's technology for the product. Efforts were made to develop high-speed Token
Ring, but these have finally been abandoned and today Ethernet reigns supreme.

Also in the field of local area networking, in 1982 the American National
Standards Institute (ANSI) began standardizing the specifications for Fiber
Distributed Data Interface (FDDI). FDDI was designed to be a high-speed (100
Mbps) fiber-optic networking technology for LAN backbones on campuses and
industrial parks. The final FDDI specification was completed in 1988, and
deployment in campus LAN backbones grew during the late 1980s and the early

1990s. But today FDDI is considered legacy technology and has been superseded in
most places by Fast Ethernet and Gigabit Ethernet (GbE).

In 1983 the ISO developed an abstract seven-layer model for networking called
the Open Systems Interconnection (OSI) reference model. Although some commercial
networking products were developed based on OSI protocols, the standard never
really took off, primarily because of the predominance of TCP/IP. Other
standards from the ISO and ITU that emerged in the 1980s included the X.400
electronic messaging standards and the X500 directory recommendations, both of
which held sway for a while but have now largely been superseded X.4OO by the
Internet's Simple Mail Transfer Protocol (SMTP) and X.5OO by Lightweight
Directory Access Protocol (LDAP).

A major event in the telecommunications/WAN field in 1984 was the divestiture of
AT&T as the result of the seven-year antitrust suit brought against AT&T by the
US. Justice Department. AT&T's 22 Bell operating companies were formed into 7
new RBOCs (only 4 are left today). This meant the end of the old Bell telephone
system, but these RBOCs soon formed the Bellcore telecommunications research
establishment to replace the defunct Bell

Laboratories. The United States was then divided into Local Access and Transport
Areas (LATAs), with intra-LATA communication handled by local exchange carriers
(the Bell Operating Companies or B003) and inter-LATA communication handled by
interexchange carriers (IXCs) such as AT&T, MCI, and Sprint Corporation.

The result of the breakup was increased competition, which led to new WAN
technologies and generally lower costs. One of the first effects was the
offering of T1 services to subscribers in 1984. Until then, this technology had
been used only for backbone circuits for long-distance communication. New
hardware devices were offered to take advantage of the increased bandwidth,
especially high-speed T1 multiplexers, or muxes, that could combine voice and
data in a single communication stream. The year 1984 also saw the development of
digital Private Branch Exchange (PBX) systems by AT&T, bringing new levels of
power and flexibility to corporate subscribers.

The Signaling System \#7 (SS7) digital signaling system was deployed within the
PSTN in the 1980s, first in Sweden and later in the United States. 887 made new
telephony services available to subscribers, such as caller ID, call blocking,
and automatic callback.

The first trials of ISBN, a fully digital telephony technology that runs on
existing copper local loop lines, began in Japan in 1983 and in the United
States in 1987. All major metropolitan areas in the United States have since
been upgraded to make ISDN available to those who want it, but ISDN has not
caught on in the United States as a WAN technology as much as it has in Europe.

The 1980s also saw the standardization of SONET technology, a high-speed
physical layer (PHY) fiber-optic networking technology developed from time—
division multiplexing (TDM) digital telephone system technologies. Before the
divestiture of AT&T in 1984, local telephone companies had to interface their
own TDM-based digital telephone systems with proprietary TDM schemes of
long-distance carriers, and incompatibilities created many problems. This
provided the impetus for creating the SONET standard, which was finalized in
1989 through a

series of Comité Consultatif International Télégraphique et Téléphonique (CCITT;
anglicized as International Telegraph and Telephone Consultative Committee)
standards known as G.707, G.608, and G.709. By the mid-1990s almost all
long-distance telephone traffic in the United States used SONET on trunk lines
as the physical interface.

The 1980s brought the first test implementations of Asynchronous Transfer Mode
(ATM) high-speed cell-switching technologies, which could use SONET as the
physical interface. Many concepts basic to ATM were developed in the early 1980s
at the France-Telecom laboratory in Lannion, France, particularly the PRELUDE
project, which demonstrated the feasibility of end-to-end ATM networks running
at 62 Mbps. The CCiTT standardized the 53—byte ATM cell format in 1988, and the
new technology was given a further push with the

creation of the ATM Forum in 1991. Since then, use of ATM has grown
significantly in telecommunications provider networks and has become a
high-speed backbone technology in many enterprise-level networks around the
world. However, the vision of ATM on users' desktops has not been realized
because of the emergence of cheaper Fast Ethernet and GbE LAN technologies and
because of the complexity of ATM itself.

The convergence of voice, data, and broadcast information remained a distant
vision throughout the 1980s and was even set back because of the proliferation
of networking technologies, the competition between cable and broadcast
television, and the slow adoption of residential ISDN. New services did appear,
however, especially in the area of commercial online services such as America
Online (AOL), CompuServe, and Prodigy, which offered consumers e-mail, bulletin
board systems (8883), and other services.

A significant milestone in the development of the Internet occurred in 1982 when
the networking protocol of ARPANET was switched from NCP to TCP/IP. On January
1, 1983, NCP was turned off permanently——anyone who had not migrated to TCP/lP
was out of luck. ARPANET, which connected several hundred systems, was split
into two parts, ARPANET and MILNET.

The first international use of TCP/lP took place in 1984 at the Conseil Européen
pour la Recherche Nucléaire (CERN), a physics research center located in Geneva,
Switzerland. TCP/lP was designed to provide a way of networking different
computing architectures in heterogeneous networking environments. Such a
protocol was badly needed because of the proliferation of vendor—specific
networking architectures in the preceding decade, including "homegrown"
solutions developed at many government and educational institutions. TCP/IP

made it possible to connect diverse architectures such as UNIX workstations, VMS
minicomputers, and Cray supercomputers into a single operational network. TCP/lP
soon superseded proprietary protocols such as Xerox Network Systems (XNS),
ChaosNet, and DECnet. It has since become the de facto standard for
internetworking all types of computing systems.

CERN was primarily a research center for high-energy particle physics, but it
became an early European pioneer of TCP/IP and by 1990 was the largest
subnetwork of the Internet in Europe. In 1989 a CERN researcher named Tim
Berners-Lee developed the Hypertext Transfer Protocol (HTTP) that formed the
basis of the World Wide Web (WWW). And all of this developed as a sidebar to the
real research that was being done at CERN--slamming together protons and
electrons at high speeds to see what fragments would appear!

Also important to the development of Internet technologies and protocols was the
introduction of the Domain Name System (DNS) in 1984. At that time, ARPANET had
more than 1000 nodes and trying to remember their numerical lP addresses was a
headache. DNS greatly simplified that process. Two other internet protocols were
introduced soon afterwards: NNTP was developed in 1987, and Internet Relay Chat
(lRC) was developed in 1988.

Other systems paralleling ARPANET were developed in the early 1980s, including
the research—oriented Computer Science NETwork (CSNET), and the Because It's
Time NETwork (BITNET), which connected IBM mainframe computers throughout the
educational community and provide email services. Gateways were set up in 1983
to connect CSNET to ARPANET, and BITNET was similarly connected to ARPANET. In
1989, BITNET and CSNET merged into the Corporation for Research and Educational
Networking (CREN).

In 1986 the National Science Foundation NETwork (NSFNET) was created. NSFNET
networked the five national supercomputing centers together using dedicated
56-Kbps lines. The connection was soon seen as inadequate and was upgraded to
1.544-Mbps T1 lines in 1988. In 1987, NSF and Merit Networks agreed to jointly
manage the NSFNET, which had effectively become the backbone of the emerging
Internet. By 1989 the Internet had grown to more than 100,000 hosts, and the
Internet Engineering Task Force (lETF) was officially created to administer its
development. In 1990, NSFNET officially replaced the aging ARPANET and the
modern Internet was born, with more than 20 countries connected.

Cisco Systems was one of the first companies in the 1980s to develop and market
routers for Internet Protocol (IP) internetworks, a business that today is worth
billions of dollars and is a foundation of the Internet. Hewlett-Packard was
Cisco's first customer for its routers, which were originally called gateways.

In wireless telecommunications, analog cellular was implemented in Norway and
Sweden in 1981. Systems were soon rolled out in France, Germany, and the United
Kingdom. The first US. commercial cellular phone system, which was named the
Advanced Mobile Phone Service (AMPS) and operated in the 800- MHz frequency
band, was introduced in 1983. By 1987 the United States had more than 1 million
AMPS cellular subscribers, and higher-capacity digital

cellular phone technologies were being developed. The Telecommunications
Industry Association (TIA) soon developed specifications and standards for
digital cellular communication technologies.

A landmark event that was largely responsible for the phenomenal growth in the
PC industry (and hence the growth of the client/server model and local area
networking) was the release of the first version of Microsoft's text-based,
16-bit MS-DOS operating system in 1981. Microsoft, which had become a privately
held corporation with Bill Gates as president and chairman of the board and Paul
Allen as executive vice president, licensed MS—DOS version 1 to IBM for its PC.
MS-DOS continued to evolve and grow in power and usability until its final
version, MS-DOS 6.22, which was released in 1993. One year after the first
version of MS- DOS was released in 1981, Microsoft had its own fully functional
corporate network, the Microsoft Local Area Network (MILAN), which linked a DEC
206, two PDP-11/7OS, a VAX 11/250, and a number of MC68000 machines running
XENIX. This type of setup was typical of the heterogeneous computer networks
that characterized the early 1980s.

In 1983, Microsoft unveiled its strategy to develop a new operating system
called Windows with a graphical user interface (GUI). Version 1 of Windows,
which shipped in 1985, used a system of tiled windows that allowed users to work
with several applications simultaneously by switching between them. Version 2
was released in 1987 and supported overlapping windows and support for expanded
memory.

Microsoft launched Its SQL Server relational database server software for LANs
in 1988. In its current version, SQL Server 2000 is an enterprise-class
application that competes with other major database platforms such as Oracle and
DB2. IBM and Microsoft jointly released their 32-bit OS/2 operating system in
1987 and released 08/2 1.1 with Presentation Manager a year later.

In miscellaneous developments, IBM researchers developed the Reduced Instruction
Set Computing (RISC) processor architecture in 1980. Apple Computer introduced
its Macintosh computing platform in 1984 (the successor of its Lisa system),
which introduced a windows-based GUI that was the precursor to Windows. Apple
also introduced the 3.5-inch ﬂoppy disk in 1984. Sony Corporation and Philips
developed CD-ROM technology in 1985. (Recordable CD-R technologies were
developed in 1991.) IBM released its AS/400 midrange computing system in 1988,
which continues to be popular to this day.

1990’s
------

The 1990s were an explosive decade in every aspect of networking, and we can
only touch on a few highlights here. Ethernet continued to evolve as a LAN
technology and began to eclipse competing technologies such as Token Ring and
FDDI. In 1991, Kalpana Corporation began marketing a new form of bridge called a
LAN switch, which dedicated the entire bandwidth of a LAN to a single port
instead of sharing it among several ports. Later known as Ethernet switches or
Layer 2 switches, these devices quickly found a niche in providing dedicated

high-throughput links for connecting servers to network backbones. Layer 3
switches soon followed, eventually displacing traditional routers in most areas
of enterprise networking except for WAN access. Layer 4 and higher switches are
now popular in server farms for load balancing and fault tolerance purposes.

The rapid evolution of the PC computing platform and the rise of bandwidth-
hungry applications created a need for something faster than 10-Mbps Ethernet,
especially on network backbones. The first full-duplex Ethernet products,
offering speeds of 20 Mbps, became available in 1992. In 1995 work began on a
standard for full—duplex Ethernet; it was finalized in 1997. A more important
development was Grand Junction Networks' commercial Ethernet bus,

introduced in 1992, which functioned at 100 Mbps. Spurred by this advance, the
802.3 group produced the 802.3u 100BaseT Fast Ethernet standard for transmission
of data at 100 Mbps over both twisted-pair copper wiring and ﬁber—optic cabling.

Although the jump from 10-Mbps to 100-Mbps Ethernet took almost 15 years, a year
after the 100BaseT Fast Ethernet standard was released, work began on a
1000-Mbps version of Ethernet popularly known as Gigabit Ethernet (GbE). Fast
Ethernet was beginning to be deployed at the desktop, and this was putting
enormous strain on the FDDI backbones that were deployed on many commercial and
university campuses. FDDI also operated at 100 Mbps (or 200 Mbps if fault
tolerance was discarded in favor of carrying traffic on the redundant ring), so
a single Fast Ethernet desktop connection could theoretically saturate the
capacity of the entire network backbone. Asynchronous Transfer Mode (ATM), a
broadband cell-switching technology used primarily in telecommunication/WAN
environments, was briefly considered as a possible successor to FDDI for
backboning Ethernet networks together, and LAN emulation (LANE) was developed to
carry LAN traffic such as Ethernet over ATM. However, ATM is much more complex
than Ethernet, and a number of companies saw extending Ethernet speeds to 1000
Mbps as a way to provide network backbones with much greater capacity using
technology that most network administrators were already familiar with. As a
result, the 802 group called 802.32 developed a GbE standard called 1000BaseX,
which it released in

1998. Today GbE is the norm for LAN backbones, and Fast Ethernet is becoming
ubiquitous at the desktop level. Work is even unden/vay on extending Ethernet
technologies to 10 gigabits per second (Gbps). A competitor of GbE for high-
speed collapsed backbone interconnects, called Fibre Channel, was conceived by
an ANSI committee in 1988 but is used mainly for storage area networks (SANs).

The 1990s saw huge changes in the landscape of telecommunications providers and
their services. "Convergence" became a major buzzword, signifying the combining
of voice, data, and broadcast information into a single medium for delivery to
businesses and consumers through broadband technologies such as metropolitan
Ethernet, Digital Subscriber Line (DSL), and cable modern systems. The cable
modem was introduced in 1996, and by the end of the decade broadband residential
Internet access through cable television systems had become a strong competitor
with telephone-based systems such as Asymmetric Digital Subscriber Line (ADSL)
and G.Lite, another variant of DSL.

Also in the 1990s, Voice over IP (VoiP) emerged as the latest "Holy Grail" of
networking and communications and promised businesses huge savings by routing
voice telephone traffic over existing IP networks. VoiP technology works, but
the bugs are still being ironed out and deployments remain slow. Recent
developments in VoiP standards, however, may help propel deployment of this
technology in coming years.

The first public frame relay packet-switching services were offered in North
America in 1992. Companies such as AT&T and Sprint installed a network of frame
relay nodes across the United States in major cities, where corporate networks
could connect to the service through their local telco. Frame relay began to eat
signiﬁcantly into the deployed base of more expensive dedicated leased lines
such as the T1 or E1 lines that businesses used for their WAN solutions,
resulting in lower prices for these leased lines and greater flexibility of
services.

In Europe frame relay has been deployed much more slowly, primarily because of
the widespread deployment of packet—switching networks such as X.25.

The Telecommunications Act of 1996 was designed to spur competition in all
aspects of the US. telecommunications market by allowing the RBOCs access to
long-distance services and lXCs access to the local loop. The result has been an
explosion in technologies and services offered by new companies called
competitive local exchange carriers (CLECs), with mergers and acquisitions
changing the nature of the service provider landscape almost daily.

The 1990s saw a veritable explosion in the growth of the internet and the
development of Internet technologies. As mentioned earlier, ARPANET was replaced
in 1990 by NSFNET, which by then was commonly called the internet. At the
beginning of the 1990s, the internet's backbone consisted of 1.544-Mbps T1 lines
connecting various institutions, but in 1991 the process of upgrading these
lines to 44.735-Mbps T3 circuits began. By the time the Internet Society (ISOC)
was chartered in 1992, the Internet had grown to an amazing 1 million hosts on
almost 10,000 connected networks. In 1993 the NSF created Internet Network
Information Center (interNIC) as a governing body for DNS. In 1995 the NSF
stopped sponsoring the Internet backbone and NSFNET went back to being a
research and educational network. Internet traffic in the United States was
routed through a series of interconnected commercial network providers.

The first commercial Internet service providers (ISPs) emerged in the early
1990s when the NSF removed its restrictions against commercial traffic on the
NSFNET. Among these early ISPs were Performance Systems International (PSI),
UUNET, MCI, and Sprintlink. (The first public dial-up ISP was actually The
World, with the URL www.world.std.com.) In the mid-1990s, commercial online
networks such as AOL, CompuServe, and Prodigy provided gateways to the Internet
to subscribers. Later in the decade, Internet deployment grew exponentially,
with personal Internet accounts proliferating by the tens of millions around the
world, new technologies and services developing, and new paradigms evolving for
the economy and business. It would take a whole book to talk about all the ways
the Internet has changed our lives.

Many Internet technologies and protocols have come and gone quickly. Archie, an
FTP search engine developed in 1990, is hardly used today. The WAIS protocol for
indexing, storing, and retrieving full-text documents, which was developed in
1991, has been eclipsed by Web search technologies. Gopher, which was created in
1991, grew to a worldwide collection of interconnected file systems, but most
Gopher servers have now been turned off. Veronica, the Gopher search tool
developed in 1992, is obviously obsolete as well. Jughead later supplemented
Veronica but has also become obsolete. (There never was a Betty.)

The most obvious success story among Internet protocols has been HTTP, which,
with HTML and the system of URLs for addressing, has formed the basis of the
Web. Tim Berners-Lee and his colleagues created the first Web server (whose
fully qualified DNS name was info.cern.ch) and Web browser software using the
NeXT computing platform that was developed by Apple pioneer Steve Jobs. This
software was ported to other platforms, and by the end of the decade more than 6
million registered Web servers were running, with the numbers growing rapidly.

Lynx, a text-based Web browser, was developed in 1992. Marc Andreessen developed
mosaic, the first graphical Web browser, in 1993 for the UNIX X Windows platform
while he was a student at the National Center for Supercomputing Applications
(NCSA). At that time, there were only about 50 known Web servers, and HTTP
traffic amounted to only about 0.1 percent of the

Internet's traffic. Andreessen left school to start Netscape Communications,
which released its ﬁrst version of Netscape Navigator in 1994. Microsoft
Internet Explorer 2 for Windows 95 was released in 1995 and rapidly became
Netscape Navigator's main competition. In 1995, Bill Gates announced Microsoft‘s
wide-ranging commitment to support and enhance all aspects of Internet
technologies through innovations in the Windows platform, including the popular
Internet

Explorer Web browser and the Internet Information Server (IIS) Web server
platform of Windows NT. Another initiative in this direction was Microsoft's
announcement in 1996 of its ActiveX technologies, a set of tools for active
content such as animation and multimedia for the Internet and the PC.

In cellular communications technologies, the 1990s were clearly the "digital
decade." The work of the TIA resulted in 1991 in the first standard for digital
cellular communication, the TDMA Interim Standard 54 (IS-54). Digital cellular
was badly needed because the analog cellular subscriber market in the United
States had grown to 10 million subscribers in 1992 and 25 million subscribers in
1995. The ﬁrst tests of this technology, based on Time Division Multiple Access
(TDMA) technology, took place in Dallas, Texas, and in Sweden, and were a
success. This standard was revised in 1994 as TDMA IS-136, which is commonly
referred to as Digital Advanced Mobile Phone Service (D-AMPS).

Meanwhile, two competing digital cellular standards also appeared. The first was
the CDMA iS-95 standard for CDMA cellular systems based on spread spectrum
technologies, which was first proposed by QUALCOMM in the late 1980s and was
standardized by the TlA as IS-95 in 1993. Standards preceded implementation,
however; it was not until 1996 that the first commercial CDMA cellular systems
were rolled out.

The second system was the Global System for Mobile Communication (GSM) standard
developed in Europe. (GSM originally stood for Groupe Spéciale Mobile.) GSM was
first envisioned in the 1980s as part of the movement to unify the European
economy, and the European Telecommunications Standards Institute (ETSI)
determined the final air interface in 1987. Phase 1 of GSM deployment began in
Europe in 1991. Since then, GSM has become the predominant system for cellular
communication in over 60 countries in Europe, Asia, Australia, Africa, and South
America, with over 135 mobile networks implemented. However, GSM implementation
in the United States did not begin until 1995.

In the United States, the FCC began auctioning off portions of the 1900-MHz
frequency band in 1994. Thus began the development of the higher-frequency
Personal Communications System (PCS) cellular phone technologies, which were
first commercially deployed in the United States in 1996.

Establishment of worldwide networking and communication standards continued
apace in the 1990s. For example, in 1996 the Unicode character set, a character
set that can represent any language of the world in 16-bit characters, was
created, and it has since been adopted by all major operating system vendors.

In client/server networking, Novell in 1994 introduced Novell NetWare 4, which
included the new Novell Directory Services (NDS), then called NetWare Directory
Services. NDS offered a powerful tool for managing hierarchically organized
systems of network file and print resources and for managing security elements
such as users and groups. NetWare is now in version 6 and NDS is now called
Novell eDirectory.

In other developments, the U.S. Air Force launched the twenty-fourth satellite
of the Global Positioning System (GPS) constellation in 1994, making possible
precise terrestrial positioning using handheld satellite communication systems.
Real Networks released its first software in 1995, the same year that Sun
Microsystems announced the Java programming language, which has grown in a few
short years to rival C/C++ in popularity for developing distributed
applications. Amazon.com was launched in 1995 and has become a colossus of
cyberspace retailing in a few short years. Microsoft WebTV, introduced in 1997,
is beginning to make inroads into the residential Internet market.

Finally, the 1990s were, in a very real sense, the decade of Windows. No other
technology has had as vast an impact on ordinary computer users as Windows,
which brought to homes and workplaces the power of PC computing and the
opportunity for client/server computer networking. Version 3 of Windows, which
was released in 1990, brought dramatic increases in performance and ease of use
over earlier versions, and Windows 3.1, released in 1992, quickly became the
standard desktop operating system for both corporate and home users. Windows for
Workgroups 3.1 quickly followed that same year. It integrated networking and
workgroup functionality directly into the Windows operating system, allowing
Windows users to use the corporate computer network for sending email,
scheduling meetings, sharing files and printers, and performing other
collaborative tasks. In fact, it was Windows for Workgroups that brought the
power of computer networks from the back room to users' desktops, allowing them
to perform tasks previously possible only for network administrators.

In 1992, Microsoft released the first beta version of its new 32-bit network
operating system, Windows NT. In 1993 came MS—DOS 6, as Microsoft continued to
support users of text-based computing environments. That was also the year that
Windows NT and Windows for Workgroups 3.11 (the final version of 16-bit Windows)
were released. In 1995 came the long-awaited release of Windows 95, a fully
integrated 32-bit desktop operating system designed to replace MS-DOS, Windows
3.1, and Windows for Workgroups 3.11 as the mainstream desktop operating system
for personal computing. Following in 1996 was Windows NT 4, which included
enhanced networking services and a new Windows 95-style user interface. Windows
95 was superseded by Windows 98 and later by Windows Millennium Edition (Me).

2000’s
------

At the turn of the millennium came the long-anticipated successor to Windows NT,
the Windows 2000 family of operating systems, which includes Windows 2000
Professional, Windows 2000 Server, Windows 2000 Advanced Server, and Windows
2000 Datacenter Server. The Windows family has how grown to encompass the full
range of networking technologies, from embedded devices and Personal Digital
Assistants (PDAs) to desktop and laptop computers to heavy-duty servers running
the most advanced, powerful, scalable, business-critical, enterprise-class
applications.

In 2000 Windows XP was released and this was a complete makeover for the masses.
It had a more appealing user interface and came bundled with advanced
accessories as standard tools. it had a video-editing program called Movie Maker
and it came with a build in Web Server that could be activated and conﬁgured to
allow each user to become a publisher of content for the Internet. From 2000 to
2007 XP was the dominant desktop operating system for the masses and reached to
a 90% market penetration level. Microsoft Office also gained the dominant spot
for end user ofﬁce software, which included a Word Processor, Spreadsheet,
Relational Database Management System, Presentation Creation Software, and a Web
Site Editing Program.

This decade was also the rise of Linux on the desktop, which is still gaining
market share. Using creative applications such as Open Office enhanced
compatibility between Linux and Windows file structure. Linux is gaining on the
market share strong hold that Microsoft enjoys. Linux was created by Linus
Trovalds ( http://en.wikipedia.org/wiki/Linus_Torvalds )at the University of
Helsinki to replace Unix. Linus wanted an open source operating system that
could be created by a community of developers who would create programs that
could be freely distributed under the GPL software license. The GPL allowed
anyone to copy the program and add to it and then repackage and redistribute the
resulting program as long as the GPL license was provided to the program and the
source code was included to allow the next person to do the same. This GPL
license was fundamentally different from the Closed Source Copyright that
Microsoft and other commercial software developers used to protect their
intellectual property and control the modifications and distribution of the
software. This is the reason that Microsoft was able to grow so large and so
fast.

Today the Open Source VS Closed Source battle continues to be fought in the
trenches of the marketplace. Both systems have merits and beneﬁts and ultimately
achieve the same results which are providing usable operating systems to end
users to allow them to be productive. Linux based web servers enjoy a larger
share of the market then Windows based web servers because the Linux based
operating system was more affordable and in many cases completely free so it was
widely implemented by small and large web site operators to gain entry to the
internet

2007 also saw the introduction of the virtual computer or virtual appliance.
This development allowed a computer running Windows XP to run a virtual computer
inside of the memory and load in a Linux virtual appliance and run that
appliance in a window on the system without affecting the host operating system.
This allowed casual users to test out Linux applications and see them run
without installing a full Linux operating system and erasing the host operating
system. We also saw the creation of multi-core processors and multi processor
motherboards which greatly enhanced the processing power of a single pc box.

The future of networking is bright as faster processors and fatter faster data
pipes are created as well as new compression and transmission protocols are
approved and released into the marketplace. It is truly an interesting time to
be in the high technology field of networking.

For more information on this subject and to keep up with new and exciting
additions to this topic visit our web site at http://wwwgeekzonehosting.com

The History of Wireless Phones
==============================

The Mobile Lifecycle
--------------------

The mobile phone is the preferred voice device for hundreds of millions of
users. The main reason is that a handset is personal: in your possession 24
hours a day, 7 days a week. It contains important information conveniently;
including contacts, and may also include diary information and notes. It can be
used to access the Internet, especially for information and email. in this
section I will look at the development of cellular radio technology since its
first inception to the present day and beyond.

Way back when Mobile communication services delivered using cellular radio were
first introduced over twenty years ago: development began in the late 1960s and
the first mobile networks were launched in the early 1980s. Before the days of
cellular networks people who desperately needed mobile communications could use
a vehicle-mounted radio telephone which was serviced by one large central
antenna in a major city. These antennas could provide 25 channels at best and
the vehicle transmitter needed to be powerful enough to transmit over distances
up to 50 miles.

First Generation Systems (1 G)
------------------------------

These first mobile networks were entirely analogue and are referred to as “first
generation", or 1G, systems. Initially the number of users was comparatively low
and growth projections modest. Most countries where mobile service was available
had just one operator that was state-owned. Co-ordination between countries was
minimal and roaming between countries with the same handset was impossible.
Several different technologies emerged:

>   The United States opted for the AMPS system (Advanced Mobile Phone System).

>   The UK implemented TACS (Total Access Communications System).

>   The Scandinavian countries implemented NMTS (Nordic Mobile Telephone
>   System).

Due to the differences in spectral allocation between these countries (the way
in which the radio spectrum was allocated for different uses), these cellular
technologies worked on different frequencies.

(AMPS is still widely used in North and South America, especially in rural
areas, and I will therefore look at it brieﬂy later on.)

Devices were bulky with limited battery life, and connections and coverage area
were, at best, adequate. Little provision was made for security and the first
mobiles were subject to a practice known as “cloning". Anyone with some spare
time and some spare cash could isolate a phone’s individual identity code and
program it into another handset, hence gaining free access to the network at
some poor individual’s expense. These first analogue networks also had little to

no capacity to handle data communications. This is not to say that these
networks could not transmit data, but initially there was little demand for such
services.

Second Generation Systems (2G)
------------------------------

Second generation systems were launched in the early 1990s and were
characterized by the implementation of digital technology. Perhaps the biggest
catalyst for digitalization came in 1982, when the European Conference of Posts
and Telecommunications Administrations (CEPT), consisting then of the
telecommunication administrations of twenty six nations, created a working group
to develop a set of standards for a pan—European cellular network — the Groupe
Spéciale Mobile (the origin of the acronym GSM). The goal was to adopt and
develop a single technology for the Single European Market. The new digital
technology would replace the existing patchwork of incompatible analogue systems
and offer European citizens the opportunity to make and receive calls anywhere
in Europe. Digital technology also provided improved voice quality, security and
data-capability at 9600 bits per second.

The first GSM (Global System for Mobile Telecommunications, as the acronym now
stands for) networks were publicly launched in Europe in 1992. Meanwhile, the
Americans also adopted digital technology. Due to the prevalence of AMPS, it was
not cost efficient to implement a new GSM system, and so existing AMPS
infrastructure was upgraded to D-AMPS (Digital Advanced Mobile Phone System).

The D-AMPS system is also referred to as the TDMA (Time Division Multiple
Access) system, as this is the technology that is employed to increase network
capacity. Read the later sections for an examination of the technologies
involved in modern and future cellular radio communications.

Due to its efficient use of the spectrum of frequency allocated to use by
cellular radio, GSM technology allowed more operators to enter the market and
hence service more subscribers. A second band was licensed for use by GSM
networks by the European regulating body to secure investment confidence, drive
competition and provide more capacity.

GSM networks were rolled out in North America, but whilst Europe and Asia were
adopting it unilaterally, initial deployment in the US was slow. Another digital
technology was also available which offered much higher capacity capabilities
than GSM and data rates at 14.4 Kbps — this technology was CDMA (Code Division
Multiple Access). Japan’s NTT DoCoMo also shunned GSM and adopted an entirely
proprietary technology called PDC (Packet Digital Communications) which is
unique to Japan.

These, then, are the four digital cellular radio technologies within the 2G
deﬁnition: GSM, TDMA, CDMA and PDC.

2.5G Systems
------------

Despite the plethora of improvements these 2G digital systems heralded, the
network operators soon realized that they would soon be outpaced by the growth
in popularity of mobile telephone communications. It also became clear that the
modern subscriber required a much broader range of services, much of them
data-related, than they were in a position to provide. They therefore decided to
implement a number of enhancements.

These enhancements are known as 2.5G systems as they are effectively “bolt- ons"
to the existing network infrastructure: a stop-gap to accommodate the ﬂood of
new subscribers while network operators can roll out 3G systems.

GSM operators implemented GPRS (General Packet Radio Service) as well as, in the
case of Orange, HSCSD (High Speed Circuit-Switched Data).

D-AMPS / TDMA operators in North America implemented CDPD (Cellular Digital
Packet Data)

CDMA operators in North America improved the existing air interface to provide
data rates of up to 19.2 Kbps

This brings us to the present day...

Today
-----

178 countries worldwide now embrace GSM, serving a truly mass market of 677
million users (as at the end of March 2002). GSM is the only cellular system
that is standardized for use in all four cellular bands worldwide (850, 900,
1800 and 1900 MHz). The GSM standard has evolved over a decade from providing
basic voice with simple text messaging (SMS) to a wide variety of speech and
data services. GSM voice quality is generally excellent, coverage in most
countries extends to over 99% of the population, and common technology ensures
automatic international roaming for voice and SMS for virtually all users around
the globe (customer services asidel).

CDMA currently services more subscribers in North America than GSM. Its
popularity among operators is due to its very efficient use of the radio
frequency assigned to them.

The Future
----------

2.5G systems are far from providing a truly global service. A European GSM
subscriber cannot roam onto a North American CDMA network with the same handset.
But, as will be discussed later, in terms of network infrastructure these
systems are not radically different. it is the different ways in which the
systems manipulate the radio spectrum that make them incompatible. If these
systems could both adopt a compatible air interface, then there would be
potential for a truly global cellular radio mobile network.

The future of mobile telecommunications, certainly from the operators’ point of
view, is data. It is estimated that by 2006 data services will generate 50% of
revenues, with networks offering audio and video streaming, real-time road GPS
and MMS (Multimedia Messaging Service).

Operators in many regions of the word have reached a penetration level exceeding
75% of the population when it comes to voice coverage. Future users will not
just talk, they will see, share and do.

How do wireless mobile phones work?
===================================

Mobile devices are radio telephones. The electrical telephone was invented in
March 1876 by Alexander Graham Bell. Guglielmo Marconi first formally presented
the radio in Italy in 1894. The design of both devices has remained
fundamentally unchanged since the original.

Cellular networks around the world today employ a variety of different
technologies, and even now there is no one true global standard. All of the
technologies however do have many common features and the fundamental concepts
behind cellular networks are common to all.

Cellular networks divide the area to be serviced into smaller cells, each cell
being the area of coverage around a cell tower, or Base Transceiver Station.

A cellular network operator will be assigned a range of frequencies to use by
the governing body in that country. In order to use this frequency allocation
efﬁciently, it is subdivided into a number of smaller channels. Each cell
operates on one of these channels, no two adjacent cells operating on the same
frequency. This practice is known as Frequency Division Multiple Access (FDMA)
and is common to all cellular systems.\*

All mobile devices have special codes associated with them, which are used to
identify the phone, the subscriber and the network operator. The information
about the individual subscriber may be held on a removable memory card (SIM
card, or Subscriber Identity Module), or could be hard-wired into the phone
itself as is the case with most non-GSM networks.

When you first power on the mobile device it searches for an SID, or System
Identity Code — a unique 5-digit identifier assigned to each network operator by
the local governing body — on the control channel. If it cannot find any such ID
it will display a “no service” message.

When the device finds an SID, it compares it to the SID that is programmed into
it or the SIM. If they match then it knows it is on the home network.

Once the mobile device has detected the network, it then transmits a
registration request. The network updates its location database with the new
information and sends a message to the device via the control channel letting it
know what frequencies to use for uplink and downlink. When the mobile network
receives a call from the landline system destined for that device, it consults
the location database to determine what cell it is in, and routes the call
accordingly.

A" mobile stations have a 15-digit identifying number called an IMEI number
(Individual Mobile Equipment Identifier), this can normally be found written on
a sticker underneath the battery. When a mobile station registers on the network
it broadcasts its ESN (Electronic Serial Number) which is verified by the EIR
(Equipment Identity Register) and used by the HLR to track the device’s
location.

As the device moves toward the edge of a cell, the base station notes that the
signal strength is diminishing. Meanwhile, the base station in the cell which
the device is moving toward (which is listening and measuring signal strength on
all frequencies) registers the signal strength increasing. The two base stations
coordinate with each other through the controller, and at some point, the device
is sent a signal on the control channel telling it to change frequencies — to
therefore move from one cell to another. This procedure is known as hand off.

Don’t concern yourself too much with the amount of acronyms l have thrown at you
already, all will hopefully become clearer in a moment.

>   This is not strictly true, but bear with me at this stage. I will look in
>   more detail at the technologies employed by different cellular systems later
>   on.

Analogue versus digital
-----------------------

The first generation of cellular technologies relied on analogue systems to
provide the air interface (the link between the mobile station and the nearest
BTS).

Second generation (and subsequent) systems use digital systems in preference to
their analogue counterparts. Digital systems use the same radio technology as
analogue systems, but they use it in a different way.

Some people are understandably confused as to how a mobile network can be
digital. Simply put, both analogue and digital systems use analogue radio waves
to communicate between the wired network (the cell towers) and the mobile
device. But digital systems use the analogue wave to transmit binary information
in a series of is and Os, rather than an analogue sound wave. Read on...

Analogue signals have a tendency to lose their integrity because of the problem
of “noise” or interference. The loss of this signal power is called attenuation.
When an analogue signal passes through an amplifier to boost the signal, the
noise that has been picked up along the way is amplified as well. The further a
signal travels the more noise it will pick up. This is why long-distance calls
sound much worse than local calls. Provided that the signal to noise ratio is
low, then the interference is not a problem. However, one the signal to noise
ratio gets too high, then the information cannot be interpreted. Data
communications, which rely on high levels of quality, are therefore not well
suited to analogue transmissions.

Digital signals are not as susceptible to this problem as they can only
represent two values: 1 or 0, whereas analogue signals can represent a range of
values.

Digital signals do not use amplifiers to boost signal strength, instead digital
signals are regenerated. A device known as a repeater examines the incoming
signal, determines whether each bit has a value of 1 or 0 and then passes the
information to the output device where a perfect signal is generated.

Any noise that was picked up along the way is removed. Therefore the same
quality of signal is maintained throughout.

How does analogue-to-digital conversion work?
---------------------------------------------

This section will examine how an analogue signal can be converted to a digital
data stream, what constitutes data, and how digital data can be transmitted over
a cellular radio link.

The best way I can think of to explain how analogue to digital conversion works
is to compare vinyl LPs and CDs.

Thomas Edison is credited with inventing the first means of recording sounds in
1877. Speaking into a “collecting horn" caused a needle attached to the horn to
vibrate. The needle could be used to etch these vibrations into a suitable
medium, such as a wax or tin cylinder. Rotating the cylinder and allowing the
needle to transfer the vibrations back to the horn could play the sound back.

This is essentially the method still used today in the preparation of vinyl
records, albeit vastly improved.

The variable-depth grooves etched into a record represent an analogue
wave.Unfortunately this means of recording is low in fidelity — the noise caused
by the etching itself is recorded, and after repeated playback the grooves
become worn out.

The goal with digital media, such as the CD, is to produce a high-fidelity
recording (it sounds the same as the original) with perfect reproduction (it
sounds the same every time it is played back).

To accomplish this, the analogue wave is converted into a series of numbers, and
the numbers are recorded instead of the wave. An analogue-to-digital converter
does this conversion. During playback, the numbers are converted back into an
analogue wave by a digital-to-analogue converter (DAC). This analogue wave is
then amplified and passed to the speakers to produce the actual sound.

Provided the numbers held on the CD are not corrupted, the analogue wave
produced will be the same every time.

To understand how analogue information can be converted to a digital signal, it
is necessary to understand how digital signals represent data.

What is data?
-------------

User data can be anything — a text document, a spreadsheet etc. As far as a
computer is concerned all data is a sequence of 1s and Us, or binary code.

The simplest unit of data that a computer deals with is a binary digit, or bit,
and can have two values — 1 or 0.

>   A bit must be grouped together with other bits in order to represent
>   information.

>   A byte is an 8-bit character. By using 8 bits a byte can hold any value from
>   1 to 256:

In the example shown above, a byte consisting of the bits 00010000 represents a
value of 16.

A byte can therefore be used to represent 256 different characters. For every
character typed into your computer there is a corresponding numeric value
(decimal). The conversion is carried out by a code known as ASCII (American
Standard Code for Information Interchange). The ASCII table contains 256
characters, each with a corresponding number value.

The first 128 characters are the alphabet (in both upper and lower case),
numbers 0 to 9, and all punctuation. The second 128 characters comprise of
graphical symbols (the second half of the ASCII set is rarely used in data
communications). Depending on the country for which your computer operating
system is conﬁgured, the ASCII set will vary to include the necessary country
specific characters.

ASCII is only a standard used in the western world, as most far eastern
ideographic character sets feature many more than 256 symbols.

So how is an analogue wave converted to digital information?
------------------------------------------------------------

Below is a representation of an analogue wave. Assume that the ticks on the
horizontal axis represent 1/1000ths of a second.

When the analogue wave is sampled, there are two variables involved — the
sampling rate and the sampling precision. The sampling rate determines how many
samples are taken a second. The sampling precision controls how many gradations
are possible when taking the sample. This is also known as Quantization. The
illustration below shows a sampling rate of 1,000 per second with a precision
rate of 10:

The green rectangles represent samples. Every 1/1000th of a second the converter
looks at the analogue wave and converts its value at that point to the closest
number between 0 and 9. Using this sampling scheme the resulting blue line is
created:

You can see that a lot of information has been lost using this scheme. A higher
quality image would be gained by using a higher sampling rate with a higher
level of gradation. In the case of CD sound, fidelity is an important goal so
the sampling rate is 44,100 samples per second and the number of gradations is
65,536. At this level the output of the DAC so closely matches the original
waveform that the sound is essentially "perfect" to most human ears.

This sampling rate produces a lot of data — an awful lot. It takes two bytes to
represent each of the possible 65,536 gradations (as discussed above, each byte
can represent any value between 0 and 256 [28:256], but 216=65,536). Two sound
streams are being recorded (one for each of the speakers in the stereo pair). A
CD can store up to 74 minutes of sound. This represents:

44,100 samples/channel/second x 2 bytes/sample x 2 channels x 74 minutes x 60
seconds/minute = 783,216,000 bytes

When an analogue voice signal is converted to digital information for
transmission over a cellular radio link, the sampling rate required is not this
high, and the range of frequencies used by human speech is not as broad, so the
amount of data transmitted does not even begin to approach this amount.

Mobile Data
===========

What does a modem do?
---------------------

With conventional dial-up landline connections a computer requires a modem to be
able to transmit data over telephone lines.

Landline telephone systems are optimized for speech. As the human voice only
operates within a distinct frequency range, networks are optimized to transmit
only these frequencies, often employing a passband filter to remove any other
signals. The copper cables used in landline networks can comfortably transmit
the frequencies used in human speech, but is not able to reliably transmit
signals outside this range.

Data transmissions require a much wider frequency spectrum and therefore cannot
pass through the landline telephone system without some form of conversion. This
function is performed by the modem. The data signal is MOdulated prior to
transmission by the sending modem, and then DEModulated by the receiving modem.

The amount of data a modem can transmit depends on the type of modulation scheme
it employs.

Modem speed is often referred to in terms of the baud rate. Technically, the
baud is the number of changes that can be made to a standing wave in one second.
When a modem is working at 300 baud, this means that the basic carrier frequency
has 300 cycles per second. Due to restrictions imposed by the physics of the
wiring, a dial-up phone line can go up to 2400 cycles, a baud rate of 2400.

Real modem speed is measured in bits per second (bps). If each cycle is one bit,
the fastest rate at which data can be transmitted is 2400 bps. However, by using
different types of modulation, more than one bit can be transmitted per cycle.

How is digital data transmitted over a cellular radio network?
--------------------------------------------------------------

As discussed above, the first analogue networks did not support the transmission
of data, partly due to the high quality of radio link required by data
connections, but also because in the early 1980s there was little demand for
such a service.

When networks are referred to a being digital, it is only the method of
conveying the information that is digital — the signal is still sent via an
analogue radio wave. The encoder converts the analogue information to be
transmitted into a series of 1s and Os. These binary digits are then sent over
the analogue carrier wave (the radio wave).

There are several methods of doing this, but all methods operate according to
the same underlying principle. In essence, data is transmitted by altering a
standing wave (the carrier wave) to convey either a value of 1 or 0.

This altering the characteristics of a carrier wave to bear information is known
as modulation.

One form of modulation is Amplitude Modulation (AM). The amplitude of the wave
(in mathematical terms, its displacement from the horizontal) is modified to
represent two possible values: 1 or 0. This method of modulation is susceptible
to interference, however.

An alternative to amplitude modulation is Frequency Modulation (FM), also known
as Frequency-Shift Keying (FSK). FSK alternates the frequency of the wave, using
one frequency to indicate a O and another to indicate a 1.

The information is recovered from the modulated carrier wave at the receiver by
a process of demodulation.

Due to the limitation of the bandwidth allocated to cellular network operators,
the amount of data that can be transmitted is very small. GSM networks can
handle up to 9.6 Kbps and CDMA networks up to 14.4 Kbps.

The amount of data required to reproduce the human voice, however, exceeds this
limitation.

The human ear can detect sounds in the range of a few hundred Hertz up to around
20 Hz, but most of the frequencies used for human speech are in the range
between 300 Hz and 4000 Hz.

To filter out the frequencies not required for human speech, a source encoder is
used. This filters out “redundant" bits from the digitized data stream.

To represent the human voice, the analogue signal needs to be sampled 8,000
times a second. This sampling rate is often referred to as the Nyqvist interval
after the Swedish scientist who isolated this value as being the optimum
required to reproduce human speech reliably. Each sample is quantized to one of
256 values, requiring 8 bits of data per sample. Therefore:

8,000 samples per second x 8 bits per sample = 64 Kbps

Because the only a specific range of frequencies is being sampled, the sampling
precision is referred to as non-linear quantization.

Whilst a data rate of 64 Kbps is fine for the landline networks, where bandwidth
is not too much of an issue and error rates are also not so consequential, the
air interface used by cellular networks is a far more limited resource and is
much more difficult to work with. Cellular networks therefore use modulation
schemes that combine forward error correction and compression techniques. l will
examine both these techniques in a moment.

Data transmission
-----------------

Data transfer can be either serial or parallel. Parallel communications are
quicker as concurrent paths are used simultaneously to transfer larger amounts
of data. But parallel communications are only feasible over short distances. For
long distance communications serial has to be used. Over serial connections data
is sent one bit at a time. As a whole byte has to be received before the
resulting value can be determined, computers use a buffer to store bits whilst
it is waiting for the whole byte to be compiled.

Data transfer can be either synchronous or asynchronous. in order for two modems
to reliably exchange information they must have a reliable clocking mechanism so
that the sampling rate being employed is synchronized. Synchronous data transfer
involves synchronizing the clocks on both the sending and the receiving modems
before any data is transmitted. Synchronous communications are only necessary
where large amounts of data need to be transferred on an almost constant basis.

Asynchronous data transfer involves adding data to the data stream to allow the
modems to synchronize clocks. Bytes are sandwiched by “start" and “stop" bits.
If you consider that error correction methods also add bits to the data stream,
this represents a significant overhead and is the reason why data transfer is so
slow — transferring a 100Kb file involves sending a lot more information than
just the file itself.

All cellular networks transmit data asynchronously. In order to understand how
modems communicate with each other, it is necessary to clarify a few terms:

### DTE

Data Terminal Equipment (DTE) is the combination of a computer, a port, and
application software that communicates with a second application — the remote
application over a telephone line. The remote application is also a
configuration of a computer, a port and an application that makes up the second
DTE.

Most computers use a chip called a universal asynchronous receiver/transmitter
(UART) to convert the computer’s synchronous parallel data from the processor
into asynchronous serial data ready for transmission to the modem.

### DCE

The Data Communications Equipment (DCE) is the modern. A DCE is also known as
data circuit-terminating equipment, and its purpose is to link the DTE to the
communication line.

Various asynchronous standards define the interface or signaling that goes on
between the DTE and the DCE, and between two DCEs.

In asynchronous transmission, data is transmitted one character at a time. A
start bit and one or more stop bits bracket each character. Asynchronous
transmission is also called start-stop transmission. The asynchronous characters
are not evenly spaced along the transmission medium. In gaps between characters,
the line is idle; nothing is transmitted. The characters are transmitted
independently with regard to timing signals.

The R8232 standard defines nine electrical circuits used for protocol
handshaking. The following sections describe what these nine circuits do.

### Carrier Detect (CD)

When the local modem detects a carrier signal from the remote modem, it raises
the voltage level on this lead, indicating to the computer that a carrier signal
is being received from the remote modem. This signal is also called the Received
Line Signal Detector (RLSD).

The carrier signal is a tone or frequency. You can hear it usually just after
you hear the originating modem dial. It is the sine wave signal that digital
data from the two computers is mapped onto by the modem’s modulation function.
Without a carrier there can be no connection. If this signal is dropped during a
connection, your communication software will usually display a lost carrier
error message. If a carrier is not detected at the start of a connection, the
calling or originating modem displays no carrier.

### Receive Data (RD)

When the local modem demodulates data received from the remote modem (DCE), the
demodulated data is sent to the computer serial port by raising and lowering the
voltage level on this lead.

### Transmit Data (TD)

The computer sends data to the local modem by changing the voltage level on the
Transmit Data circuit.

### Data Terminal Ready (DTR)

The computer turns this circuit on when it is ready to be connected to the phone
line. If the modem does not get this “ready” signal, no attempt to dial can
occur and no commands from the computer will be received.

### Signal Ground (SG)

Signal Ground (SG) provides a reference level or benchmark voltage for the other
leads (circuits). Asynchronous signals (events, state changes) are sent between
the serial port and the modem by voltage changes in circuits. For example, the
computer sends or asserts DTR to the modem by changing the voltage on pin 4 to
the voltage that indicates ON.

However, to make sense of this voltage change, a constant benchmark voltage is
needed; this is the voltage level on pin 5 that is considered to be the zero
voltage level.

Note, that there is another ground, called protective ground, which is like the
third prong on a three-prong appliance plug. Protective ground is pin 1 in
25-pin connectors, and if it is present, SG is usually strapped or connected to
protective ground. This pairing is not possible on a 9-pin connector because it
does not have a protective ground lead.

### Data Set Ready (DSR)

The modem turns this circuit on (asserts DSR to the computer) when it is ready
and physically connected to the phone line. If the terminal does not get this
ready signal, any attempt to dial will fail.

Request To Send (RTS) - The RTS circuit is turned on by the serial port to tell
the modem that it actually has data queued for transmission. if the modem has
CTS (Clear To Send) turned on, actual transmission will start. CTS off,
signalled by the modem to the computer, tells the serial port that the modem
temporarily cannot accept any data for transmission. When the asynchronous
application writes a buffer destined for the remote application, this causes the
serial port to raise RTS. RTS and CTS are used for flow control between DTE and
DOE.

### Clear To Send (CTS)

The CTS circuit is turned on by the modem to signal the serial port that the
modem wants to write to the computer. If the serial port has RTS turned on (it
responds with RTS), the modem will actually transfer data to the serial port.
Data is accumulated byte by byte in a buffer by the serial port. When the buffer
is full, it is transferred to an application buffer and is then ready to be
filled again by the modem. If the serial port cannot accept data from the modem
(DCE), it turns RTS off and the modem waits for RTS on. If RTS is off most of
the time, it usually means the application is too slow. The modem is receiving
from the remote DCE faster than the local application can read and process data.

### Ring Indicator (RI)

The Rl circuit is turned on by the modem to signal the serial port that there is
an inbound call. The line rings for one second, and then there is a four second
pause. The RI circuit is turned on for each ring. It serves as a wake-up signal
to the DTE. The DTE responds by asserting DTR to tell the modem that it is ready
to be connected over the telephone company line. If carrier is detected, then
data transmission starts.

### Modem buffers

When the asynchronous application does a write, the bytes are queued in one or
more buffers. The computer operating system then empties the application buffers
by sending or writing the bytes out the serial port to a buffer in the modem.
The asynchronous application in a single write can fill a large buffer; for
example, 1024 bytes. Because the modem sends or transmits data one byte at a
time over the line, the modem buffer can be overwhelmed. Application data in the

modern buffer can be overwritten and lost. When the application is reading data,
the modem can transmit too fast, causing data in the application buffers to be
overwritten. To keep this from happening, for data going in either direction,
flow control is implemented.

Flow Control
------------

When the modem’s buffer is so full that additional data written into the buffer
causes data to be overwritten and lost, the modem has to signal the serial port
to stop sending data. For inbound data, if the application can’t process the
data fast enough (that is, it empties the modem buffer too slowly), and then the
serial port has to signal the modem to stop sending. This handshaking or
signaling is called flow control. Typically, flow control will be implemented in
one of three ways: RTS/CTS or hardware flow control; XON/XOFF or software ﬂow
control; or ENQ/ACK or Enquire /Acknowledge flow control.

### RTS/CTS or Hardware Flow Control

With RTS/CTS ﬂow control, when the serial port wants to send data to the modem,
it signals Request To Send (RTS) to the modem. If the modem can accept the data
(that is, the buffer is not too full), it returns a Clear To Send (CTS) to the
serial port. If the modem cannot accept the data, it turns off the CTS signal.
On a 9-pin connector, RTS is signalled by raising or lowering the voltage levels
on pin 7; CTS is signalled by raising or lowering the voltage on pin 8. For
example, if the modem cannot receive any more data, it drops the voltage on pin
8, turning off CTS. Sometimes the application cannot process its buffers fast
enough. To keep the modem from over-writing data, the application can tell the
modem to drop RTS. The DOE (the modem) senses that RTS is dropped and stops
filling the application buffer. When the application has emptied some of its
buffers, it raises RTS again and the modem starts ﬁlling the buffer.

### XON/XOFF or Software Flow Control

When a modem is receiving data from the local terminal or serial port too fast
for the modem to process the data, the modem will send an ASCII 19 or Device
Control 3 character to the serial port. This is the default XOFF character. It
is sent on pin 3 (RD).

When the modem buffer is no longer full (that is, data has been transmitted to
the remote modem), the modem sends the XON character — an ASCII 17 or Device
Control 1 character to the serial port, and the serial port starts sending data
again. The characters used for XON/XOFF can often be changed in the application
and the modem if desired.

One problem with the XON/XOFF is that the ASCII characters sent to the serial
port, could be characters embedded in a data file. The data is unintentionally
interpreted as a control signal, causing transmission errors.

### ENQ/ACK or Enquire / Acknowledge Flow Control

In ENQ/ACK ﬂow control, when the serial port wants to send data, it transmits an
ASCII enquire character. If the modem can accept data, it ACKs the serial port
(transmits back the ACK, positive acknowledgement character). It if can’t accept
the data (buffer full), the device or modem sends back a NAK, or negative
acknowledgement, in response to the ENQ.

### Error correction Cyclic Redundancy Check (CRC)

When the first modem has accumulated a buffer full of data for transmission, it
applies a formula to the data block, calculating a value D. A second formula is
applied to the data and the result is divided into the ﬁrst value. The final
result is a whole number quotient and a remainder.

Apply first formula to data. Result = D

Apply second formula to data. Result = G

Calculate D/G = Q + R

D is the result of applying the first formula to the data block. G is the value
of the second formula. The remainder R is the cyclic redundancy check (CRC). It
is usually 16 bits long and is appended to the end of the data block when the
modem, which is not sending at that time, transmits.

When the receiving modern gets the data block, it goes through the same
calculation. If there was a transmission error, the second CRC value will not
equal the original one, and the receiving modem transmits a negative
acknowledgement (NAK). The originating modem then retransmits the data block. If
the two CRCs match, the receiving modem transmits a positive acknowledgement
(ACK). The most commonly used formula, or polynomial, is called CRC-16.

Other methods
-------------

The simplest method of data compression is known as Run Length Encoding (RLE).
RLE counts repeated characters or bit patterns, and then replaces the repeated
characters with a representative bit pattern and a multiplier equal to the
number of times the pattern or character is repeated.

The Lemple—Ziv—Welch (LZW) algorithm inspects the data for recurring sequences
of information. LZW then builds a dictionary of repeated sequences. A pointer to
the appropriate dictionary entry can now replace each repeated sequence in the
data stream.

Huffman encoding looks at the data and counts which characters are most
frequently used. These characters are assigned the shortest bit values. At the
same time, the least frequently occurring characters are assigned the longest
bit values.

Parity Bit
----------

Bits are sometimes dropped because of noise. A parity check can be added to the
data packet to recover from errors in which a single bit is dropped or added.
Many communication software packages allow you to configure parity settings, or
you may also be able to specify parity information through the serial port
conﬁguration options in an operating system. Parity, the number of stop bits,
and the number of data bits, can all be set as serial port parameters.

if a parity bit is used, then an extra bit is added to each character to make
the total number of 13 in the character either odd or even, depending on which
parity type is used. If you configure your communication software to use parity
bits, then there are seven data bits. If parity is not used, then there are
eight data bits.

Compression
-----------

Compression works by examining the data to be transmitted and substituting
commonly occurring bit sequences with a code that denotes that sequence.
Provided that the receiving device supports the same compression protocol it
will know to substitute that code with the actual bit sequence.

AMPS (Advanced Mobile Phone Service)
------------------------------------

AMPS is the ﬁrst generation analogue cellular technology still widely in use in
North and South America today.

AT&T began development in 1960s and first commercially launched it in 1983 in
Chicago. AMPS uses a range of frequencies between 824 MHz and 894 MHz for
analogue cell phones. Due to the US government’s laws on monopolies — each major
city in the US must be serviced by at least two network operators. These are
referred to as A and B carriers.

Carriers A and B are each assigned 832 frequencies: 790 for voice and 42 for
signaling (not to be confused with data). A pair of frequencies (one for
transmit and one for receive) is used to create one channel. The frequencies
used in analog voice channels are typically 30 kHz wide -- 30 kHz was chosen as
the standard size because it gives you voice quality comparable to a normal
wired telephone.

The frequencies used for uplink are separated from those used for downlink by 45
MHz to keep them from interfering with each other. Each carrier therefore has
395 voice channels, as well as 21 data channels to use for housekeeping
activities like registration and paging.

GSM (Global System for Mobile Communication)
--------------------------------------------

The development of GSM began properly in 1982, when the European Conference of
Posts and Telecommunications Administrations (CEPT), consisting then of the
telecommunication administrations of twenty six nations, created a working group
to develop a set of standards for a pan-European cellular network — the Groupe
Spéciale Mobile (the origin of the acronym GSM).

The first GSM (Global System for Mobile Telecommunications, as the acronym now
stands for) networks were not publicly launched until 1992. Before then the
first generation of mobile telecommunications services (1G) were almost entirely
analogue, unsecured and had little to no ability to handle data transmissions.
Individual nations developed mobile networks independently of each other and
roaming between countries with the same handset was impossible.

The first decision the Groupe Spéciale Mobile made was to adopt a digital
system. Digital communications provide several benefits over analogue systems:
security, data-capability and also devices can be physically smaller. It was
originally intended that the digital mobile network would interface with the
landline ISDN network, but limitations of the radio link meant that data rates
of only 9.6kbps could be achieved compared to the 64kbps of the landline system.

GSM was originally only licensed for use in the 900 MHz frequency range.
However, today GSM operates on 1800MHz and 1900MH2 due to local limitations in
the allocation of available frequencies to mobile operators. Because if this,
you may hear GSM networks being referred to as D08 or PCS networks.

DCS (Digital Communication Service) is identical to GSM but operates on 1800
MHz. The advantage of using the higher frequency means that mobile stations can
use much lower power levels.

PCS (Personal Communications Services) is the umbrella term used to refer to
networks operating at 1900 MHz in North America. The PCS initiative was launched
when mobile operators realised that the older AMPS system was rapidly nearing
saturation level and that an alternative was necessary. PCS technologies include
D-AMPS, GSM and CDMA.

GSM Network Infrastructure
==========================

A GSM network is made up of a number of key components. The first of these is
the subscriber’s mobile device. I use the term “device”, as it may be a mobile
phone, PCMCIA radio card, cellular engine or any other device that incorporates
a GSM transceiver and SIM card. In GSM terms this referred to as the terminal or
“mobile station".

The terminal contains an antenna that sends and receives radio signals to the
nearest cell tower, or Base Transceiver Station (BTS). The radio coverage of an
individual BTS is called a “cell”.

BTS are located every few miles, sometimes closer if the landscape prevents
effective propagation of radio signals (or in areas of dense population). A
country may have many thousands of BTS. Increasing or decreasing the
transmitting power can vary the effective coverage area of a single BTS.

A handful of BTSs will be connected to a Base Station Controller (BSC). The BSC
monitors the performance of individual BTSs and also enables subscribers to-pass
from one cell to another (“handover").

Multiple BSCs will be linked into the network’s Mobile Switching Centre (MSC). A
country will only have several MSCs.

The MSC handles the routing of calls between GSM cells (if the call is made
between two subscribers on the same network) and also to the terrestrial PSTN
(Public Switched Telephone Network) and ISDN (Integrated Services Digital
Network) networks.

The MSC that routes calls to the terrestrial network is known as the “gateway
MSC”.

Located at each MSC is a component known as the VLR (Visitor Location Register).
The VLR is responsible for keeping track of all subscribers and their cell
locations within the geographical region of the nearest MSC. By knowing a
subscriber’s location a GSM network operator can route incoming calls with the
minimum impact on network resources.

The VLR is a dynamic database that has up to the minute records of where mobiles
are, which mobiles are in conversation and which mobiles are banned for various
reasons. When a mobile is switched off, the VLR typically removes that mobile
from its database, thus keeping the number of active subscriber records to a
minimum.

This information is also passed onto a larger central computer known as the HLR
(Home Location Register).

The HLR is responsible for routing incoming calls from the terrestrial network
to the required part of the GSM network.

The HLR also contains information on current services available to individual
subscribers (data, fax, roaming, call forwarding etc).

In addition to the HLR, the central MSC also houses two computers, the EIR
(Equipment Identity Register) and the AuC (Authentication Center).

Together these computers verify the identity of all subscribers by checking the
information contained both within the mobile device and the Subscriber identity
Module (SIM).

The Gateway MSC interfaces with the landline telephone system via an IWF, or
inter-Working Function (also referred to as an IWU — Inter—Working Unit). This
is essentially a bank of modems which converts the incoming digital data stream
into the analogue tones used by the PSTN.

The entire network is divided into three subsystems:

>   1. Base Station Subsystem (BSS) — BTS and 880

>   2. Network Switching Subsystem (NSS) — MSC and HLR

>   3. Network Maintenance Subsystem (NMS) — monitors the performance of all

>   network components

GSM frequency usage
-------------------

For GSM to succeed as a global standard, all member countries would ideally
operate networks on the same frequency. However, despite the best efforts GSM
still operates on three different frequencies:

>   900 / 1800 MHZ

>   1900 MHz (North America only)

Handsets operating on two or all of these frequencies are referred to as “dual-
band" or "tri-band" devices.

These frequencies were chosen because they were available in almost all of the
countries that wished to implement the standard. But they also serve practical
considerations. A radio receiver needs to be as large as half the wavelength of
the frequency it is designed to receive. Low frequency waves have long
wavelengths. High frequency waves have short wavelengths, which means that the
network’s radio interface does not have to be very large.

With the small amount of the frequency spectrum available, to accommodate the
large number of mobile subscribers the networks developed some clever techniques
to increase capacity.

A combination of three techniques is used today: "' FDMA (Frequency Division
Multiple Access)”

Networks that operate on 900 MHz actually use the frequency range 905 to 915 MHz
for uplink, and 950 to 960 MHz for downlink. 2 operators, giving 5 MHz for each
operator, share this 10 MHz “passband”.

This 5 MHz “bandwidth” is further sub-divided into 25 “carriers” of 200 KHz
each. 24 carriers are available for subscribers while one carrier is reserved
for network control signals.

Cellular Frequency reuse
------------------------

The twenty four 200 KHz carriers are distributed among a number of abutted cells
and then re-used over and over again.

This is possible as long as no two adjacent cells use the same carrier
frequency. Using this technique only a small number of carriers can service a
whole nation of subscribers.

TDMA (Time Division Multiple Access)
------------------------------------

Each 200 KHz carrier is further divided into 8 “time-slots", with one subscriber
assigned to each time-slot.

Each subscriber on the carrier transmits for just 0.5 milliseconds before
passing communications over to the next subscriber on the same carrier. This
process is repeated in a cyclic pattern with each subscriber transmitting for an
eighth of the time. Because each user is only able to transmit for the duration
of the timeslot, voice and data traffic is buffered and transmitted in short
bursts. The time slots are so small and the cyclic rate so high that the user
perceives an uninterrupted communication channel.

One weakness with this system is that data has to be transmitted to preserve the
individual data streams. That is to say, if one user has no data to transmit,
“blank” data must be sent over the carrier to ensure that the other time slots
do not fall out of sync.

This represents a further eightfold increase in the possible number of
subscribers per cell.

Each cell has a finite number of subscribers it can service. This is based on
the number of carriers multiplied by the number of time-slots per carrier (8).

The size of a cell can be multiplied by the increasing the output power of the
BTS. In this way small-diameter cells in close proximity can service urban
areas, whereas larger—diameter cells can service rural areas.

GSM Data Communications
-----------------------

GSM was the ﬁrst mobile standard to allow for data and fax communication. Due to
limitations in technology and available bandwidth, the speed of transmission was
limited to 9600 bits per second per time slot allocated to the subscriber.

In terms of performance this equates to about 1.5 pages of A4 text being
transmitted every second.

For fax communication 9600 bps is comparable to the performance of a landline
fax machine, but for data transfer this is less than a fifth of the speed of a
modern fixed line modem.

This speed limitation puts severe restrictions on the applications that can be
used by a mobile subscriber.

The following applications will operate adequately at 9600 bps:

>   Text-based email

>   WAP

Instant messaging

Applications involving graphic or file transfer require a much higher data rate.

Making a data connection over GSM is similar to making a data connection via a
landline.

>   1. A number is dialled and after about 20 to 30 seconds a circuit is
>   established through the network to the destination.

>   2. While the circuit is open, the operator charges the subscriber for the
>   duration of the call. This is referred to as a “circuit-switched” call.

### Limitations

There are many weaknesses to this approach to data communications:

>   1. Subscribers have to wait up to 30 seconds to get connected to the
>   Internet

>   2. Subscribers are billed for “dead" time when they are not necessarily
>   transferring any data but the connection is still open.

>   3. While the subscriber is connected to the internet, but not necessarily
>   sending or receiving data, that channel is not available to any other
>   subscriber, effectively wasting the resource.

CDMA (Code Division Multiple Access)
====================================

i imagine that most of you reading this in the UK will not have come across CDMA
before, as Europe has unilaterally adopted GSM as its cellular technology of
choice. CDMA is not new, however. It has enjoyed widespread implementation in
North America and services more subscribers there than GSM networks.

Code Division Multiple Access was first developed by the US military during the
1940s as a robust transmission system, which could withstand jamming attempts
and was developed as a mobile telecommunications standard by Qualcomm in 1993.

CDMA is a spread spectrum technology. The term “spread spectrum" simply means
that data is transmitted in small pieces over a number of frequencies within the
entire available frequency range, rather than within a single defined channel.
Digital Spread Spectrum (DSS) was developed during World War ll by the Americans
as a robust communications standard resistant to deliberate or natural
interference. It was initially used to direct torpedoes but has developed into a
standard used in many different military and commercial applications, including
cordless phones.

Bizarrely an American actress called Hedy Lamarr invented it. Lamarr and her
arranger invented the frequency—hopping concept. Since Lamarr didn't use her
stage name on the patent, it took years for the story of patent 2,292,387 to
surface. It never made her any money.

There are two approaches to spread spectrum: Direct Sequence Spread Spectrum
(D888) and Frequency Hopping Spread Spectrum (FHSS).

DSSS works by splitting data up into a number of packets and sending them
simultaneously over a number of different channels.

FHSS works by sending a short burst of data over one frequency, changing to
another frequency, then sending another burst of data. The choice of the next
frequency is random, so it is nearly impossible for someone to eavesdrop or jam
the signal. Both sending and receiving devices need to agree on the frequency-
hopping algorithm used before transmission. This mechanism allows for several
different FHSS systems to work side by side without interfering with each other.

D888 is capable of much greater speeds than FHSS, but FHSS is less prone to
interference than DSSS. CDMA uses FHSS technology.

Instead of dividing the frequency range available to the network operator into
discrete channels and splicing users into that channel using a time—based
multiplexing algorithm like TDMA systems, CDMA assigns each user an individual
code and spreads their transmission over all of the available bandwidth.

One weakness of the TDMA system is that when a user has no data to transmit,
“blank" data must be sent in order to prevent the other time slots from becoming
out of sync.

CDMA makes a much more efficient use of the available bandwidth, and due to this
improved efficiency allows for a higher number of subscribers per cell compared
to GSM (as much as five times as many) and also enables data rates of up to 14.4
kbps.

By increasing the bandwidth available to a single channel, the harmful effects
of interference (either deliberate or inadvertent) are reduced giving rise to
high signal quality.

The choice of frequency is not entirely random — if it were the two parties
involved in communication would have no hope of remaining in contact with each
other. They employ what is known as a pseudo-random algorithm to decide what
frequency-hopping pattern will be used for the duration of the communication.
The algorithm is agreed upon during the initial channel setup negotiations.

The code in use may be different in each cell, so this negotiation needs to be
conducted during the hand-off procedure each time the mobile device moves from
cell to another.

Security is not the principle reason for adopting this system for use in public
networks, although it is an added bonus. The main reason is the shortage of
available frequency spectrum. Spread spectrum gives engineers a way to fit
mobile devices into existing spectrum without jamming the devices already using
it. Assume a phone is transmitting at 1 watt, but is hopping between dozens or
hundreds of channels very rapidly. Other devices don't "see" the phone because

it is transmitting for only a fraction of a second on any channel. Therefore,
the average perceived power on any given channel is extremely low, and other
devices using that channel don't even notice it. The phone creates the
equivalent of a low-power noise pattern across all of the channels it uses.
Other devices deal with noise already, so the phone is essentially invisible to
devices using specific channels.

This system has several other benefits:

>   increased network capacity

>   Less interference meaning higher audio quality

>   Increased reliability for data connections

>   Reduced power output meaning less interference caused to other devices

>   Reduced health risks

The main benefit to network operators, therefore, is increased capacity. Whereas
with conventional FDMA (Frequency Division Multiple Access), networks re-use
frequencies in a 7-cell pattern ensuring that no two abutting cells use the same
frequency:

With CDMA each user is assigned a different instance of the carrier wave, a
different phase of the wave:

Each cell therefore uses the entire passband.

The CDMA systems currently in use in the world today are referred to as cdmaOne
systems — they are the first generation of CDMA systems (but remember that CDMA
itself is still a 2G technology). HSCSD (High Speed Circuit-Switched Data)

Orange have implemented, in addition to a GPRS service, another 2.56 system.
HSCSD offers data rates of up to 28.8 kbps by offering subscribers more than one
timeslot.

By reducing the amount of error correction involved, the amount of redundant
data transmitted is reduced so users get an effective data throughput of 14.4
kbps as opposed to 9.6 kbps.

By allocating subscribers two time slots, they can establish a connection to the
Internet at 28.8 kbps.

### Multi-slot operation

As incoming and outgoing data is transmitted on different frequencies, it is
possible to operate different “multi-slot schemes". This enables the operator to
“weight" either the sending or receiving bandwidth to suit the subscriber’s
application. For example, a 3-down 1-up scheme would suit web browsing as very
little data is ever uploaded.

GPRS (General Packet Radio Service)
===================================

GPRS is a 2.5G technology implemented by GSM network operators to increase
network capacity (to service the growth in the number of subscribers), to
provide higher data rates for subscribers (to address the increased need for
high-speed connections to the Internet and corporate networks) and also as an
intermediate step on the path towards 3G systems.

GPRS uses the same physical network infrastructure and is designed to coexist
with GSM. The air interface is still based on TDMA structure, with subscribers
being allocated one or more “timeslots”.

A conventional GSM device is either on or off. When it is on, it is attached to
the network and the HLR tracks the device to know which Base Station is it
registered on.

When in a call, a conventional GSM device maintains a dedicated circuit from
endpoint to endpoint. The mobile station is then the sole user of a timeslot.
When no data is being sent, the network resources are still consumed.

GPRS is an extension of GSM. Instead of requiring a phone number to be dialed
and a permanent circuit to be created until the user disconnects, GPRS is
packet-based. When the mobile station registers on the network, it is assigned
an IP address that enables data to be routed to it by other nodes on the
network. The mobile station is then able to send and receive data almost
immediately. This is referred to as an “always on” connection.

GPRS has a number of advantages over CSD dial-up Internet connections:

>   1. Unlike CSD, no 20 to 30 second delay is required to establish a
>   connection. The GPRS terminal is able to exchange data with the network at a
>   moment’s notice. This is referred to as being “always on”.

>   2. The network operator can monitor how many data “packets" are exchanged
>   with the subscriber’s terminal, enabling a billing system based on the
>   amount of data transferred rather than the length of a call.

>   3. As radio resources are only used when the subscriber is sending or
>   receiving data, this more efficient use of the resources enables more
>   subscribers to be serviced by a single cell.

>   4. The fourth benefit is a greater data transfer rate.

The mobile station still registers on the network in the same way as before, and
the location of the terminal is still tracked by the HLR. GPRS does require that
additional components be added to the network, however.

Each Base Station is upgraded with a Packet Control Unit (PCU) which determines
whether the data being sent by the mobile station is voice traffic, CSD traffic
or GPRS trafﬁc, and then routes it accordingly.

GPRS data traffic is then sent to a Serving GPRS Support Node (SGSN) which works
in a similar way to the MSC (Mobile Switching Center). Its main function is that
of a router, examining the destination address of each packet and routing it
according to the information held in its routing tables.

Data that is destined for a node that is not on the same network is routed to
the Gateway GPRS Support Node (GGSN). The gateway between the mobile network and
the wired Internet is also known as the Access Point Node (APN), which also
records billing information.

Because GPRS uses Internet technology to identify nodes on the network and in
the transmission of data, it makes it relatively simple to integrate with other
Packet Data Networks (PDNs) such as the Internet and corporate networks.

Handset classes
---------------

GPRS mobile stations can fall into one of three categories.

Class A handsets will support simultaneous attach, activation, monitoring,
invocation and data transfer on both GPRS and GSM modes. Calls can be made or
received on both services. This is the most sophisticated class of device.

Class B handsets will be able to attach to, activate and monitor both GSM and
GPRS services simultaneously but only be able to support data transfer on one at
a time. Active GPRS “virtual circuits" are not cleared down when the GSM
facilities are in use, but any attempt to contact the device’s assigned IP
address will result in a busy or “held” indication.

For instance, if an incoming GSM call is answered, the GPRS connection is put on
hold. This may cause problems for some applications, eg a file transfer may be
aborted because the transfer protocol timeout expired.

Class C handsets will only be able to operate exclusively in one mode at a time.
Manually changing from one mode to another involves detachment from the
previously selected service.

GPRS Attach
-----------

When you activate your GPRS terminal, the nearest SGSN will assign the device an
available iP address automatically. This process is known as the GPRS Attach.
Once attached, GPRS connectivity is provided by a “context”. To be able to send
and receive data, you will need to enter the address of the Access Point

Node, or GGSN. This is the APN which all of you will be familiar with and is
usually a "friendly" text phrase.

The HLR will then track what SGSN the mobile is attached to, rather than what
BTS it is physically communicating with, to be able to route incoming data
packets correctly.

For devices capable of supporting simultaneous GSM and GPRS attach (Class A and
Class B), the device may be registered on different cells for GSM and GPRS as
the network sees ﬁt.

PDP Context Activation
----------------------

In order for the user to be able to transfer data, a Packet Data Protocol (PDP)
Context must be activated in the MS, SGSN and GGSN. The user initiates this
procedure, which is similar to logging on to the required destination network.
The process is described below.

>   1. The user initiates the logging on process, using an application on the PC
>   or MS.

>   2. The MS requests sufficient radio resources to support the Context
>   Activation procedure.

>   3. Once the radio resources are allocated, the Mobile Station sends the
>   Activate PDP context request to the SGSN. This signalling message includes
>   key information about the user's static IP address (if applicable), the QoS
>   requested for this context, the APN of the external network to which
>   connectivity is requested, the user's identity and any necessary lP
>   configuration parameters (e.g. for security reasons).

>   4. After receiving the Activate PDP context message, the SGSN checks the
>   user's subscription record to establish whether the request is valid.

>   5. If the request is valid, the SGSN sends a query containing the requested
>   APN to the DNS server.

>   6. The DNS server uses the APN to determine the IP address of at least one
>   GGSN that will provide the required connectivity to the external network.
>   The GGSN IP address is returned to the SGSN.

>   7. The SGSN uses the GGSN IP address to request a connection tunnel to the
>   GGSN.

>   8. Upon receiving this request the GGSN completes the establishment of the
>   tunnel and returns an IP address to be conveyed to the MS. The GGSN
>   associates the tunnel with the required external network connection.

Once this procedure is completed, a virtual connection is established between
the MS and the GGSN. The GGSN also has an association between the tunnel and the
physical interface to the external network. Data transfer can now take place
between the MS and the external network.

### GPRS Context Deactivation and Detach

GPRS provides two additional, independent, procedures that enable a PDP context
to be deactivated and the MS to disassociate itself, or detach from the network.
GPRS detach can be performed when:

\- the MS is powered off

\- the user wishes to detach from the GPRS network, but wants to remain
connected to the GSM network for circuit switched voice services.

What happens to an incoming voice call during a GPRS data session?

GPRS Class B mobiles can be attached to both the GPRS and GSM networks, but they
cannot transmit or receive on both simultaneously. If a mobile is in an active
GPRS data session when an incoming voice call is detected, the user will be
notified by an on—screen message, and then has the option to suspend the data
session and accept the call, or continue with the data session and reject the
call.

### GPRS Coding Schemes

GPRS is specified on paper to achieve a maximum performance of up to 171 kbps.
Realistically network performance will vary from anywhere between 9 to 50 kbps.

Network operators can choose to implement one of four different “coding schemes"
to attain higher rates of data throughput.

in all data communications user data is broken down into smaller units called
“packets”. A packet is comprised of the data itself (the “payload"), the address
of the source machine and destination machine, and also “redundancy”, bits used
for error correction to ensure that the packet has been received successfully.

With a CSD connection this redundancy can account for 50% of a data packet.

GPRS reduces the amount of redundancy used to increase the amount of user data
that can be included in each packet.

Different coding schemes reduce this level of error correction to different
degrees, thus making the connection less reliable, but increasing the data
throughput.

The four schemes are described in the table below:

[table]

Due to the requirement for reliable connections, networks will typically
implement either coding scheme 1 or 2.

Due to the allocation of radio resources by the network, GPRS speeds will
fluctuate throughout the day. The more voice subscribers there are on the
network, the fewer the resources available for GPRS data. This is due to the way
in which networks use spare network capacity to accommodate GPRS traffic.

It must be remembered that whilst GPRS is suitable for “bursty” applications
such as web browsing, it is not suited to all applications, such as video
streaming, which require constant connections.

GPRS Multi-slot Operation
-------------------------

As with an HSCSD connection, GPRS is able to use more than one time slot to
increase data throughput. The speed of each time slot will depend on the coding
scheme used by the network.

As incoming and outgoing data is transmitted on different frequencies, it is
possible to operate different multi—slot schemes. This enables the operator to
“weight" either the sending or receiving bandwidth to suit the subscriber's
application. For example, a 3-down 1-up scheme would suit web browsing as very
little data is ever uploaded.

CDPD (Cellular Digital Packet Data)
-----------------------------------

CDPD is an AMPS overlay in much the same way that GPRS is an extension to GSM.

Although AMPS is itself an analogue technology, CDPD itself is entirely digital,
sharing frequency with the AMPS system but only transmitting in the blank spaces
between AMPS voice calls.

Substantial amounts of data can be transmitted by hopping from one "gap" to
another harnessing the otherwise unused channel capacity.

This channel hopping occurs completely transparently to the user.

CDPD data traffic was originally expected to be infrequent bursts of data
typical of telemetry or credit-authorization applications. it quickly became
apparent that during busy periods there were few if any channels available for
CDPD to switch to and system became "blocked”. The carriers resolved this
problem by offering dedicated channels for data traffic.

CDPD offers data rates of up to 19.2 Kbps. Error control overhead means that
actual throughput of useful data can be up to 12 Kbps on a clean, lightly—loaded
channel. Data transfer rates may be lower on congested networks with many voice
or CDPD transmissions underway.

Like any data communications system, each CDPD data link has a maximum capacity
it can support. The maximum 19.2 kbps throughput of CDPD limits the amount of
data that can be sent over the channel. On average, if a user has an application
that requires 5% of this maximum channel capacity, then one radio channel data
link can support 20 users. On average, if a user has an application that
requires 1% of this maximum channel capacity (not unusual, especially for light
Internet connectivity), then a single radio channel data link can support 100
Users.

The nature of most user data applications is that the amount of data sent is
small (a few frames) and the rate at which the data is sent is bursty (short
periods of activity, followed by long periods of idle time). As a result, a CDPD
radio channel data link can support a large number of users at one time. The
maximum number of users that can be supported on a single radio channel data
link depends on the nature of the data traffic that the users’ applications
send.

Like wired Ethernet networks, CDPD is a “contention-based” system. It uses
DSMA—CD (digital sense, multiple access, collision detect), while Ethernet uses
CSMA—CD (carrier sense, multiple access, collision detect). in both systems,
when a device has data to send it senses the transmit medium to determine if it
is currently busy. If not, it will send its data. If it is, it will wait a
random interval before trying again.

Carrier Sense Multiple Access / Collision Detection (CSMA/CD)
-------------------------------------------------------------

Let‘s represent our radio channel as a dinner table, and let several people
engaged in polite conversation at the table represent the mobile devices trying
to use it. The term Multiple Access means that when one mobile station
transmits, all the stations on the medium hear the transmission, just as when
one person at the table talks, everyone present is able to hear him or her.

Now let's imagine that you are at the table and you have something you would
like to say. At the moment, however, I am talking. Since this is a polite
conversation, rather than immediately speak up and interrupt, you would wait
until I finished talking before making your statement. This is the same concept
described in the Ethernet protocol as Carrier Sense. Before a station transmits,
it "listens" to the medium to determine if another station is transmitting. If
the medium is quiet, the station recognizes that this is an appropriate time to
transmit.

Carrier Sense Multiple Access gives us a good start in regulating our
conversation, but there is one scenario we still need to address. Let's go back
to our dinner table analogy and imagine that there is a momentary lull in the
conversation. You and I both have something we would like to add, and we both
"sense the carrier" based on the silence, so we begin speaking at approximately
the same time. In network terminology, a collision occurs when we both spoke at
once.

In our conversation, we can handle this situation gracefully. We both hear the
other speak at the same time we are speaking, so we can stop to give the other
person a chance to go on. Mobile stations also listen to the medium while they
transmit to ensure that they are the only station transmitting at that time. If
the stations hear their own transmission returning in a garbled form, as would
happen if some other station had begun to transmit its own message at the same
time, then they know that a collision occurred. A single Ethernet segment is
sometimes called a collision domain because no two stations on the segment can
transmit at the same time without causing a collision. When stations detect a
collision, they cease transmission, wait a random amount of time, and attempt to
transmit when they again detect silence on the medium.

The random pause and retry is an important part of the protocol. If two stations
collide when transmitting once, then both will need to transmit again. At the
next appropriate chance to transmit, both stations involved with the previous
collision will have data ready to transmit. If they transmitted again at the
first opportunity, they would most likely collide again and again indefinitely.
Instead, the random delay makes it unlikely that any two stations will collide
more than a few times in a row.

Network infrastructure
----------------------

To implement a CDPD service, AMPS operators need to upgrade the Base Stations to
Mobile Data Base Stations (MDBS). These perform the same function as the Packet
Control Units in a GPRS system: they examine the data and determine where it
needs to be sent. A series of Mobile Data intermediate Stations (MD-IS) control
a number of base stations and are responsible for managing access to the radio
channel and channel hopping. They are also responsible for forward error
correction.

CDPD is a packet-based system and uses Internet addressing schemes. As with
GPRS, when the mobile station is activated, it will “attach" to the network and
assigned an IP address by the network.

Introduction to 3G
==================

Third Generation (3G) systems represent the future of mobile communications.
Conservative estimates tentatively suggest that 3G services represent a
cumulative revenue potential of one trillion dollars for mobile services
providers between now and 2010.

A total of 2.25 billion mobile subscribers is forecast for 2010, of which more
than 600 million will be 3G subscribers.

The goal of SG is to provide a truly global, compatible, cellular radio network.
All of the current disparate technologies will adopt a common air interface
providing greater network capacity, higher data rates, information-based
services and global roaming.

This section will examine the services that operators are likely to offer and
how they will be billed. I will look at the technologies that 3G will employ to
attain these higher data rates, and also the steps that operators will need to
take to evolve from the current 2.5G systems.

3G Services
-----------

3G services are likely to fall into one of three business models: the Access-
focused, Portal-focused and Mobile Specialised Services approaches.

The six service categories that represent the major areas of demand for
3G-enabled services over the next 10 years can defined as follows:

>   \* Customized Infotainment

>   \* Multimedia Messaging Service

>   \* Mobile Intranet / Extranet Access

>   \* Mobile Internet Access

>   \* Location-based Services

>   \* Rich Voice (simple and enhanced voice)

The figure below shows how the UMTS Forum has defined SG-based mobile service
categories:

[image]

By 2010, 28 per cent of the world’s 2.25 billion mobile cellular subscribers
will be 3G subscribers. Figure 2 shows the worldwide revenues forecast from all
3G services for the 2001-2010 timeframe

Forecasts predict that total service provider-retained revenues for 3G services
in 2010 will reach US\$322 billion. Of those revenues, 66 percent will come from
3G-enabled data services. The cumulative revenue potential for mobile services
providers between now and 2010 is over one trillion dollars. The consumer
segment will contribute about 65 percent — a true mass-market success.

Customized Infotainment is the earliest and single largest revenue opportunity
by virtue of its low cost and mass-market appeal, contributing US\$86 billion in
2010.

As services mature, prices will decline. Services like email and Web browsing
are likely to have little or no direct revenue potential; as users will expect
them to be included as part of their service package at no additional charge.
Their benefit comes mainly from their role as drivers of traffic. Much of the
additional revenue generated by 3G services and applications will come through
increased usage rather than through new sources of revenue.

3G is about services, not technology. Services will be interesting, easy to use,
reliable, affordable (with transparent charging) and ubiquitous. Customers do
not buy “bytes”, bit rates or technology, only content and services which are
interesting on an individual/personalized basis, and which are affordable by
“adding value" to lifestyles. Users will expect seamless anytime/anywhere access
to voice, data, the Internet and multimedia services on a global basis.

Revenue is likely to be shared by the mobile operator and third party
application developers and content providers. There are three likely business
models:

1.) The Access-focused services provider offers mobile and lP network access.
Users are “Internet-experienced” and prefer to browse directly to the sites they
want. Likely service categories offered are Mobile Internet Access (consumer)
and Mobile Intranet / Extranet Access (business). Revenue comes only from ISP
subscription and airtime.

2.) The Portal-focused services provider offers access to the mobile IP networks
and selected partner content, all via a mobile portal. Users are subscribers to
voice mobile services, and are not expert Internet users. They appreciate easy
access to content that is tailored to their preferences and interests. Revenue
sources include subscription, airtime, transaction fees and advertising. The
likely service category offered is Customized Infotainment (consumer).

3.) The Mobile Specialized Services services provider offers specialized
service— sets targeted to a specific market. Revenue sources include airtime,
messages, subscription, advertising, transaction and messaging fees. Likely
service categories offered are Multimedia Messaging, aimed at a particular
demographic group, and Location-based Services.

Positioning 3G as the “Internet made mobile” sends the wrong message to the
market and paints an incomplete picture of 3G service potential. The advantages
of mobility, personalisation, location capability and other features of 3G
services rather than speed should be highlighted. Customer expectations should
be managed; current fixed Internet subscribers will expect 3G services providers
to offer a full, high-speed, low-cost Internet experience. 3G will replace some
high-speed fixed Internet access, but will never be a complete substitute.

What is a mobile portal?
------------------------

Portals will be critical to the end user’s experience of 3G services. Although
superficially similar to the familiar portals of today’s fixed Internet, 3G
mobile portals exhibit signiﬁcant differences in character due to the additional
challenges of content optimization for small form factor devices, and the
necessity of delivering that content to the mobile user.

Defined as “an entry point to a wealth of information and value added services”,
a portal is Internet/Intranet based with a browser-based interface, and can be
personalised, delivering content according to a device‘s characteristics and a
user’s needs.

Content and services provided by these portals can be considered as falling into
six main categories: '

>   \* Communications and community — email, calendar and chat

>   \* information — news, weather, directories

>   \* Lifestyle — listings of events, restaurants, movies and games

>   \* Travel — hotel listings, direction assistance and timetables

>   \* Transaction -— banking, stock trading, purchasing and auctions

>   \* Other — information about personalization, location- based services,
>   device type and advertising

There are five likely types of 3G mobile portal:

\* Mobile Intranet / Extranet portal — a 3G portal that provides secure mobile
access to corporate LANs, VPNs and the Internet. Typical services include
corporate email, calendar, training and customer relationship management tools

\* Customized Infotainment portal — a 3G portal that provides device—
independent access to personalized content anytime anywhere. Typical services
include streaming music, short film/video clips and m—commerce applications.

\* Multimedia Messaging Services portal — a 3G portal that offers non-real—time,
multimedia message access allowing the provision of third party content.
Examples of typical services include multimedia postcards, video clips and movie
trailers.

\* Mobile Internet portal — a 3G portal that offers mobile access to content
services with near-wireline transmission quality and functionality. Typical
services include browsing, gaming and m-business.

\* Location—based Services portal — a business and consumer 3G portal that
enables users to find other people, vehicles, resources, services or machines.
It also enables others to find users, as well as enabling users to identify
their own location via terminal or vehicle identification. Typical services
include emergency services, asset tracking, navigation and localized shopping
information.

Implications of 3G
------------------

Insufficient battery life and power consumption issues will impede the ability
of 3G terminals to deliver the full potential of mobile multimedia services in
the short term. Research needs to be intensified into the area of battery
technology.

A number of other significant technical issues will impact on the delivery and
market acceptance of BG portal services. Perhaps the most contentious of these
is security, where a current lack of privacy policies and open-standard
solutions, coupled with uncertainty over strategies for dealing with mobile junk
mail (spam) and viruses, presents several major challenges to industry.

Another area where there is a current lack of clear industry coordination is
billing, charging and payment: while the financial industry and manufacturers
are working to integrate micropayment technology into mobile devices,
uncertainty remains over which billing systems will be accepted by end users and
timescales to bring solutions to market. This issue will be of even greater
relevance to 3G due to the large amounts of network traffic that will be
generated by multimedia messaging and file transfers.

Demand for 36 mobile data services is real. Users have consistently demonstrated
strong interest in trying new services that combine mobility with content and
personalisation. The strong growth in SMS has clearly demonstrated market
acceptance for messaging which has now evolved into Multimedia Messaging.

Billing
-------

The move to 3G demands a fundamental shift in the manner in which mobile
services and applications are billed, and will require a revolutionary
transition from the operators’ perspective. Issues such as who supplies what to
whom, and the fragmented segmentation of next-generation end-users will also
mark a significant departure from the traditional mobile billing model. Billing
systems that evolve to support 3G services will have to cater for a wide range
of events and services, including revenue sharing and the apportionment of
revenues to third parties.

The ability to bundle services will help to promote service personalization by
allowing customers to tailor packages and encourage experimentation, for
example, providing a movie package that allows, say, four films per month, a
daily personalized news service and local cinema information. A premium would
then be charged on top of the fixed fee for access to the latest film releases.
Alternatively, the number of pages used or service duration on an ongoing basis
could dictate fees. Again, the consumer chooses the service package to suit
personal preferences and lifestyle.

Initial experience with packet-based services shows there are three basic
components to the billing model:

>   \* A fixed monthly data services subscription

>   \* A fixed service or content area subscription

>   \* A variable data traffic fee

EDGE (Enhanced Data rates for Global Evolution)
===============================================

EDGE is a third generation (3G) system which proposes to offer a maximum data
throughput of 473.6 Kbps (but as with all systems this is an absolute maximum
and will not be attainable in a commercial system).

EDGE manages to offer such high data rates by using complex modulation
techniques and a greater frequency bandwidth than current cellular radio
systems.

The main benefit to EDGE, however, is that it can be upgraded to from all of the
current 2.5G systems, offering the possibility of a single compatible global
mobile network, comprised of individual network operators.

As is to be expected with such an area of research, there are a lot of acronyms
in use — many different countries using different acronyms to refer to the same
thing.

The theoretical global mobile network is referred to as the Universal
Terrestrial Radio Access Network (UTRAN). All 3G technologies and concepts fall
under the umbrella term UMTS (Universal Mobile Telecommunications System).

EDGE is also sometimes referred as EGPRS (Extended GPRS).

The principles of data transmission — a refresher

As you will remember from the introduction, data is sent over an analogue radio
wave by altering the characteristics of that wave for a brief instant in such a
way so that the receiving device can interpret from that change a value of
either 1 or 0.

This technique is known as modulation. The most commonly used forms of
modulation involve altering either the wave’s amplitude or frequency.

The more 1’s and 0’s that can be interpreted from a single change will mean a
higher data rate.

Modem speed is referred to in terms of baud rate. Technically a baud rate is the
number of voltage or frequency changes the device can complete in a second. When
a modem is said to be operating at 300 baud, this means that the carrier wave
has 300 cycles per second.

Real modem speed, however, is measured in bits per second. If each cycle
(frequency change) can convey one bit (a value of either 1 or 0), then a modem
operating at 300 baud can transmit 300 bits per second.

However, if a modulation technique is used that sends two bits per baud, then
the modem will be able to transmit 600 bits per second.

Landline networks use a variant of amplitude modulation called Quadrature
Amplitude Modulation (QAM) which sends two bits per baud. As all bits can only
have 2 possible values, it follows that 2 bit groups can only have 4 possible
values:

00

01

10

11

QAM works by altering the amplitude of the carrier wave by one of 4 degrees,
each degree corresponding to one of the four possible values:

However, due to the susceptibility of radio waves to interference, this method
is not suited to cellular radio systems. GSM uses a technique known as Gaussian
Minimum Shift Keying (GMSK) which combines frequency modulation with encryption
technology.

EDGE uses a different modulation technique that alters the phase of the wave.
This is known as Phase—Shift Keying (PSK).

Waves have relative starting points, or phases. Two identical waves, which have
different starting points, are said to be out of phase. If you’re interested in
Hi-Fi you may have spent a considerable amount of time ensuring that your
speakers are positioned so that the stereo image is focused on your normal
listening position, ie to ensure that when the two sound waves from the speakers
converge at the listening point they are in phase ..... or maybe it’s just me!

This change in phase can be used to send data. If you treat one wave as having a
value of 0, then any wave that is out of phase can represent a value of 1.

EDGE uses a 4 phase shift pattern. This means that as QAM sends 2 bits per baud
by using 4 waves of differing amplitude, so is EDGE able to send 2 bits per baud
by using 4 waves of differing phases:

If the shift pattern is increased to 8 then 3 bits will be sent per baud and the
rate of data throughput will be correspondingly higher. However, due to the
reliance of EDGE on signal quality, the level of error correction necessary to
guarantee an acceptable level of reliability means the net data throughput will
not be a direct 3 to 1 ratio. EDGE has the ability to tailor the coding scheme
used depending on the quality of the radio signal. In poor radio conditions,
robust coding schemes are selected, whereas in good radio conditions less robust
coding schemes are employed.

Network infrastructure
----------------------

In terms of network infrastructure, little needs to be changed to upgrade an
existing GSM/GPRS system to EDGE. The principle difference is that the air
interface is changed. Instead of the FDMA / TDMA system, CDMA is employed
instead. As discussed above, CDMA uses the available spectrum very efficiently
and so can offer higher capacity and higher rates of data throughput than GSM.
Using this air interface in conjunction with improved modulation techniques
means that data rates will be increased many-fold.

EDGE will use a wider frequency range than is currently allocated to GSM/GPRS
network operators. This additional frequency was auctioned in European countries
not so long ago — you may recall the press coverage. The vast sums paid by
network operators for this frequency almost bankrupted some of them, and returns
on this investment will be slow in materializing. Due to the incompatibility of
CDMA with existing GSM systems, new handsets will need to be developed and the
two systems will need to be run side-by-side for some time.

This use of CDMA technology over a larger frequency range is referred to as
Wideband CDMA (W-CDMA).

Globalization
=============

The key to creating a globally-compatible internetwork of mobile cellular
networks is the adoption of W-CDMA as a common air interface.

GSM is already the leading cellular standard worldwide and the evolution from
cdmaOne networks to W-CDMA is not a difficult one. If all GSM, cdmaOne and

AMPS networks adopt W-CDMA as their air interface then a truly global network
will be possible.

WAP — Wireless Application Protocol
===================================

http://www.wapforum.org

You may remember adverts on television urging viewers to “surf the net, surf the
BT Cellnet."

The service being advertised was WAP. WAP stands for Wireless Application
Protocol and enables WAP-capable handsets to retrieve and display web pages in a
micro browser.

WAP was first launched in 1999. Whilst the benefits and possibilities offered by
WAP are many, due to the relatively arduous setup procedure, the slow connection
speed and the limitations of phone displays, uptake was slow.

Before GPRS was launched, WAP connections were expensive as users would often
forget that they were paying for a call once connected and would leave their
connections open.

With the advent of GPRS and billing based on data transfer rather than call
duration, WAP is coming into its own. OTA (Over The Air) provisioning tools such
as Mouse2Mobile make the process of setting up the connection painless and even
completely transparent to the user.

The companies who have joined the WAP Forum, companies such as Nokia and
Ericsson, indicate the seriousness with which the technology is being taken, and
it is not an exaggeration to say that all phones manufactured today contain a
micro browser and are therefore WAP-capable. The adoption of the standard by all
major handset manufacturers allows manufacturers, network operators, content
providers and application developers to offer compatible products and services
that work across varying types of digital services and networks.

The mobile phone is evolving into much more than a wireless telephone; it is
becoming a portable data terminal. However, whilst WAP delivers web content to a
subscriber’s mobile device, due to the limitations of phone displays and
processing power, it is important to think of WAP in realistic terms. WAP uses
Internet technology to deliver content, but in terms of user experience it is
far from the multimedia extravaganza that the Internet is rapidly becoming.

Let us look briefly at typical Internet architecture:

When a user enters the address of a web page in their browser, an HTTP
(HyperText Transfer Protocol) GET request is sent to the appropriate server,
which delivers the HTML page itself to the user, as well as any graphics, videos
or sound files that the HTML code links to.

WAP is an air link-independent protocol, meaning it can use CSD, HSCSD, GPRS or
even SMS bearer services. There are a number of network components a network
operator must install, however, to provide a WAP service.

The most significant component in the WAP solution is the WAP gateway. The WAP
gateway is in essence the route through which all data must pass on its route
from the Internet to the mobile phone. The gateway effectively condenses the
protocols and languages of the Internet into the more condensed and efficient
protocols used in WAP.

For example, the transport protocol used by the Internet is TCP. TCP offers the
significant benefit of being able to check for errors during data delivery and
can retransmit data should it get lost en route.

This constant signaling back and forth acknowledging the receipt of data
constitutes a significant amount of data that the low bandwidth-capability of
GSM cannot afford. Hence the equivalent protocol in the WAP environment is WDP.

Another significant thing that the WAP gateway does is to condense the
information that is passing through it, in order to optimize the connection
speed.

Whereas conventional web pages are written in HTML (HyperText Markup Language),
WAP pages are written in their own text-based language called WML (Wireless
Markup Language). However, being text based it is not efficient. The WAP gateway
also compiles the text code into binary information, making it considerably
smaller and therefore faster.

WML can only display text, bullets and simple black and white graphics. Although
for many applications much more is not required as a great deal of information
can be portrayed with just this simple level of display ﬂexibility.

### Security for WAP

With the ever-increasing reliance on the secure transmission of data over the
Internet, the WAP standard was revised to include support for encryption and
public key technology.

Version 1.2.1 of the WAP standard includes support for secure data transfer for
applications such as WAP banking and online shopping.

Based on SSL technology, WAP uses WTLS (Wireless Transport Layer Security) and
is supported in nearly all handsets today.

I don’t intend to go into too much detail about how encryption works, see my
guide to encryption, authentication and digital certiﬁcates for more information
on this subject.

Brieﬂy, to ensure that data being transmitted over the Internet cannot be
intercepted; it is encrypted using a key. Only someone in possession of the same
key can decrypt the information.

To ensure that the encrypted data is coming from a trusted source, the encrypted
information is then digitally signed using a unique identifier which is verified
as being authentic by a third party certificate authority.

There are three modes of operation of WTLS, corresponding to three ascending
levels of authentication. Class \| WTLS includes no authentication, Class ll
WTLS includes authentication of the server to the client, and Class III includes
mutual authentication, ie authentication of both the server and the client to
each other.

### The Gap in WAP

The bridging of two secure connections at the WAP Gateway, where the WAP Gateway
is run by the network operator, has been termed “the gap in WAP" by some media,
since the data is decrypted for a brief period at the WAP Gateway.

This can be addressed if the content originator operates its own gateway, so
that WTLS-encrypted packets can be routed through or around the wireless
carrier's gateway. If this is not the case then another solution is
application-level security.

### Application-level security

The data created by an application can be encrypted before it is packaged for
transmission over the Internet and further encrypted.

WML provides support for application—level security via the use of its SignText
function, which allows the creation of digital certificates at the application
level.

Introduction to Computer Basics
===============================

To compute is to ascertain the value of a number by calculation. The earliest
form of the computer is the abacus which was first invented by the Chinese over
2500 years ago and is still in use today.

The evolution of the computer
-----------------------------

The computer has evolved from very humble beginnings to the power processing
engines of today.

The first mechanical computer was the analytical engine, conceived and partially
constructed by Charles Babbage in London between 1822 and 1871. It was designed
to receive instructions from punched cards, make calculations with the aid of a
memory bank, and print out solutions to mathematical problems. However the
thousands of moving parts required for its construction was beyond the technical
expertise of the day.

The first electrically-driven computer designed expressly for data processing
was patented on January 8, 1889, by Dr. Herman Hollerith of New York. The
prototype model was built for the U. S. Census Bureau and computed results in
the 1890 census.

Using punched cards containing information submitted by respondents to the
census questionnaire, the Hollerith machine made instant tabulations from
electrical impulses actuated by each hole. It then printed out the processed
data on tape. Dr. Hollerith left the Census Bureau in 1896 to establish the
Tabulating Machine Company to manufacture and sell his equipment. The company
eventually became IBM.

The first modern digital computer, the ABC (Atanasoff—Berry Computer), was built
in a basement on the Iowa State University campus in Ames, Iowa, between 1939
and 1942. The development team was led by John Atanasoff, professor of physics
and mathematics, and Clifford Berry, a graduate student. This machine included
many features still in use today: binary arithmetic, parallel processing,
regenerative memory, separate memory, and computer functions. When completed, it
weighed 750 pounds and could store 3000 bits (0.4 KB) of data.

The 1960s and 1970s heralded the era of the mainframe computer using technology
from the original ABC computer.

The first true personal computer with its own operating system arrived on the
scene in 1977 in the form of the Apple II.

Microsoft released Windows 3.11 in 1992 and the rest is history.

Computer communications
-----------------------

Telegraphs and early radio communication used codes for transmissions. The most
common, Morse code (named after its creator, Samuel F. B. Morse), is based on
assigning a series of pulses to represent each letter of the alphabet. These
pulses are sent over a wire in a series. The operator on the receiving end
converts the code back into letters and words.

Morse notation uses two values - a dot or a dash. By stringing these values
together the telegraph operator can form letters and hence entire messages.

This is a form of binary notation. Computers also use binary notation, but
rather than deal with dots and dashes they deal with 13 and Os.

Computers are essentially very complicated switch boxes, each switch having only
two possible states — on or off. A switch that is on has value of 1, and a
switch that is off has a value of 0.

All information processed by a computer is a string of 1s and Us.

The smallest piece of information used by a computer is a bit — a single 1 or O.

A byte is a group of 8 bits. A byte is required to signify one character of
information and can represent up to 256 different characters.

Parallel and Serial communications
----------------------------------

The wires in a computer only allow one unit of data to pass through at a time,
much like a tunnel that only allows one person to walk down it at a time.

To speed things up we can add more wires to allow more data to be sent at the
same time. In early PCs eight wires are often together to allow a byte to
transmitted. These eight parallel wires are called a bus.

Buses in modern computers are normally either 32 or 64-bits wide allowing for
the simultaneous transmission of more data.

There are several different types of bus on a computer. The term bus refers
simply to the wires connecting the different parts of a computer to each other.
A modern computer has several different buses which may different in width.

An overview of computing
========================

Despite the apparent complexity of computers, all aspects of a PCs functionality
and its components fall into one or sometimes two categories:

Input

Processing

Output

[image]

Input refers to any device that enters information into the computer from the
outside world, be it a keyboard, mouse, ﬂoppy disk, CD, DVD, scanner or modem
for example.

Processing refers to the manipulation of the inputted data by the computer.

Output refers to the presentation of the manipulated data, be it a monitor,
printer, CD—R, modem or sound card, for example.

Let’s look at the principle components of a modern PC

### Microprocessor

The microprocessor is an integrated circuit that contains the CPU on a single
chip.

### External Data Bus

I explained above that all data moves around a PC via a bus. The external data
bus (also referred to as simply the data bus) is the primary route for data in a
PC. All data-handling devices are connected to it so that any data placed on the
bus is available to all devices connected to the PC.

### Motherboard

The motherboard defines the computer's limits of speed, memory, and
expandability. A computer needs more than just a CPU and memory. To accept input
from the user, it needs devices, such as a keyboard and a mouse. it also needs
output devices, like monitors and sound cards, to cope with the powerful
graphics and sound capabilities of the programs available today. A computer also
needs "permanent" storage devices, such as floppy disk drives and hard disk
drives, to store data when it is turned off. It is the function of the
motherboard to provide the connectivity for all these devices.

There are two main types of motherboard — AT and ATX, the main difference
between them being the type of power supply used.

### CPU

The CPU is the part of a computer in which arithmetic and logical operations are
performed and instructions are decoded and executed. The CPU controls the
operation of the computer. Early PCs used several chips to handle the task. Some
functions are still handled by support chips, which are often referred to
collectively as a “chip set." The image below shows an Intel Pentium CPU:

[image]

The CPU handles two operations, which used to be performed by two separate
chips: the Control Unit (CU) and the Arithmetic Logic Unit (ALU). The ALU
handles the mathematical computations and the CU controls the flow of data.

The CPU uses Registers as temporary memory to store data while it is being
manipulated. A register is a row of switches which are set to either on or off.
The wider the register the higher the performance of the PC.

The chip set is often a collection of chips that come pre-soldered to the
motherboard (the image above shows a VIA chipset). When buying a processor it is
important to check that it will be compatible with the chipset of the
motherboard. A typical chipset will consist of:

>   Bus controller

>   Memory controller

>   Data and address buffer

>   Peripheral controller

### The BIOS

In addition to the chipset you will also find another chip pre-connected to the
motherboard — the ROM BIOS. A ROM BIOS chip contains data that specifies the
characteristics of hardware devices, such as memory and hard disk and floppy
disk drives, so the system can properly access them.

ROM (read-only memory) is a type of memory that stores data even when the main
computer power is off. This is necessary so that the system can access the data
it needs to start up. When stored in ROM, information that is required to start
and run the computer cannot be lost or changed.

BIOS chips can either be standalone and contain static information, or they can
be used in conjunction with CMOS chips which store user—definable settings which
the BIOS accesses. CMOS stands for Complementary Metal Oxide Semiconductor and
gets its name from the way it is manufactured rather than from the information
it holds.

The information contained in a CMOS chip will depend on the manufacturer.
Typically, CMOS contains at least the following information:

>   Floppy disk and hard disk drive types

>   CPU

>   RAM size

>   Date and time

>   Serial and parallel port information

>   Plug and Play information

>   Power Saving settings

The BIOS is accessible when the PC first boots — you will normally see a line on
the bottom of the screen prompting you to press F2 to enter Setup, or words to
that effect. A typical BIOS screen looks something like this:

[image]

### The Clock

Timing is essential in PC operations. Without some means of synchronization,
chaos would ensue. Timing allows the electronic devices in the computer to
coordinate and execute all internal commands in the proper order.

Timing is achieved by placing a special conductor in the CPU and pulsing it with
voltage. Each pulse of voltage received by this conductor is called a "clock
cycle." A” the switching activity in the computer occurs while the clock is
sending a pulse

The clock speed is measured in megahertz (MHz) and indicates how many commands
can be completed in two cycles. The process of adding two numbers together would
take about four commands (eight clock cycles). A computer running at 450 MHz can
do about 44 million simple calculations per second.

### Memory

The ability of the CPU to store data is very limited. To compensate additional
chips are installed with the sole purpose of storing the data that the CPU may
require. These chips are called random access memory (RAM). The term random
access is used because the CPU can place or retrieve bytes of information in or
from any RAM location at any time.

### Address Bus

The contents of RAM is constantly changing as the CPU uses portions of it to
hold data, manipulate it, then store the result of the calculation. It is
vitally important that the CPU knows what memory locations are being used by
what process and what memory is free to use. RAM is controlled by the Memory
Controller Chip which passes requests to and from the CPU via the Address Bus.

[image]

How Microprocessors work
------------------------

Let's look at a simple task: adding two numbers such as 2 and 2 together and
obtaining their sum (2 + 2 = 4). The CPU can do math problems very quickly, but
it requires several very quick steps to do it. Knowing how a CPU performs a
simple task will help you understand how developments in PC design have improved
PC performance.

When the user pushes a number key (in a program like Calculator, which can add
numbers), the keystroke causes the microprocessor's prefetch unit to ask for
instructions on what to do with the new data. The data is sent through the
address bus to the PC's RAM and is placed in the instruction cache, with a
reference code (let's call it 2 = a).

The prefetch unit obtains a copy of the code and sends it to the decode unit.
There it is translated into a string of binary code and routed to the control
unit and the data cache to tell them what to do with the instruction. The
control unit sends it to an address called "X" in the data cache to await the
next part of the process.

When the plus (+) key is pressed, the prefetch unit again asks the instruction
cache for instructions about what to do with the new data. The prefetch unit
translates the code and passes it to the control unit and data cache, which
alerts the ALU that an ADD function will be carried out. The process is repeated
when the user presses the "2" key.

Next (yes, there's still more to do), the control unit takes the code and sends
the actual ADD command to the ALU. The ALU sums "a" and "b" are added together
after they have been sent up from the data cache. The ALU sends the code for "4"
to be stored in an address register.

Pressing the equal sign (=) key is the last act the user must execute before
getting the answer, but the computer still has a good bit of work ahead of it.
The prefetch unit checks the instruction cache for help in dealing with the new
keystroke. The resulting instruction is stored, and a copy of the code is sent
to the decode unit for processing. There, the instruction is translated into
binary code and routed to the control unit. Now that the sum has (finally) been

computed, a print command retrieves the proper address, registers the contents,
and displays them. (That involves a separate flurry of activity in the display
system, which we won‘t worry about.)

### Virtual memory

Virtual memory is the art of using hard disk space to hold data not immediately
required by the processor; it is placed in and out of RAM as needed. Although
using virtual memory slows the system down as electronic RAM is much faster than
a mechanical hard drive.

### Real mode versus Protected mode

In simple terms if a processor is said to be running in real mode it will only
address the RAM, whereas a processor running in protected mode can access
physical (RAM) as well as virtual memory held in a “swap" or “paging” file on
the hard disk.

Power supplies
--------------

A standard power supply draws power from a local, alternating current (AC)
source (usually a wall outlet) and converts it to either 3.3 or 5 volts direct
current (DC), for on-board electronics, and 12 volts DC for motors and hard
drives. Most PC power supplies also provide the system's cooling and processor
fans that keep the machine from overheating.

There are two main types of power supply for the different types of motherboard
in production — AT and ATX.

The older AT-style power supply contained the on/off control and connected to
the motherboard through a pair of 6-wire connectors.

The ATX-style motherboard houses the on/off control itself and connects to the
power supply through a single 20-pin connector.

Power supplies are rated in terms of the wattage they can supply. It is
important to keep in mind that the power supply must produce at least enough
energy to operate all the components of the system at one time.

### Power supply problems

A role of the PC power supply is to “clean" the power supply as well as convert
the voltage. Domestic power supplies often have power surges that can damage the
delicate components within a PC. The power supply regulates the power supply so
that the PC always a regular feed.

Occasionally a power spike (a surge of very high voltage for a short period of
time) will damage (sometimes irreparable) PC components. Businesses therefore
often invest in a UPS, or Uninterruptible Power Supply. The USP is connected to
the mains and the PC to the UPS. The UPS serves both as a surge protector and
also as a backup battery in the event of a power failure.

Expansion bus
-------------

As discussed earlier, all devices on a PC are connected to the data bus to
enable them to send and receive data to and from the CPU.

Expansion sockets are standardised connectors that allow other devices to be
added to a PC that are not already soldered to the motherboard.

Expansion cards that are inserted into an expansion socket connect to the data
bus via the expansion bus.

To ease the processing burden on the CPU 3 lot of expansion cards have their own
smaller processors dedicated to specific functions. Because these need to run at
the same speed as the rest of the PC they often have jumper settings on them
which enables the user to set the speed of the card so that it matches the speed
of the CPU.

There are several different types of expansion socket in use today.

The first expansion socket was designed by IBM to enable third party
manufacturers to produce hardware for the standard IBM PC design. This was
called the ISA (Industry Standard Architecture) Bus. It used a 16-bit bus that
ran at a top speed of 8.33 MHz.

As CPU power increased the speed of this bus became a hindrance. ISA cards also
suffered from being difficult to configure and had a confusing array of jumpers
and switches on them.

This lead to the development of the MCA (Micro Channel Architecture) bus.

The MCA used a 32-bit bus and offered a higher speed of 10MHz. All MCA devices
also came with a software configuration utility so users did not have to fiddle
with hardware switches. However it was totally incompatible with the ISA design
and did not prove popular.

IBM responded to the MCA design with a redeveloped version of ISA — EISA
(Enhanced Industry Standard Architecture). EISA uses a 32-bit bus and is
compatible with older ISA cards.

EISA uses a variation of the ISA slot that accepts older ISA cards, with a two-
step design that uses a shallow set of pins to attach to ISA cards and a deeper
connection for attaching to EISA cards. In other words, ISA cards slip part-way
down into the socket; EISA cards seat farther down.

[image]

Today’s preferred expansion technology is PCI (Peripheral Component
Interconnect). It uses a 32-bit bus and operates at around 33 MHz.

PCl uses bus-mastering technology which enables it to regulate the ﬂow of data
over the data bus without the CPU’s involvement.

PCI devices are self-configuring which means users do not have to fiddle with
hardware switches or jumpers and PCI also allows for IRQ sharing. We will look
at IRQs in further details later on. Essentially each device on a PC has its own
address which it uses to communicate with the CPU. Normally an IRQ cannot be
shared as this leads to loss of information and crashes. PCI enables devices
that are not in use at the same time to share lRQs.

However, whilst PCI may seem like the ideal solution, it is important to bear in
mind that other manufacturers also use the PCI model, such as Apple. 80 whilst
an expansion card may physically fit in your PC, it does not mean it was
designed for use with that system.

Accelerated Graphics Port
-------------------------

With the advent of the GUI (Graphical User Interface) and 3D graphics the amount
of processing required to display these images soon meant that a new interface
had to be developed. The data bus could not handle all of the information being
generated by the graphics adapter, so Intel developed the AGP, or Accelerated
Graphics Port. This removed all data being generated by the display adapter from
the data bus and provided its own pipe directly into the CPU. It also provided a
direct path to the system memory for handling graphics. This process is known as
DME, or Direct Memory Execute.

Universal Serial Bus (USB)
--------------------------

The latest addition to the PC architecture is the Universal Serial Bus (USB). A
USB interface can be added to your PC via an expansion card but most
motherboards now have USB ports pre-soldered on them.

USB devices can be connected while the computer is running. Normally the user
will be prompted to provide a driver and the device will then be immediately
available.

USB can provide either 1.5 Mbps or 12 Mbps data rates depending on the type of
device being connected.

I/0 Addresses

As all of the PCs devices are attached to the same data bus, the CPU needs a
method of keeping track of them.

Each device is therefore assigned its own address. Normally this is built into
the device itself. When the CPU wishes to communicate with a particular device,
it sends out the unique code assigned to that device over the data bus, so that
device knows that the following data is destined for it. All other devices
ignore the data because it is not destined for them.

As mentioned above PCl expansion cards are self—configuring — they determine the
best \|/O address automatically.

Older ISA cards need to be configured manually with the use of switches and
jumpers on the board itself. This requires knowledge of available I/O addresses
and can lead to hardware conflicts with other devices.

To view a list of available and used I/O addresses you can use the “winmsd”
program (or just “msd” if you running in DOS).

[image]

The l/O address identifies the unique device to the CPU, but to ensure that data
is transmitted to and from the CPU reliably, the CPU needs to ensure that no two
devices “talk" at the same time.

Controlling the ﬂow of data is called interruption. Every CPU has an interrupt
wire. When a device needs to communicate with the CPU it places voltage on this
wire. The CPU then stops what it is doing at that time and attends to the
device, querying the BIOS to ascertain how it should communicate with the
device.

Because the CPU only has one interrupt wire, but it must communicate with
several peripheral devices, a secondary chip, known as the Interrupt Controller,
or 8259 chip, handles the interrupt requests. If a device needs to interrupt the
CPU it goes through the following steps:

>   1. The device applies voltage to the 8259 chip through its IRQ wire.

>   2. The 8259 chip informs the CPU, by means of the WT wire, that an interrupt
>   is pending.

>   3. The CPU uses a wire called an INTA (interrupt acknowledge) to signal the
>   8259 chip to send a pattern of 1s and 05 on the external data bus. This
>   information conveys to the CPU which device is interrupting.

4. The CPU knows which BIOS to run.

[image]

To see a list of ROS in use on your PC you can run winmsd also:

[image]

Just like \|/O addresses, IRQ settings are normally configured automatically by
plug and play devices like PCI cards, but older “legacy” devices will need to be
configured manually.

The CPU therefore runs the BIOS, the Operating System, any applications that are
running as well as the HO addresses and lRQs of the PC’s peripherals. This
results in the constant movement of a lot of data. Whilst this consumes a lot of
the CPU‘s time, moving data is a relatively uncomplicated task and a waste of
the CPU’s resources. Another chip, the DMA chip or 8237 chip, is used to control
the data passing from peripherals to the RAM and vice versa. This is known as
Direct Memory Access (DMA).

Like l/O addresses, all devices need their own address, or DMA Channel, to be
able to communicate with the DMA controller. DMA channel configuration can also
be seen in winmsd:

[image]

Like IRQs, devices can inadvertently share the same DMA channel leading to a
conﬂict.

To make the installation of modems and other serial devices easier, all PCs have
predefined HO and lRQ values for common ports. Provided a modem Is connected to
a port that no other device' IS using, it will work Here Is a list of predefined
values:

Port l/O Address lRQ

COM1 3F8 4

COM2 2F8 3

COM3 5E8 4

COM4 2E8 3

LPT1 378 7

LPT2 278 5

If you buy a network card that by default has an IRQ setting of 4, it will most
likely conﬂict with COM1 — the serial port. If you don’t intend to use the
serial port you can disable that port, hence freeing up its l/O address and IRQ.
If you want to use both devices then you would need to manually change the
setting on the network card itself.

Operating System Basics
=======================

Simply put, an operating system is software that controls the operation of a
computer. It is the interface between the computer’s hardware and the software
applications that run on it. It controls the allocation and usage of hardware
resources such as memory, central processing unit (CPU) time, disk space, and
peripheral devices.

It‘s important to realize that not all computers have operating systems. The
computer that controls the microwave oven in your kitchen, for example, doesn't
need an operating system. It has one set of relatively simple tasks to perform,
very simple input and output methods (a keypad and an LCD display), and simple,
never—changing hardware to control. For a computer like this, an operating
system would be unnecessary baggage, adding complexity where none is required.
Instead, the computer in a microwave oven simply runs a single program all the
time.

All desktop computers have operating systems. The most common are the Windows
family of operating systems (Windows 95, 98, 2000, NT and CE), the UNIX family
of operating systems (which includes Linux, BSD UNIX and many other derivatives)
and the Macintosh operating systems. There are hundreds of other operating
systems available for special-purpose applications, including specializations
for mainframes, robotics, manufacturing, real-time control systems and so on.

An OS can be divided into three areas:

>   The boot files which take control of the computer once the BIOS has
>   initialised

>   The file system which allows the computer to manage information

>   The utilities which allow the user to optimize system performance

At the simplest level, an operating system does two things:

It manages the hardware and software resources of the computer system. These
resources include things such as the processor, memory, disk space, etc.

It provides a stable, consistent way for applications to deal with the hardware
without having to know all the details of the hardware.

[image]

The first task is important as various programs and input methods compete for
the attention of the central processing unit (CPU) and demand memory, storage
and input/output (l/O) bandwidth for their own purposes. in this capacity, the
operating system plays the role of the good parent, making sure that each
application gets the necessary resources while playing nicely with all the other
applications, and husbanding the limited capacity of the system to the greatest
good of all the users and applications.

Within the broad family of operating systems, there are generally four types,
categorised based on the types of computers they control and the sort of
applications they support. The broad categories are:

>   Real-Time Operating System (RTOS) -- Real-time operating systems are used to
>   control machinery, scientific instruments and industrial systems. An RTOS
>   typically has very little user interface capability, and no end-user
>   utilities, since the system will be a "sealed box" when delivered for use. A
>   very important part of an RTOS is managing the resources of the computer so
>   that a particular operation executes in precisely the same amount of time
>   every time it occurs. In a complex machine, having a part move more quickly
>   just because system resources are available may be just as catastrophic as
>   having it not move at all because the system was busy.

>   Single-User, Single Task -- As the name implies, this operating system is
>   designed to manage the computer so that one user can effectively do one
>   thing at a time. The Palm OS for Palm computers is a good example of a
>   modern single-user, single-task operating system.

>   Single-User, Multitasking --This is the type of operating system most people
>   use on their desktop and laptop operating systems today. Windows 9x and the
>   MacOS are both examples of an operating system that will let a single user
>   have several programs in operation at the same time. For example, it's
>   entirely possible for a Windows user to be writing a note in a word
>   processor while downloading a ﬁle from the Internet while printing the text
>   of an e-mail message.

>   Multi-User -- A multi-user operating system allows many different users to
>   take advantage of the computer's resources simultaneously. The operating
>   system must make sure that the requirements of the various users are
>   balanced, and that the programs they are using each have sufficient and
>   separate resources so that a problem with one user doesn't affect the entire
>   community of users. Unix, VMS and mainframe operating systems, such as MVS,
>   are examples of multi-user operating systems.

Now let’s look at the basic functions of an operating system.

When the power to a computer is turned on, the first program that runs is
usually a set of instructions kept in the computer's Read-Only Memory (ROM) that
examines the system hardware to make sure everything is functioning properly.
This Power-On Self Test (POST) checks the CPU, memory, and basic input- output
systems for errors and stores the result in a special memory location. Once the
POST has successfully completed, the software loaded in ROM (sometimes called
firmware) will begin to activate the computer‘s disk drives. In most modern
computers, when the computer activates the hard disk drive, it finds the first
piece of the operating system, the bootstrap loader.

The bootstrap loader is a small program that has a single function: It loads the
operating system into memory and allows it to begin operation. in the most basic
form, the bootstrap loader sets up the small driver programs that interface with
and control the various hardware subsystems of the computer. It sets up the
divisions of memory that hold the operating system, user information and
applications. It establishes the data structures that will hold the myriad
signals, flags and semaphores that are used to communicate within and between
the sub-systems and applications of the computer. Finally it turns control of
the computer over to the operating system.

The operating system‘s tasks, in the most general sense, fall into six
categories:

>   Processor management

>   Memory management

>   Device management

>   Storage management

>   Application Interface

>   User Interface

Let’s look at each of these categories in turn.

Processor Management
--------------------

The heart of managing the processor comes down to two related issues: First,
ensuring that each process and application receives enough of the processor's
time to function properly and, second, using as many processor cycles for real
work as is possible. The basic unit of software that the operating system deals
with in scheduling the work done by the processor is either a process or a
thread, depending on the operating system.

it's tempting to think of a process as an application, but that gives an
incomplete picture of how processes relate to the operating system and hardware.
The application you see (word processor or spreadsheet or game) is, indeed, a
process, but that application may cause several other processes to begin, for
tasks like communications with other devices or other computers. There are also
numerous processes that run without giving you direct evidence that they ever
exist. A process, then, is software that performs some action and can be
controlled -- by a user, by other applications or by the operating system.

It is processes, rather than applications, that the operating system controls
and schedules for execution by the CPU. in a single-tasking system, the schedule
is straightforward. The operating system allows the application to begin
running, suspending the execution only long enough to deal with interrupts and
user input. interrupts are special signals sent by hardware or software to the
CPU. It's as if some part of the computer suddenly raised its hand to ask for
the CPU's attention in a lively meeting. Sometimes the operating system will
schedule the priority of processes so that interrupts are masked, that is, the
operating system will ignore the interrupts from some sources so that a
particular job can be finished as quickly as possible. There are some interrupts
(such as those from error conditions or problems with memory) that are so
important that they can't be ignored. These non-maskable interrupts (NMls) must
be dealt with immediately, regardless of the other tasks at hand.

While interrupts add some complication to the execution of processes in a
single- tasking system, the job of the operating system becomes much more
complicated in a multitasking system. Now, the operating system must arrange the
execution of applications so that you believe that there are several things
happening at once. This is complicated because the CPU can only do one thing at
a time. In order to give the appearance of lots of things happening at the same
time, the operating system has to switch between different processes thousands
of times a second. Here's how it happens.

A process occupies a certain amount of RAM. In addition, the process will make
use of registers, stacks and queues within the CPU and operating system memory
space. When two processes are multi-tasking, the operating system will allow a
certain number of CPU execution cycles to one program. After that number of
cycles, the operating system will make copies of all the registers, stacks and
queues used by the processes, and note the point at which the process paused in
its execution. It will then load all the registers, stacks and queues used by
the second process and allow it a certain number of CPU cycles. When those are
complete, it makes copies of all the registers, stacks and queues used by the
second program, and loads the first program.

All of the information needed to keep track of a process when switching is kept
in a data package called a process control block. The process control block
typically contains an ID number that identifies the process, pointers to the
locations in the program and its data where processing last occurred, register
contents, states of various flags and switches, pointers to the upper and lower
bounds of the memory required for the process, a list of files opened by the
process, the priority of the process, and the status of all l/O devices needed
by the process. When the status of the process changes, from pending to active,
for example, or from suspended to running, the information in the process
control block must be used like the data in any other program to direct
execution of the task-switching portion of the operating system.

This process swapping happens without direct user interference, and each process
will get enough CPU time to accomplish its task in a reasonable amount of time.
Trouble can come, through, if the user tries to have too many processes
functioning at the same time. The operating system itself requires some CPU
cycles to perform the saving and swapping of all the registers, queues and
stacks of the application processes. If enough processes are started, and if the
operating system hasn‘t been carefully designed, the system can begin to use the
vast majority of its available CPU cycles to swap between processes rather than
run processes. When this happens, it's called thrashing, and it usually requires
some sort of direct user intervention to stop processes and bring order back to
the system.

In a system with two or more CPUs, the operating system must divide the workload
among the CPUs, trying to balance the demands of the required processes with the
available cycles on the different CPUs. Some operating systems (called
asymmetric) will use one CPU for their own needs, dividing application processes
among the remaining CPUs. Other operating systems

(called symmetric) will divide themselves among the various CPUs, balancing
demand versus CPU availability even when the operating system itself is all
that's running.

Memory management is the next crucial step in making sure that all processes run
smoothly.

Memory Management
-----------------

[image]

When an operating system manages the computer's memory, there are two broad
tasks to be accomplished. First, each process must have enough memory in which
to execute, and it can neither run into the memory space of another process, nor
be run into by another process. Next, the different types of memory in the
system must be used properly, so that each process can run most effectively. The
first task requires the operating system to set up memory boundaries for types
of software, and for individual applications.

As an example, let's look at an imaginary system with 1 megabyte of RAM. During
the boot process, the operating system of our imaginary computer is designed to
go to the top of available memory and then "back up" far enough to meet the
needs of the operating system itself. Let‘s say that the operating system needs
300 kilobytes to run. Now, the operating system goes to the bottom of the pool
of RAM, and starts building up with the various driver software required to
control the hardware subsystems of the computer. In our imaginary computer, the
drivers take up 200 kilobytes. Now, after getting the operating system
completely loaded, there are 500 kilobytes remaining for application processes.

When applications begin to be loaded into memory, they are loaded in block sizes
determined by the operating system. If the block size is 2 kilobytes, then every
process that is loaded will be given a chunk of memory that is a multiple of 2
kilobytes in size. Applications will be loaded in these fixed block sizes, with
the blocks starting and ending on boundaries established by words of 4 or 8
bytes. These blocks and boundaries help to ensure that applications won't be
loaded on top of one another's space by a poorly calculated bit or two. With
that ensured, the larger question of what to do when the 500 kilobyte
application space is filled.

In most computers it's possible to add memory beyond the original capacity. For
example, you might expand RAM from 1 to 2 megabytes. This works fine, but tends
to be relatively expensive. It also ignores a fundamental fact of life -— most
of the information that an application stores in memory is not being used at any
given moment. A processor can only access memory one location at a time, so the
vast majority of RAM is unused at any moment. Since disk space is cheap compared
to RAM, then moving information in RAM to hard disk intelligently can greatly
expand RAM space at no cost. This technique is called Virtual Memory

Management. Operating systems that can only manage the conventional memory space
are said to run in real mode. OSes that can manage memory beyond this
restriction are said to run in protected mode. Windows 95 and later Microsoft
OSes use a technology known as paging which allows for the use of virtual
memory. This involves using a portion of the hard disk to write temporary
information to. This is considerably slower than conventional RAM, however. The
space on the hard disk which is used by the operating system for virtual memory
is known as the page file or the swap file.

Device Management
-----------------

The path between the operating system and virtually all hardware not on the
computer‘s motherboard goes through a special program called a driver. Much of a
driver's function is as translator between the electrical signals of the
hardware subsystems and the high—level programming languages of the operating
system and application programs. Drivers take data that the operating system has
defined as a file and translate them into streams of bits placed in specific
locations on storage devices, or a series of laser pulses in a printer.

Because there are such wide differences in the hardware controlled through
drivers, there are differences in the way that the driver programs function, but
most are run when the device is required, and function much the same as any
other process. The operating system will frequently assign high priorities
blocks to drivers so that the hardware resource can be released and readied for
further use as quickly as possible.

One reason that drivers are separate from the operating system is so that new
functions can be added to the driver-and thus to the hardware subsystems-
without requiring the operating system itself to be modified, recompiled and
redistributed. Through the development of new hardware device drivers,
development often performed or paid for by the manufacturer of the subsystems
rather than the publisher of the operating system, input/output capabilities of
the overall system can be greatly enhanced.

Managing input and output is largely a matter of managing queues and buffers,
special storage facilities that take a stream of bits from a device, from
keyboards to serial communications ports, holding the bits, and releasing them
to the CPU at a rate slow enough for the CPU to cope with. This function is
especially important when a number of processes are running and taking up
processor time. The operating system will instruct a buffer to continue taking
input from the device, but to stop sending data to the CPU while the process
using the input is suspended. Then, when the process needing input is made
active once again, the operating system will command the buffer to send data.
This process allows a keyboard or a modem to deal with external users or
computers at a high speed even though there are times when the CPU can‘t use
input from those sources.

Application Interface
---------------------

Just as drivers provide a way for applications to make use of hardware
subsystems without having to know every detail of the hardware's operation,
Application Program Interfaces (APls) let application programmers use functions
of the computer and operating system without having to directly keep track of
all the details in the CPU's operation. Let‘s look at the example of creating a
hard disk file for holding data to see why this can be important.

A programmer writing an application to record data from a scientific instrument
might want to allow the scientist to specify the name of the file created. The
operating system might provide an API function named MakeFile for creating
files. When writing the program, the programmer would insert a line that looks
like:

MakeFile [1, %Name, 2]

In this example, the instruction tells the operating system to create a file
that will allow random access to its data (1), will have a name typed in by the
user (%Name), and will be a size that varies depending on how much data is
stored in the file (2). Now, let's look at what the operating system does to
turn the instruction into action.

First, the operating system sends a query to the disk drive to get the location
of the first available free storage location. With that information, the
operating system will create an entry in the file system showing the beginning
and ending locations of the file, the name of the file, the file type, whether
the file has been archived, which users have permission to look at or modify the
file, and the date and time of the file's creation. Next, the operating system
will write information at the beginning of the file that identifies the file,
sets up the type of access possible and includes other information that ties the
file to the application. In all this information, the queries to the disk drive
and addresses of the beginning and ending point of the file will be in formats
heavily dependent on the manufacturer and model of the disk drive.

Because the programmer has written his or her program to use the API for disk
storage, she doesn't have to keep up with the instruction codes, data types, and
response codes for every possible hard disk and tape drive. The operating
system, connected to drivers for the various hardware subsystems, will deal with
the changing details of the hardware-the programmer must simply write code for
the API and trust the operating system to do the rest.

User Interface
--------------

Just as the API provides a consistent way for applications to use the resources
of the computer system, a user interface (Ul) brings structure to the
interaction between a user and the computer. In the last decade, almost all
development in user interfaces has been in the area of the Graphical User
Interface (GUI) with two models, Apple's Macintosh and Microsoft Windows,
receiving most of the attention and gaining most of the market share. There are
other user interfaces, some graphical and some not, for other operating systems
as well.

Unix, for example, has user interfaces called shells that present a user
interface more flexible and powerful than the standard operating system
text-based interface. Programs such as the Korn Shell and the C Shell are
text-based interfaces that add important utilities, but their main purpose is to
make it easier for the user to manipulate the functions of the operating system.
There are also graphical user interfaces, such as X—Windows and Gnome, that make
Unix and Linux more like Windows and Macintosh computers from the user's point
of view.

It‘s important to remember that in all these examples the user interface is a
program, or set of programs, that sit as a layer above the operating system
itself. The same thing is true, with somewhat different mechanisms, of both
Windows and Macintosh operating systems. The core operating system functions,
the management of the computer system, lie in the kernel of the operating
system. The display manager is separate, though it may be tied tightly to the
kernel beneath. The ties between the operating system kernel and the user
interface, utilities and other software define many of the differences in
operating systems today, and will further define them in the future.

Operating Systems
-----------------

Now let’s look at some of the common operating systems that have been developed
over the years.

### MS-DOS (Microsoft Disk Operating System)

Microsoft released MS-DOS in 1981 for lBM PCs and compatibles. It was a
single-tasking, single-user operating system with a command line interface.

The IBM PC was a class of personal computer introduced in 1981 which quickly
became the de facto standard for PC development. Due to IBM’s decision to
release the speciﬁcation into the public domain a wide range of “compatible” PCs
developed by other manufacturers which complied to the same specification soon
arose.

DOS has three boot files — “io.sys”, “msdos.sys” and “command.com”

io.sys contains the drivers for the DOS BIOS. It contains drivers for the
keyboard and screen, a parallel and serial port, and the system clock.

msdos.sys contains the drivers for the DOS kernel, and provides the functions of
all traditional operating systems: file and directory management, character
input and output, time and date support, memory management, and country-specific
conﬁguration settings.

command.com is the “command interpreter". it provides what is known as the
shell. It is responsible for the handling of internal commands, commands that do
not have a “.com” or “.exe" extension (such as copy, for example).

If you enter a command at the prompt, command.com will check whether it is an
internal command, if not, then it will search the current folder for a file with
a .com or .exe extension matching the command. If it is unable to ﬁnd such a
file, it will return “Bad command or file name”.

Note — DOS also use another file, “config.sys”, which allows you to modify the
memory allocation and also specify an alternative to “command.com” for the
shell.

These are the system files. Once the system has loaded, DOS then looks for a
file called “autoexec.bat". This file contains user-defined boot settings, the
equivalent of the startup folder in Windows.

DOS memory management

A PC’s memory is divided into four distinct sections:

>   Conventional memory

>   Upper memory

>   High memory

>   Extended memory

The Conventional memory area is limited to 640 KB. DOS can only handle this
amount of memory and will load programs into this memory.

As programs became more complicated they became too large to fit into this
memory area. Microsoft changed DOS so that it was possible to load DOS itself
into the high memory area (HMA), thus freeing up more conventional memory for
applications. They also made it possible to load device drivers into the upper
memory area (UMA), freeing up even more space.

The DOS file system places an 8-character on the length of filenames and
requires that each file has a 3—character extension.

### Windows 3.x

Windows 3.x was a multi—tasking Graphical User Interface (GUI) which ran on top
of the DOS environment. Windows for Workgroups added limited support for
networking.

### Windows 9x (95, 98, 988E, ME)

Windows 95 was the first “self-contained” Microsoft operating system that did
not use the DOS environment and run on top of it as a shell. lt replaced the DOS
file system and included support for filenames up to 255 characters in length
and had a radically reworked interface.

The biggest improvement, however, was the introduction of Plug and Play hardware
installation. Generally speaking, plug and play refers to the ability of an
operating system to automatically configure any new hardware which is added to
it. With DOS, users had to manually assign system resources to a device and
specify the location of the device's drivers. Windows 95 was the first Microsoft
OS to perform these functions automatically.

Windows 95 also provided support for the FAT16 file system, allowing for
partitions of up to ZGB in size.

Windows 98 looks very similar to Windows 95 in terms of appearance, but it
provided greater interaction with the Internet, allowing users to access remote
files in the same way as they would from their desktop. It also provided support
for the FAT32 file system allowing for partitions of up to 2TB in size
(Terabytes).

Windows 98SE (Second Edition) provided support for USB devices and also included
the Windows Driver Model (WDM). Simply put, the Windows Driver Model contains
generic drivers for devices of a similar class, such as modems and printers.
This allows manufacturers to write relatively “lightweight" drivers with only
the necessary information which is relevant to their hardware.

This allows the manufacturer to write one driver for their device which will
work on all OSes that support the WDM. Previously manufacturers had to write
separate drivers for Windows 95, 98 and NT.

Windows ME is essentially unchanged from Windows 988E. The interface is slightly
improved and there are a number of wizards to simplify tasks such as setting up
a home network, but in terms of functionality it is not a radical improvement.

Another dubious improvement introduced with Windows 98, 98 SE and ME was the
introduction of ACPI — Advanced Configuration and Power Interface. This has the
goal of managing power more efficiently by enabling the OS to automatically
power devices on and off as required. But as you will know, this doesn’t always
work quite so seamlessly in practice and many problems involving PC cards are
resolved by disabling this feature altogether.

### Windows NT

Windows NT (New Technology) was first released in 1993. It is a self-contained
operating system and NT4 (released in 1996) looks very similar to Windows 95 in
terms of the user interface. It is a 32-bit, preemptive multitasking operating
system that features networking, symmetric multiprocessing, multithreading, and
security.

Windows NT was a radical departure from the thinking behind Windows 95 (hence
the name, New Technology) and was designed as a Network Operating System (NOS).
It therefore came in two ﬂavours: NT Server and NT Workstation.

The Windows NT architecture is the basis of later Microsoft Operating Systems:
Windows 2000, Windows XP and Windows Vista. I will look in more detail at the
Windows NT architecture in a later section.

### Windows 2000

Windows 2000, like Windows NT before it, is a multi threaded, multitasking
32—bit operating system. Implemented in desktop and several server versions,
Windows 2000 focuses overall on improved ease of use, networking, management,
reliability, scalability, and security.

it comes in several ﬂavors:

[table]

[table]

### Macintosh

A popular series of personal computers introduced by the Apple Computer
Corporation in January 1984. The Macintosh was one of the earliest personal
computers to incorporate a graphical user interface and the first to use
3.5—inch floppy disks. Despite its user—friendly features, the Macintosh lost
market share to PC-compatible computers during the 1990s, but it still enjoys
widespread use in desktop publishing and graphics-related applications.

[image]

### Santa Cruz Operating Systems (SCO)- SCO Xenix

In 1988 SCO Xenix was released and with it brought the power of UNIX to the
lower powered Intel 386 class of PC's. This multi-user, multi-threaded networked
OS was able to work with multi-user enabled software such as Lotus 123
Spreadsheet, WordPerfect Word processor, and FoxBase Relational Database
Management System and allow multiple users to share access through smart ASCII
based smart terminals to high end peripherals such as high speed dot matrix and
laser printers. This worked well in office environments with multiple users
needing access to these application programs. This OS had a text based command
line interface which made it rather unfriendly to the general user but had more
power than dos did at the time. This 08 did have the advantage that it would run
the most popular office applications at that time right out of the box.

Introduction to SMS Messaging
=============================

This guide will look at the evolution of mobile messaging from its early
inception as SMS up to the current technological advances of MMS.

It will also explain the mechanics behind message delivery and how networks are
using the potentials of messaging to increase their revenues.

SMS
---

SMS (Short Message Service, or “texting” as it is more commonly known) has long
been a feature of GSM, and has been available since 1992.

It is a common form of communication and today accounts for 15% of mobile
operators’ revenue.

[image]

>   The earliest SMS service enabled users to send messages only to other users
>   within the same network.

>   The ability to send SMS messages between networks — nationally and
>   internationally — was not possible until the late 1990s.

>   Today almost 15 billion messages are delivered globally each month.

>   A single SMS message can contain up to 160 characters. Some mobile devices
>   allow several messages to be “concatenated” (joined together) allowing a
>   maximum of 9 messages to be sent, representing a total message length of
>   1358 characters.

These messages will be delivered as a constant stream of text and the recipient
will not notice any breaks in the text provided that their handset also supports
concatenation. If it does not then the user will see 9 messages with random
characters at the beginning and end of each message.

Note — the sender is still charged for sending 9 messages.

SMS messages can be received while a phone call is in progress, as all SMS
trafﬁc is sent via the GSM signalling channel. This enables the network to make
a more efficient use of scarce radio resources.

The signalling channel, known as 887, enables the mobile device to communicate
with the base station for the purposes of location management and call setup.

[image]

Smart Messaging
---------------

In 1999, Nokia announced a development of the conventional SMS standard which
allowed for additional data to be sent with a message, including:

>   \* Ringtones

>   \* Business cards

>   \* Calendar appointments

>   \* WAP or Internet settings

>   \* Picture messages (comprised of three messages)

This heralded the first development of multimedia messaging, but as it was a
proprietary Nokia standard messages could only be exchanged between those Nokia
handsets which supported the standard.

It did mean, however, that handsets could be provisioned for new services such
as WAP automatically without user intervention. The sending of service settings
via SMS is called OTA provisioning (Over The Air - a service which WDS now
provides with Mouse2Mobile)

Enhanced Messaging Service (EMS)
--------------------------------

Launched in 2000, EMS was Ericsson’s answer to Nokia’s Smart Messaging platform.
It did not support OTA provisioning nor calendar or contact information
transfer.

But it did provide support for pictures, sounds and stylised text. This meant
that messages could be personalised with emoticons — small graphics to express a
sentiment, and users could personalise their phones with logos, ringtones and
screensavers.

How are SMS messages sent?

Standard SMS, Smart Messages and EMS messages are all sent in the same way. All
messages pass through a component of the network called the SMSC (Short Message
Service Centre)

The SMSC consults with the HLR and VLR to ascertain the current location of the
user, and sends the information to the MSC for routing across the network to the
target device.

[image]

The SMSC is responsible for storing messages if delivery is not immediately
possible (if the target device is switched off or is out of range).

Delivery attempts are repeated at a pre—defined interval. Some devices allow a
“message validity" to be set, so that a message will expire if not delivered
within a certain time frame.

Both Smart Messaging and EMS brought new life to the SMS service with sounds and
graphics, and added impetus to further development of the service, which has now
culminated in the development of MMS — the Multimedia Messaging Service.

[image]

MMS will enable multiple forms of media to be combined in a single message
—text, graphics, animation and audio samples.

Later stages of the service will develop with the evolution of GSM as a whole
and allow for the inclusion of motion video.

This could be delivered in two ways — 30 seconds of motion video could recorded
with the terminal device’s built-in camera and included in the body of a
message.

Alternatively, the message could contain a link to a web server which would live
stream video to the terminal.

How are MMS messages delivered?

As you will remember SMS messages are delivered over the GSM network signaling
channel. Due to the larger amount of data transfer involved in sending an MMS
message, this is no longer possible.

Both the signaling channel and conventional voice and data channels will be
required.

[image]

Whereas SMS messages are delivered using the 887 protocol, MMS messages will use
a new protocol — Wireless Session Protocol, or WSP.

WSP is already used by WAP services for the routing of data. It handles the
sending of a WAP page request to the WAP server that holds the page, and sending
the page back to the terminal device.

This will guarantee compatibility with a wide range of existing handsets.
However, although MMS uses protocols in use by WAP, MMS does not use the browser
function of the handset.

If the mobile network operator implements an MMS email gateway, it is possible
to send MMS messages from the mobile device to an Internet email recipient.

All MMS messages are encoded as a MIME (Multipurpose Internet Mail Extension)
message and sent over the network using SMTP. This guarantees that the message
will be viewable in almost any email program.

[image]

Due to the amount of data involved in the sending of an MMS message, it is not
suitable for sending over conventional CSD. HSCSD or GPRS carriers are
preferred.

### MMS Network Components

In order to provide an MMS service, the network operator needs to add several
components to the existing GSM network infrastructure. Together these are called
the Multimedia Message Service Environment.

[image]

MMSC — Multimedia Messaging Service Centre

As with conventional SMS, the MMSC is responsible for storing the message until
it has been successful received by the intended recipient. It comprises two
pieces of hardware — the MMS Proxy Relay and the MMS Server. The Server stores
the message, the Proxy Relay forwards messages or sends message notifications to
terminal devices.

Application Gateway

The Application Gateway allows Internet—based service providers to send content
to mobile devices in the form of an MMS message. The gateway can perform the
necessary format conversion where necessary (from HTML to an MMS-compatible
format, for example), and can resize images for display on smaller screens.

User Database

The User Database holds information on the capabilities of a user’s mobile
device and what services the user has subscribed to. User’s details are stored
in the database in a User Agent Profile (UAP)

The user agent profile might contain such information as:

>   Maximum size of message supported in bytes

>   Maximum size of image supported in bytes

>   Supported content formats

>   Supported character sets

>   Accepted languages

>   Accepted encoding formats

>   Screen size

>   Processor type

Users may be able to access their profile via a web-based interface and specify
custom user settings, such as setting up filters to automatically discard
undesired messages, specifying which message formats they wish to have forwarded
to their device, and which message types they wish saved for later retrieval.

Message Store

Also known as the MMS Terminal Gateway. This provides MMS support for devices
that do not themselves support MMS messaging. This will store the MMS message
and send notification via conventional SMS to the device to indicate that a
message is waiting for the user. The user will then need to connect to the store
via an Internet connection, from their PC for example.

Such devices are known as legacy devices.

The Message Store could also serve as a personal album for users. Each user will
be assigned a certain amount of storage space and can opt to save favourite
messages here for later retrieval.

Multimedia Email Gateway

The MMS Email Gateway provides the necessary functionality for MMS messages to
be sent to email recipients. Similarly, incoming emails can be converted to MMS
format for delivery to mobile terminals.

Multimedia Voice Gateway

The Voice Gateway enables the user to record voice samples and send them
directly to the recipient via the MMS service. Serving a similar function to
voicemail, this is a more personalised and much faster service.

WAP Gateway

The WAP Gateway enables multimedia content to be compressed prior to
transmission over the wireless network to increase network efficiency. Most
network operators will already provide WAP gateways for their existing WAP
services. In this context it is responsible for converting HTML pages held in
the “wired” Internet, into WML format for interpretation by the “wireless"
browser on the user’s handset.

MMS — from composition to delivery

In its initial phase, an MMS message will resemble a miniature slide slow. A
slide could contain any of the following:

>   Text in various sizes and colours

>   Animated colour graphics

>   Digital photographs

>   Sampled digital audio

>   Synthesised audio (beeps or tones)

MMS messages M” be created using a programming language optimised for the new
format, known as SMIL (Synchronised Multimedia Integration Language).

Pronounced “smile", the language is similar in format to HTML and WML and
contains information governing the content and timing of the presentation.

[image]

Each MMS message will contain one slide. Each slide may be comprised of a
graphic region and a text region. Timing options can be set to that one or the
other appears to the recipient first.

[image]

The orientation of the message can be specified also — landscape or portrait —
to best harness the screen resolution of the recipient’s device.

In Phase 1, MMS messages will be delivered using existing WAP protocols and
network infrastructure. Due to limitations of this technology MMS messages will
initially be limited to a maximum of 30 Kb.

The message itself will be “built” as follows:

[image]

>   Header information

>   SMIL code

>   The image for the ﬁrst slide

>   The image for the second slide

>   The text for the ﬁrst slide

>   The text for the second slide

>   The audio for the first slide

>   The audio for the second slide

The SMIL code will contain the slide layout and also information governing the
timing of presentation of the different message elements.

Once complete, all of the elements are compressed and saved as a single MIME
“object", guaranteeing efficient data transfer over the network as well as
compatibility with a wide range of both other mobile devices and also other
devices connected to the wired Internet.

[image]

To send the message the sender will need to enter the recipient’s voice number
or email address.

The MMS message is then sent from the sender’s mobile device to the network’s
MMSC using the WSP protocol.

The message is then stored at the MMSC and a notification message is then sent
to the recipient’s terminal.

The message is sent using “PUSH” technology. This was developed to enable WAP
content providers to send unsolicited messages to users without the user’s
specific request.

To avoid receiving junk messages users can now specify from which gateways and
servers they wish to receive messages — unless the user has granted access to
the correct gateway then messages may be blocked.

Provided the recipient’s terminal can receive an MMS message, it will send a
request message to the MMSC to download the message.

The MMSC then sends the message to the terminal, and once it receives
confirmation of the message’s receipt, it is removed from the message store.

It is possible for the user to specify that a conventional SMS be sent to the
terminal to indicate that an MMS is waiting for them, so that the user can then
decide whether or not to download the message.

Due to the "always on" nature of GPRS connections, it is the preferred “bearer"
service for MMS messaging, enabling seamless message delivery with little or no
user intervention.

Introduction to TCP/IP
======================

You will all (hopefully!) have heard of TCP/IP and know that it is somehow used
by the Internet. But what is it really and how does it work? This chapter will
give an introduction to how computers exchange information on a network and how
networks can be connected together to form an internetwork. I will also explain
what TCP and IP are and how they work together. This chapter assumes no prior
knowledge on any of these subjects, but is intended as a brief introduction
only.

Sections

An overview of TCP

An introduction to networking

The OSI Reference Model

Bridges, Routers and Gateways

Network Addressing

An overview of IP

IP Addressing

The IP header

DHCP

SLIP and PPP

A Datagram’s life

What is NAT and how does it work?

What are ports and sockets?

Appendices

An overview of TCP

TCP stands for Transmission Control Protocol.

TCP is actually a suite of protocols. It was developed during the 1970s and
1980s by the US Department of Defence and is designed to allow dissimilar
systems (such as Windows and Unix) to communicate with each other. It is an open
system which means it is completely hardware and software independent.

Its main function is to translate the data created by applications requiring
access to the network (be it email, FTP or whatever) into a common form. It is
also responsible for monitoring the transmission of data and correcting any
errors that occur.

Together with IP (Internet Protocol) it can ensure that data is routed across
the network over the shortest route possible. It is also able to automatically
reroute data if one or more nodes on the network become damaged.

TCP/IP was first developed in 1969 by the US Department of Defence. Prior to
1969 the entire intelligence and defence reﬂex computer system was held in one
location. This was considered to be a potentially vulnerable target, so millions
of dollars was invested in creating what became known as DARPAnet (Defence
Advanced Research Projects Agency Network). DARPAnet became one of the largest
Wide Area Networks in the world, spanning several US states and enabling data to
be replicated between sites.

In order to understand how TCP operates and how it interacts with other
protocols it is necessary to understand how networks communicate.

An introduction to networking

In simple terms, a network is a set of computers and peripherals (printers,
scanners etc) that are connected by some medium. The connection can be direct
(via cable, IR or radio), or indirect (via a modem).

Whatever the connection method, all networks communicate following a set of
rules (protocols).

Networks can be conﬁned to a room, a building or can be scattered across many
different continents. The geographical layout of a network is known as the
network topology.

A group of connected computers in a building is known as a Local Area Network
(LAN). Two different LANs in different cities could be connected to form a Wide
Area Network (WAN). Although these networks are miles apart, as far as the
Internet is concerned it is one entity as it has a single ”domain". Taking WDS
as an example, the company has a domain of wdsglobal.com, but has several
subdomains of uk, us, au and za. Each region has its own local area network, but
each local area network can communicate with each other, forming a wide area
network.

[image]

Computers can communicate over networks in many ways and for many reasons, but
almost all of the process to transfer information from one machine to another is
unconcerned with the nature of the data itself. By the time the data generated
by the transmitting computer reaches the cable or other medium, it has been
reduced to signals that are native to that medium. These might be electrical
voltages, for a copper cable network; pulses of light, for ﬁber optic; or
infrared or radio waves. The receiving computer translates these signals back
into binary data and it is interpreted by the relevant application, be it an
email client, web browser or whatever.

Baseband and broadband

In most cases, LANs use a shared network medium. The cable connecting the
computers can carry one signal at a time, and all of the systems take turns
using it. This type of network is called a baseband network.

Baseband networks transmit data in small chunks of data called packets. For this
reason they are known as packet-switched.

The alternative to packet-switching is circuit-switching. Circuit-switching
means that the two systems wanting to communicate establish a circuit before
they transmit any information. That circuit remains open throughout the duration
of the exchange, and is only broken when the two systems are finished
communicating. This is an impractical solution for computers on a baseband
network, because two systems could conceivably monopolise the network medium for
long periods of time, preventing other systems from communicating. Circuit
switching is more common in environments like the public switched telephone
network (PSTN), in which the connection between your telephone and that of the
person you're calling remains open for the entire duration of the call.

A broadband network is the opposite of a baseband network — it can carry
multiple signals on the same medium simultaneously.

Half duplex and full duplex

When two computers communicate over a LAN, data typically travels in only one
direction at a time, because the baseband network used for most LANs supports
only a single signal. This is called half-duplex communication. By contrast, two
systems that can communicate in both directions simultaneously are operating in
full-duplex mode. An example of half duplex communication would be a CB radio,
where each party has to wait for the other to ﬁnish before transmitting. An
example of full duplex would be a telephone conversation.

[image]

Clients and servers

A sewer is simply a computer that provides services to other computers, for
example a ﬁle server or a print server. A client is the computer that accesses
the server to make use of these services.

All computers can perform both roles. On most networks servers are more powerful
machines with a single role to perform. Networks comprised of machines which
perform both client and server functions are called peer-to-peer networks.

The OSI Reference Model

In 1983, the International Organization for Standardization (ISO) and what is
now the Telecommunications Standardisation Sector of the International
Telecommunications Union (lTU-T) published a document called "The Basic
Reference Model for Open Systems Interconnection."

The OSI model deﬁnes the networking process and breaks it down into 7 layers.
This is a theoretical construction designed to make the process easier to
understand. At the top of the model is the application that requires access to a
resource on the network, and at the bottom is the network medium itself. As data
moves down through the layers of the model, the various protocols operating
there prepare and package it for transmission over the network. Once the data
arrives at its destination, it moves up through the layers on the receiving
system, where the same protocols perform the same process in reverse.

The seven layers are shown below:

[image]

>   The Application, Presentation and Session layers are known as the upper
>   layers and are responsible for the user interface.

>   They are independent of (and oblivious to) the four layers below them.

>   The lower layers deal with the transmission of data.

>   They don’t differentiate between applications, they are concerned with data
>   and where to send it.

The protocols operating at each layer of the OSI model add a header (and in one
case a footer) to the data received from the layer above it. For example, when
an application generates a request for a network resource, it passes the request
down through the protocol stack. When the request reaches the transport layer,
the transport layer protocol adds its own header to the request. The header
consists of fields containing information that is speciﬁc to the functions of
that protocol.

The transport layer protocol, after adding its header, passes the request down
to the network layer. The network layer protocol then adds its own header in
front of the transport layer protocol's header. The original request and the
transport layer protocol header thus become the payload for the network layer
protocol. This entire construct then becomes the payload for the data-link layer
protocol. which typically adds both a header and a footer. The ﬁnal product,
called a packet, is then ready for transmission over the network. After the
packet reaches its destination, the entire process is repeated in reverse. The
protocol at each successive layer of the stack (traveling upwards this time)
removes the header applied by its equivalent protocol in the transmitting
system. When the process is complete, the original request arrives at the
application for which it was destined in the same condition as when it was
generated.

The process by which the protocols add their headers and footer to the request
generated by the application is called data encapsulation.

[image]

The procedure is functionally similar to the process of preparing a letter for
mailing. The application request is the letter itself, and the protocol headers
represent the process of putting the letter into an envelope, addressing,
stamping, and mailing it.

Let’s look at each of the layers in a little more detail.

The Physical Layer

At the bottom of the OSI model (or “protocol stack") is the physical layer. This
is the hardware of the computer's network interface adapter, be it an Ethernet
card, an infrared port or whatever. The physical layer is often determined by
the protocol used in the data link layer. For example Ethernet is a data link
layer protocol but supports many different types of cabling.

The Data Link Layer

The protocol operating on the data link layer is the conduit between the
computer’s networking hardware and its networking software (in the network
layer). The protocols on the network layer pass data down to the data link layer
which packages it for transmission over the network. At the receiving machine
the data link layer protocol receives data from the physical layer and passes it
up to the network layer after removing the header and footer generated by the
data link layer protocol on the other machine.

[image]

[image]

It is important to understand that data-link layer protocols are limited to
communications with computers on the same LAN. The hardware address in the
header always refers to a computer on the same local network, even if the data's
ultimate destination is a system on another network.

The other primary function of the data link layer is to determine what protocol
on the network layer created the frame — a computer can have several different
protocols operating on the network layer. It is up to the data link layer to
ensure that frames created by one protocol get delivered to the same protocol on
the receiving machine.

The data link layer is also responsible for error detection. This is
accomplished by the Cyclic Redundancy Check (CRC) which is included in the
footer. The CRC is a means of checking that the packet has been received
successfully. It takes the sum of all the 15 in the payload and adds them
together. The result is stored as a hexadecimal value in the trailer. The
receiving device adds up the is in the payload and compares the result to the
value stored in the trailer. If the values match, the packet is good. But if the
values do not match, the receiving device sends a request to the originating
device to resend the packet.

[image]

[image]

The Network Layer

Data link layer protocols only function on the local area network. Network layer
protocols are what is known as “end-to-end" protocols. They are responsible for
communicating with the destination machine to ensure that transmitted data is
received successfully.

The most commonly used network layer protocol is IP, Internet Protocol. As a
frame moves across the Internet, the data link layer protocol may change several
times (as the physical medium of the network it is on changes), but the network
layer protocol will remain intact throughout the trip.

Like the data link layer protocol, the network layer protocol applies a header
to the data it receives from the layer above.

[image]

The unit of data created by the network layer is called a datagram.

[image]

[image]

As mentioned earlier, network layer protocols are responsible for ensuring the
datagram‘s entire journey to the destination machine. This process is known as
routing.

The individual networks that make up the Internet are connected by Routers. A
router's function is to examine each packet that passes through it and determine
whether or not it is destined for a machine on the network it is connected to
or, if not, forward it to another router. To do this the router must examine the
IP address of the packet, which means passing the datagram up to the network
layer:

[image]

The router then processes the packet and sends it back down to the data link
layer to be transmitted to its next destination.

[image]

The Transport Layer

Transport layer protocols work very closely with network layer protocols. IP is
a network layer protocol. The most common transport layer in use today is TCP
(Transmission Control Protocol).

There are two types of protocol in the transport layer — connection-oriented and
connection/ass.

A connection-oriented protocol is one in which the two communicating systems
exchange messages to establish a connection before they transmit any application
data. This ensures that the systems are both active and ready to exchange
messages. TCP, for example, is a connection' oriented protocol. When you use a
Web browser to connect to an Internet sewer, the browser and the server ﬁrst
perform what is known as a three-way handshake to establish the connection. Only
then does the browser transmit the address of the desired Web page to the
server. When the data transmission is completed, the systems perform a similar
handshake to break down the connection.

Connection-oriented protocols also provide additional services such as packet
acknowledgment, data segmentation, flow control, and end-to-end error detection
and correction. Because of these services, connection-oriented protocols are
often said to be reliable. The drawback of this type of protocol is that it
greatly increases the amount of control data exchanged by the two systems,
effectively slowing the connection.

Connectionless protocols (such as UDP, User Datagram Protocol), do not offer
these services and are therefore said to be unreliable. But as they transmit
much less data they are often used for network signalling and control messages.

The Session Layer

Up until now all the protocols discussed have been involved in the communication
of data. The Session layer is not concerned with data transmission, rather it
coordinates the exchange of data between the lower layers and the upper layers.

The Presentation Layer

The presentation layer is responsible for determining what application the “raw"
data from the lower layers is destined for, and delivering it to the
corresponding port number (we’ll look at ports in more detail later).

The Application Layer

The application layer is the entrance point that programs use to access the OSI
model and utilise network resources. Most application layer protocols provide
services that programs use to access the network, such as the Simple Mail
Transfer Protocol (SMTP), which most e-mail programs use to send e-mail
messages. In some cases, such as the File Transfer Protocol (FTP), the
application layer protocol is a program in itself.

Remember that the OSI seven-layer model is a theoretical construct, a guide to
understanding the procedure for transmitting data over a network. In the TCP
model application layer protocols include the session and presentation layer
functions. As a result, the TCP protocol stack consists of four layers -
application, transport. network and data-link.

Bridges, Routers and Gateways

A bridge is a piece of hardware normally with two network interface adapters
used to connect to network segments together. The bridge analyses the
destination address of all packets and determines which of the two networks the
packet is destined for. A bridge can only be used where the two protocols use
the same data link layer protocol (ie the same network technology).

A router is simply a machine on a network (a node) that forwards packets around
the network.

A gateway is a device that performs routing functions, but can perform protocol
translation from one network to another if necessary.

Network Addressing

All networks use a form of addressing to transfer data from one machine to
another. Network addresses are analogous to mailing addresses in that they tell
a system where to deliver data. Three terms commonly used in networking relate
to addressing: name, address, and route.

A name is a specific identiﬁcation of a machine, a user, or an application. It
is usually unique and provides an absolute target for the data. An address
typically identiﬁes where the target is located, usually its physical or logical
location in a network. A route tells the system how to get data to the address.

Each device on a network that communicates with others has a unique physical
address, sometimes called the hardware address. The hardware address is also
known as the MAC address (Media Access Control) and is burned into the network
adapter card itself. Legally no two adapter cards can have the same MAC address;
therefore no two machines on the same network can have the same address. The MAC
address consists of a 3-byte value called an organizationally unique identiﬁer
(OUI), which is assigned to the adapter's manufacturer by the IEEE, plus another
3-byte value assigned by the manufacturer itself.

Ethernet, like all data-link layer protocols, is concerned only with
transmitting packets to another system on the local network. If the packet's
final destination is another system on the LAN, the Destination Address field
contains the address of that system's network adapter. If the packet is destined
for a system on another network, the Destination Address ﬁeld contains the
address of a bridge or gateway on the local network that provides access to the
destination network. It is then up to the network layer protocol to supply a
different kind of address (such as an Internet Protocol [IP] address) for the
system that is the packet's ultimate destination. A typical Ethernet packet
looks like this:

[image]

This is the addressing system used on local networks. The addressing system used
on an Ethernet network cannot be used to send a packet to a machine on another
network. Ethernet is a data link layer protocol. Only network layer protocols
can address packets to other networks (as they are end-to-end protocols,
remember?)

As mentioned earlier lP (Internet Protocol) is the most common network layer
protocol in use.

Internet Protocol

Despite what I have just said above, although the word “Internet” appears in
this protocol’s name, it is not limited to use on the Internet, it can be used
on dedicated networks that have no connection to the Internet as well.

IP is responsible for sending the data provided by the transport layer protocol
(TCP most commonly) to its destination. It can work with any transport layer
protocol, it does not have to be TCP.

lP was developed by a man called Vint Cerf who was part of the DARPAnet project.

The destination address in the header created by IP is the address of the target
machine. Along the way to this machine the datagram may be encapsulated with
headers from several other protocols, but the lP datagram itself will never be
changed en route. The process is similar to the delivery of a letter by the post
ofﬁce, with IP functioning as the envelope. The letter might be placed into
different mailbags and transported by various trucks and planes during the
course of its journey, but the envelope remains sealed. Only the addressee is
permitted to open it and make use of the contents.

Now let’s get a bit technical!

The IP Header

When the IP protocol receives data from the transport layer it adds a 20-byte
header to it:

[image]

The header provides the following information:

-   Version (4 bits) — the version of IP that created the datagram (the version
    in current use is 4, but version 6 is being developed).

-   IHL (Internet Header Length) (4 bits) — tells the receiving system the
    length of the IP header.

-   Type of Service (1 byte) — speciﬁes the priority for the datagram. Rarely
    used this field can be used to specify the importance of the datagram so
    that a router will forward this as soon as it receives it, rather than
    buffering it.

-   Total Length (2 bytes) — tells the receiving system the length of the entire
    datagram (in case it will need to fragment it).

-   ldentification (2 bytes) — this ﬁeld numbers the datagram so that the
    receiving machine can ensure datagrams are reassembled in the correct order.

-   Flags (3 bits) — used to regulate the fragmentation process.

-   Fragment offset (13 bits) — if a datagram is fragmented, this field
    identiﬁes the fragments place in the datagram for reassembly.

-   Time To Live (1 byte) - This ﬁeld specifies the number of networks that the
    datagram should be permitted to travel through on the way to its
    destination. Each router that forwards the datagram reduces the value of
    this field by one. If the value reaches zero, the datagram is discarded.
    This is important otherwise datagrams could in theory roam the Internet
    forever.

-   Protocol (1 byte) — This ﬁeld contains a code that identiﬁes the protocol
    that generated the information found in the Data ﬁeld.

-   Header Checksum (2 bytes) — This ﬁeld contains a Checksum value computed on
    the IP header ﬁelds only (and not the contents of the Data ﬁeld), for the
    purpose of error detection.

-   Source IP Address (4 bytes) — This ﬁeld speciﬁes the IP address of the
    system that generated the datagram.

-   Destination IP Address (4 bytes) — This ﬁeld speciﬁes the IP address of the
    system for which the datagram is destined.

-   Options (variable) — This ﬁeld is present only when the datagram contains
    one or more of the 16 available lP options. The size and content of the ﬁeld
    depends on the number and the nature of the options. Options include
    “timestamping” the datagram, allowing the IP header to increase to include
    the IP address of all routers it has passed through among others

-   Data (variable) — This ﬁeld contains the information generated by the
    protocol speciﬁed in the Protocol ﬁeld. The size of the field depends on the
    data-link layer protocol used by the network over which the system will
    transmit the datagram.

IP Addressing

The IP protocol is unique in that it has its own unique self-contained
addressing system that it uses to identify computers on networks of almost any
size.

IP uses a 32-bit addressing scheme which contains both a network ID and a host
ID, to identify both the network which the target machine is on as well as the
target machine’s logical location on that network.

[image]

The address is notated using four decimal numbers ranging from O to 255,
separated by periods, as in 192.168.1.44. This is known as dotted decimal
notation. This provides a total range of some 4,294,967,296 unique addresses.

Unlike hardware addresses, which are hardcoded into network interface adapters
at the factory, network administrators must assign IP addresses to the systems
on their networks. It is essential for each network interface adapter to have
its own unique IP address; when two systems have the same IP address, they
cannot communicate with the network properly.

As mentioned earlier, IP addresses consist of two parts: a network identiﬁer and
a host identifier. All of the network interface adapters on a particular subnet
have the same network identiﬁer, but a different host identiﬁer. For systems
that are on the Internet, the network identiﬁers are assigned by a body called
the Internet Assigned Numbers Authority (IANA). This is to ensure that there is
no address duplication on the Internet. When an organization registers its
network, it is assigned a network identiﬁer, and it is then up to the network
administrators to assign unique host identifiers to each of the systems on that
network. This two-tier system of administration is one of the basic principles
of the Internet.

IP Address Classes

The most complicated aspect of an IP address is that the division between the
network identifier and the host identiﬁer is not always in the same place. A
hardware address, for example, consists of 3 bytes assigned to the manufacturer
of the network adapter and 3 bytes which the manufacturer itself assigns to each
card. IP addresses can have various numbers of bits assigned to the network
identiﬁer, depending on the size of the network.

Class A addresses are for large networks that have many machines. The 24 bits
for the local address (also frequently called the host address) are needed in
these cases. The network address is kept to 7 bits, which limits the number of
networks that can be identified.

Class B addresses are for intermediate networks, with 16-bit local or host
addresses and 14-bit network addresses.

Class C networks have only 8 bits for the local or host address, limiting the
number of devices to 256. There are 21 bits for the network address.

Finally, Class D networks are used for multicasting purposes, when a general
broadcast to more than one device is required. The lengths of each section of
the IP address have been carefully chosen to provide maximum ﬂexibility in
assigning both network and local addresses.

[image]

There is another class of address, Class E, that is as yet unused.

DHCP — Dynamic Host Control Protocol

Because each machine on a network must have a unique IP address to ensure that
data is sent to the correct destination, network administrators used to have an
arduous task of maintaining large networks. Microsoft developed a protocol to
address this specific issue and to make IP assignment an automatic process. DHCP
is their solution.

DHCP consists of three components: a client, a server, and the protocol that
they use to communicate with each other. Most TCP/IP implementations these days
have DHCP integrated into the networking client, even if the operating system
doesn't speciﬁcally refer to it as such. On a Microsoft Windows 98 system, for
example, when you select the “Obtain An IP Address Automatically” radio button
in the TCP/IP Properties dialog box, you are actually activating the DHCP
client. The DHCP server is an application that runs on a computer and exists to
service requests from DHCP clients.

The core function of DHCP is to assign IP addresses. This is the most
complicated part of the service, because an IP address must be unique for each
client computer. The DHCP standard deﬁnes three types of IP address allocation,
which are as follows:

>   Manual allocation An administrator assigns a specific IP address to a
>   computer in the DHCP server and the server provides that address to the
>   computer when it is requested.

>   Automatic allocation The DHCP server supplies clients with IP addresses
>   taken from a common pool of addresses, and the clients retain the assigned
>   addresses permanently.

>   Dynamic allocation The DHCP server supplies IP addresses to clients from a
>   pool on a leased basis. The client must periodically renew the lease or the
>   address returns to the pool for reallocation.

SLIP and PPP

SLIP (Serial Line Internet Protocol) and PPP (Point to Point Protocol) are data
link layer protocols. They are part of the TCP/IP protocol suite and are not
designed to connect machines on a LAN. Despite being data link layer protocols
they are in fact end-to-end protocols. They are designed to connect one system
to another using a dedicated connection, such as a telephone line. Because the
medium is not shared there is no need for a MAC mechanism, and because there are
only two systems involved there is no need to address the packets.

SLIP is so simple it hardly deserves to be called a protocol. It is designed to
transmit signals over a serial connection (which in most cases means a modem and
a telephone line) and has very low control overhead, meaning that it doesn't add
much information to the network layer data that it is transmitting. Compared to
the 18 bytes that Ethernet adds to every packet, for example, SLIP adds only 1
byte. Of course, with only 1 byte, SLIP can't provide functions like error
detection, network layer protocol identiﬁcation, security, or anything else.
Because of its limitations it is rarely used today, having largely been replaced
by PPP.

PPP is, in most cases, the protocol you use when you access the Internet by
establishing a dial-up connection to an Internet Service Provider (ISP). PPP is
more complex than SLIP and is designed to provide a number of services that SLIP
lacks. These include the ability of the systems to exchange IP addresses, carry
data generated by multiple network layer protocols (which is called
multiplexing), and support different authentication protocols. Still, PPP does
all this using only a 5-byte header, larger than SLIP but still less than half
the size of the Ethernet frame.

Establishing a PPP connection

In order to reduce the amount of data transmitted the PPP frame does not contain
all of the information necessary to fulﬁl all of the functions described above.
Instead the PPP protocol involves a lengthy handshaking procedure at the
beginning of the connection. Because the packet is smaller the connection is
more efﬁcient. Whilst it is necessary in Ethernet for every packet to contain
the address of the target machine, in a PPP connection this is not necessary as
there are only two machines involved.

The connection is established as follows:

>   1. At the beginning the link is dead until one of the two systems runs a
>   program, such as one that causes a modem to dial.

>   2. Link Establishment —Once the physical layer connection is established,
>   one system generates a PPP frame containing a Link Control Protocol (LCP)
>   Request message. The systems use the LCP to negotiate the parameters they
>   will use during the rest of the PPP session. The message contains a list of
>   options, such as the use of a specific authentication protocol, header
>   compression, network layer protocols, and so on. The receiving system can
>   then acknowledge the use of these options or deny them and propose a list of
>   its own. Eventually, the two systems agree on a list of options they have in
>   common.

>   3. Authentication —If the two systems have agreed to the use of a particular
>   authentication protocol during the link establishment phase, they then
>   exchange PPP frames containing messages particular to that protocol in the
>   Data ﬁeld. Systems commonly use the Password Authentication Protocol (PAP)
>   or the Challenge Handshake Authentication Protocol (CHAP), but there are
>   other authentication protocols as well.

>   4. Link Quality Monitoring —If the two systems have negotiated the use of a
>   link quality monitoring protocol during the link establishment phase, the
>   exchange of messages for that protocol occurs here.

>   5. Network Layer Protocol Configuration —For each of the network layer
>   protocols that the systems have agreed to use, a separate exchange of
>   Network Control Protocol (NCP) messages occurs at this point.

>   6. Link Open —Once the NCP negotiations are complete, the PPP connection is
>   fully established, and the exchange of packets containing network layer
>   application data can commence.

>   7. Link Termination— When the two systems have ﬁnished communicating, they
>   sever the PPP connection by exchanging LCP termination messages, after which
>   the systems return to the Link Dead state.

Summary — A datagram’s life

You have now looked at how TCP constructs a datagram, and how IP is used to
route it to the destination machine — be it on an Ethernet network, or over the
Internet. Let’s examine how a datagram passes along the Internet in a bit more
detail.

When an application must send a datagram out on the network, it performs a few
simple steps. First, it constructs the IP datagram within the legal lengths
stipulated by the local IP implementation. The checksum is calculated for the
data, and then the IP header is constructed. Next, the ﬁrst hop (machine) of the
route to the destination must be determined to route the datagram to the
destination machine directly over the local network, or to a gateway if the
internetwork is used. If routing is important, this information is added to the
header using an option. Finally, the datagram is passed to the network for its
manipulation of the datagram.

As a datagram passes along the internetwork, each gateway stops the packet and
removes the outer header. The IP protocol on the network layer on the gateway’s
network interface calculates the checksum and veriﬁes the integrity of the
datagram. If the checksums don't match, the datagram is discarded and an error
message is returned to the sending device. Provided it does, next the TTL ﬁeld
is decremented and checked. If the datagram has expired, it is discarded and an
error message is sent back to the sending machine. After determining the next
hop of the route, either by analysis of the target address or from a speciﬁed
routing instruction within the Options field of the IP header, the datagram is
rebuilt with the new TTL value and new checksum.

If fragmentation is necessary because of an increase in the datagram length or a
limitation in the software, the datagram is divided, and new datagrams with the
correct header information are assembled. If a routing or timestamp is required,
it is added as well. Finally, the datagram is passed back to the network layer.

When the datagram is finally received at the destination device, the system
performs a checksum calculation and—assuming the two sums match—checks to see if
there are other fragments. If more datagrams are required to reassemble the
entire message, the system waits, meanwhile running a timer to ensure that the
datagrams arrive within a reasonable time. If all the parts of the larger
message have arrived but the device can't reassemble them before the timer
reaches 0, the datagram is discarded and an error message is returned to the
sender. Finally, the IP header is stripped off, the original message is
reconstructed if it was fragmented, and the message is passed up the layers to
the upper layer application. If a reply was required, it is then generated and
sent back to the sending device.

It’s a miracle it works at all!

What is NAT and how does it work?

NAT stands for Network Address Translation.

If I were to publish this article on my website and you were to view from your
work machine then you would be using NAT right now!

When the Internet was ﬁrst “developed", everyone thought that there were plenty
of addresses to cover any need. Theoretically, you could have 4,294,967,296
unique addresses (2”). The actual number of available addresses is smaller
(somewhere between 3.2 and 3.3 billion) because of the way that the addresses
are separated into classes, and because some addresses are set aside for
multicasting, testing or other special uses.

With the explosion of the Internet and the increase in business networks, the
number of available IP addresses is simply not enough. The obvious solution is
to redesign the address format to allow for more possible addresses. This is
being developed (called IPv6), but will take several years to implement because
it requires modiﬁcation of the entire infrastructure of the Internet.

Network Address Translation allows a single machine to act as a gateway between
a private network and the public Internet. Therefore only one IP address needs
to be visible to the Internet and can represent a whole network of other
machines.

A Local Area Network uses IP addresses internally. Most of the network trafﬁc in
a LAN is local, so it doesn't travel outside the internal network. A LAN can
include both registered (global) and unregistered (local) IP addresses. Of
course, any computers that use unregistered IP addresses must use Network
Address Translation to communicate with the rest of the world.

Here is an example of how NAT works.

>   An internal network (stub domain) has been set up with IP addresses that
>   were not speciﬁcally allocated to that company by IANA (Internet Assigned
>   Numbers Authority), the global authority that hands out IP addresses. These
>   addresses should be considered non-routable since they are not unique.

>   The company sets up a NAT-enabled router. The router has a range of unique
>   IP addresses given to the company by IANA.

>   A computer on the stub domain attempts to connect to a computer outside the
>   network, such as a Web server.

>   The router receives the packet from the computer on the stub domain.

>   The router saves the computer's non-routable IP address to an address
>   translation table. The router replaces the sending computer's non-routable
>   IP address with the first available IP address out of the range of unique IP
>   addresses. The translation table new has a mapping of the computer's
>   non-routable IP address matched with the one of the unique IP addresses.

>   When a packet comes back from the destination computer, the router checks
>   the destination address on the packet. It then looks in the address
>   translation table to see which computer on the stub domain the packet
>   belongs to. It changes the destination address to the one saved in the
>   address translation table and sends it to that computer. If it doesn't find
>   a match in the table, it drops the packet.

>   The computer receives the packet from the router. The process repeats as
>   long as the computer is communicating with the external system.

What are ports and sockets?

A port is the means by which an application accesses the top layer of the TCP
stack. Each application has its own individual port number. This ensures that
data from one application does not get confused with data from another.

At the end of this document is a list of common port numbers and what
application is associated with them.

As with the network and data link layers, one of the important functions of the
transport layer protocol is to identify which application the data was created
by and is destined for. TCP does this by specifying the port number assigned to
the process that created the data. All applications are assigned a port number
by the lANA (Internet Assigned Numbers Authority). When a TCP/IP packet arrives
at its destination, the transport layer protocol receiving the IP datagram reads
the value in the Destination Port ﬁeld and delivers the information in the Data
ﬁeld to the program or protocol associated with that port.

When one TCP/IP system addresses traffic to another, it uses a combination of an
IP address and a port number. The combination of an IP address and a port is
called a socket. To specify a socket in a Uniform Resource Locator (URL), you
enter the IP address first and then follow it with a colon and then the port
number. So then the socket 192.168.2.10:21, for example, addresses port 21 on
the system with the address 192.168.2.10. Since 21 is the port number for FTP,
this socket addresses the FTP server running on that computer.

You usually don't have to specify the port number when you're typing a URL,
because the program you use assumes that you want to connect to the well-known
port. Your Web browser, for example, addresses all the URLs you enter to port
80, the Hypertext Transfer Protocol (HTTP) Web sewer port, unless you specify
otherwise. The lANA port numbers are recommendations, not ironclad rules,
however. You can conﬁgure a Web server to use a port number other than 80, and
in fact, many Web servers assign alternate ports to their administrative
controls, so that only users who know the correct port number can access them.
You can create a semi-secret Website of your own by conﬁguring your server to
use port 81 (for example) instead of 80. Users would then have to type a URL
like http://wwwmyserver.com:81 into their browsers instead of just http://www.
myserver. com.

Appendix A
==========

TOP enables the use of many different applications on a TCP/IP network. Below is
a list of some of them.

Telnet

The Telnet program provides a remote login capability. This lets a user on one
machine log onto another machine and act as though he or she were directly in
front of the second machine. The connection can be anywhere on the local network
or on another network anywhere in the world, as long as the user has permission
to log onto the remote system.

File Transfer Protocol

File Transfer Protocol (FTP) enables a ﬁle on one system to be copied to another
system. The user doesn't actually log in as a full user to the machine he or she
wants to access, as with Telnet, but instead uses the FTP program to enable
access. Again, the correct permissions are necessary to provide access to the
ﬁles.

Once the connection to a remote machine has been established, FTP enables you to
copy one or more files to your machine. (The term transfer implies that the ﬁle
is moved from one system to another but the original is not affected. Files are
copied.) FTP is a widely used service on the Internet, as well as on many large
LANs and WANs.

Simple Mail Transfer Protocol

Simple Mail Transfer Protocol (SMTP) is used for transferring electronic mail.
SMTP is completely transparent to the user. Behind the scenes, SMTP connects to
remote machines and transfers mail messages much like FTP transfers ﬁles. Users
are almost never aware of SMTP working, and few system administrators have to
bother with it. SMTP is a mostly trouble-free protocol and is in very wide use.

Kerberos

Kerberos is a widely supported security protocol. Kerberos uses a special
application called an authentication server to validate passwords and encryption
schemes. Kerberos is one of the more secure encryption systems used in
communications and is quite common in UNIX.

Domain Name System

Domain Name System (DNS) enables a computer with a common name to be converted
to a special network address. For example, a PC called Darkstar cannot be
accessed by another machine on the same network (or any other connected network)
unless some method of checking the local machine name and replacing the name
with the machine's hardware address is available. DNS provides a conversion from
the common local name to the unique physical address of the device's network
connection.

Simple Network Management Protocol

Simple Network Management Protocol (SNMP) provides status messages and problem
reports across a network to an administrator. SNMP uses User Datagram Protocol
(UDP) as a transport mechanism. SNMP employs slightly different terms from
TCP/IP, working with managers and agents instead of clients and servers
(although they mean essentially the same thing). An agent provides information
about a device, whereas a manager communicates across a network with agents.

Network File System

Network File System (NFS) is a set of protocols developed by Sun Microsystems to
enable multiple machines to access each other's directories transparently. They
accomplish this by using a distributed ﬁle system scheme. NFS systems are common
in large corporate environments, especially those that use UNIX workstations.

Remote Procedure Call

The Remote Procedure Call (RPC) protocol is a set of functions that enable an
application to communicate with another machine (the server). It provides for
programming functions, return codes, and predeﬁned variables to support
distributed computing.

Trivial File Transfer Protocol

Trivial File Transfer Protocol (TFTP) is a very simple, unsophisticated ﬁle
transfer protocol that lacks security. it uses UDP as a transport. TFTP performs
the same task as FTP, but uses a different transport protocol.

Transmission Control Protocol

Transmission Control Protocol (the TCP part of TCP/IP) is a communications
protocol that provides reliable transfer of data. It is responsible for
assembling data passed from higher-layer applications into standard packets and
ensuring that the data is transferred correctly.

User Datagram Protocol

User Datagram Protocol (UDP) is a connectionless—oriented protocol, meaning that
it does not provide for the retransmission of datagrams (unlike TCP, which is
connection-oriented). UDP is not very reliable, but it does have specialized
purposes. If the applications that use UDP have reliability checking built into
them, the shortcomings of UDP are overcome.

Internet Protocol

Internet Protocol (IP) is responsible for moving the packets of data assembled
by either TCP or UDP across networks. It uses a set of unique addresses for
every device on the network to determine routing and destinations.

Internet Control Message Protocol

Internet Control Message Protocol (ICMP) is responsible for checking and
generating messages on the status of devices on a network. It can be used to
inform other devices of a failure in one particular machine. ICMP and IP usually
work together.

Appendix B - Frequently used TCP port numbers
=============================================

[table]

[table]

Appendix C — RFCs
=================

The TCP/IP standards are published in documents called Requests for Comments
(RFCs) by the Internet Engineering Task Force (IETF).

The list of RFCs contains documents that define protocol standards in various
stages of development, but also contains informational, experimental, and
historical documents that range from the fascinating to the downright silly.
These documents are in the public domain and are accessible from many Internet
Web and FTP sites. For links to the standards, see the IETF home page at [link]

Appendix D — Subnetting
=======================

Subnetting allows a single IP address (regardless of Class) to be divided into
smaller pieces. This method of dividing addresses was introduced to help
overcome some of the problems outlined above.

if the administrator of a corporate network wanted to create a new network at
their site, they would have to request a new IP address for that network if it
was to be accessible from public networks. Due to the finite number of addresses
available this was not always possible. Subnetting adds another level of
hierarchy to the IP addressing structure.

Instead of the classful two-level hierarchy (network number and host number),
subnetting supports a three-level hierarchy:

[image]

Subnetting leaves the network prefix intact, but divides the host number into
subnetwork prefix and host number. This allows a single network number to
subdivided into smaller networks. Because the network preﬁx is the same, the
route from the Internet to the host is the same, but once inside the network,
routers will be used to determine which subnetwork the information is destined
for. As far as the Internet is concerned all subnets belong to the same address,
therefore only one entry in the routing table is required.

This practice allows local administrators to introduce varying levels of
complexity to their private network without increasing the size of the
Internet’s global routing table.

Design considerations

The deployment of an addressing plan requires careful thought on the part of the
network administrator. There are four key questions that must be asked before
any design should be undertaken:

>   1. How many total subnets does the organisation need today?

>   2. How many total subnets will the organisation need in the future?

>   3. How many hosts are there on the organisation’s largest subnet today?

>   4. How many hosts will there be on the organisation's largest subnet in the
>   future?

The ﬁrst step in the planning process is to take the maximum number of subnets
required and round up to the nearest power of two. For example if a network
requires 9 subnets, 23 (=8) will not be enough subnet addressing space, so 2‘1
(=16) will be required.

The second step is to ensure that there are enough host addresses for the
organisation’s largest subnet. If the largest subnet needs to support 50 host
addresses, 25 (=32) will not provide enough addresses, so 26 (=64) will be
required.

These calculations determine in what way the original IP address is divided to
provide subnets. As with normal IP addresses, subnet addresses can vary in the
amount of bits that identify the subnetwork number and number of bits that
identify the host number.

As examined above, conventional IP addresses indicate what class of address they
are using at the beginning on the address. Similarly subnet addresses indicate
wht addressing scheme they are using (ie, where the divider between subnet
number and host number lies). This is known as the subnet mask.

An example

Let’s assume that an organisation has been assigned the network number 193.1.1.0
(Class C) by the Internet’s governing body. The organisation decides it wishes
to deﬁne 6 subnets. The largest subnet will need to support 25 hosts.

The ﬁrst step is to determine the number of bits required to define the six
subnets. Since a network address can only be subnetted along binary boundaries,
subnets must be created in blocks of powers of two (2,4,8,16 etc). therefore it
is not possible to create an IP address that deﬁnes 6 subnets (as 6 is not a
power of 2). In this case the administrator must create 8 subnets and not use 2
of them (or have 2 reserved for future growth).

Now here’s where it gets complicated!

8 = 23. Therefore three bits will be required to enumerate the eight subnets.

The organisation has a class C address assigned to it, which means 24 bits are
assigned to the network number. The new “extended" network preﬁx (network number
and subnet number) will therefore be 27 bits long. The subnet mask will
therefore be 255.255.255.224 to indicate that this is the addressing scheme
used.

Don’t worry if that is as clear as mud! It makes more sense when you look at it
in binary notation.

The organisation has been assigned the IP address 193.110. In binary notation
this would be written:

11000001 .00000001 .00000001 .00000000

This is a class C address, which means the ﬁrst three numbers deﬁne the network
address, and the last number the host address on that network.

To create 8 subnets we are going to “borrow" three bits from the host address.
To identify that this is what we are doing we create a subnet mask:

11111111.1111111.11111111.11100000

This deﬁnes the three hits of the host address that are being used to create an
extended network preﬁx — now we only have 5 bits left to identify the host
machine, but this is enough to identify up to 32 machines (25), so is enough for
our purposes. In dotted notation this subnet mask is 255.255.255.224

With me so far? We’re not finished yet!

The 8 subnets will be numbered 0 through 7:

00000000

00000001

00000010

00000011

00000100

00000101

00000110

00000111

To identify the subnet, the administrator places the three-bit identiﬁer (the
last three bits in the bytes shown above) in the three-bit subnet identiﬁer we
have borrowed from the host number as shown above.

Therefore the eight subnets would be identified as follows:

Subnet \#0 11000001 .00000001 .00000001 .00000000 193.1.1.0

Subnet \#1 11000001.00000001.00000001.00100000 193.1132

Subnet \#2 11000001 .00000001 .00000001 .01000000 19311.64

Subnet \#3 11000001.00000001.00000001.01100000 19311.96

Subnet \#4 11000001 .00000001 .00000001 .10000000 193.1.1.128

Subnet \#5 11000001 .00000001 .00000001 .10100000 193.1 .1 .160

Subnet \#6 11000001.00000001.00000001.11000000 193.1.1.192

Subnet \#7 11000001 .00000001 .00000001.1 1 100000 193.1.1224

The remaining 5 bits can be used to identify individual host addresses — up to
32 in total.

Subnet conventions

It is not possible to use subnet masks that are either all 0s or all 1s as this
can cause routing errors. Not all routers on the Internet have the ability to
examine subnet information. Therefore an IP address of 193.1.1.0 would be
indistinguishable from the same IP address using a subnet mask of
255.255.255.224:

[image]

These IP addresses are normally reserved for internal use anyway for
broadcasting. If you want to send a message to all machines on the network you
send it to 255.255.255.255 or 000.000.000.000 depending on how the router has
been conﬁgured.

TCP rate adaption and error correction

This guide builds and develops on the areas and concepts covered in my
Introduction to TCP/IP, and looks in more detail at the handshaking process that
occurs between two modems when they agree on a mutually agreeable connection
state.

A brief recap

You will remember that TCP is a transport layer protocol and provides an
end-to-end communications channel between processes on each host system. The
channel is reliable (has error-checking and correction capability), full-duplex
and streaming. To achieve this functionality, TCP breaks up the data being
passed to it from the session layer into separate segments and attaches a number
of headers to the data containing information for the target system. An IP
header is attached to this TCP “packet" containing addressing details of the
target machine. This packet (or datagram or frame) is passed to the network
layer for delivery.

>   TCP is said to be a unicast protocol as it supports the exchange of data
>   between two parties only — it is not a multicast protocol.

>   TCP is a connection-oriented protocol in that it establishes a connection
>   between the two parties in order to exchange data. It then ensures the
>   reliable transfer of data by the acknowledging of all received packets. The
>   protocol detects when segments of the data stream have been discarded,
>   reordered, duplicated or corrupted. Where necessary it will retransmit
>   damaged segments to the receiver. This implies that the sender must keep a
>   local copy of all transmitted data until it has been successfully received.
>   It is therefore also said to be reliable.

>   TCP is a full-duplex protocol in that it allows both parties to send and
>   receive data within a single TCP connection.

TCP is a streaming protocol. Streaming implies that for every “write" action by
the sender, there is a corresponding "read” action by the receiver. During the
establishment of the connection, the sender and receiver agree on the Maximum
Transmission Unit (MTU) — the maximum size of segment both parties can
comfortably handle. This value is regular updated by both parties during the
connection. Data is sent in a “stream" of packets.

TCP is also a rate adaptive protocol in that the rate of data transfer adapts to
the network load conditions. There is no pre-deﬁned TCP data transfer rate; if
the network and the receiver increase in capacity after the connection has
already been established, the sender will attempt to inject more data into the
network to occupy this space. Similarly, if there is a sudden congestion, the
sender will reduce the amount of data being put into the network to allow it to
recover. This function aims to achieve the maximum amount of data transfer
without incurring data loss.

The TCP Protocol Header

In the Introduction to TCP/IP I looked in some detail at the IP header used by
the Internet Protocol. TCP also uses its own header structure — it is the values
in this header which provide the error checking and correction capabilities of
the protocol.

The TCP header structure uses a pair of 16-bit source and destination Port
addresses. The next ﬁeld is a 32-bit sequence number, which identiﬁes the
sequence number of the segment within the whole steam. The sequence number does
not necessarily always start with a value of 1, the initial value is intended to
prevent delayed data from an old connection inadvertently being incorrectly
incorporated in a new connection.

After the sequence number is the Acknowledgement sequence number which is used
to inform the originating end of the connection that data has been successfully
received. A segment which is travelling to the sender to acknowledge receipt of
a previously transmitted segment is known as an ACK.

Following the acknowledgement sequence number, comes the Data Offset field. This
ﬁeld contains six “ﬂags” which contain information on the type of packet which
is being sent. These ﬂags are single bits and have a value of 1 or 0 to indicate
on or off status.

>   the URG ﬂag is used to indicate whether the packet is urgent or not

>   the ACK flag is used to indicate whether the packet is an ACK or not

>   the PSH ﬂag is used if the sending system wants to “push” data to the remote
>   application

>   the RST flag is used to reset the connection

>   the SYN ﬂag (for synchronise) is used during the establishment of the
>   connection

>   the FIN ﬂag (for ﬁnish) is used to close the connection

following the data offset ﬁeld comes the Window ﬁeld which can contain an
optional amount of redundant data, up to 16 bits) to ensure that the preceding
ﬁelds all result in a header of the same size.

You then have the TCP Checksum which is used by the receiving machine to ensure
that the received data has no errors.

Following the checksum are a number of TCP Options. As their name indicates
these are optional settings, including:

>   Maximum receive segment size (MTU) — this option is used when the connection
>   is opened. It is intended to inform the remote machine of the maximum
>   segment size that the sender is willing to receive. This option is only used
>   in the initial SYN packet (the initial packet exchange that opens a TCP
>   connection).

>   Window scale — this option is used to address the issue of latency (the
>   delay between transmission and reception of data across the network). This
>   option sets the amount of user data the packet holds. In conditions of high
>   delay, fewer packets containing more data may be preferable to a lot of
>   packets containing a little data. The maximum amount of data the packet can
>   hold must fall within the MTU value.

>   SACK-permitted — SACK stands for Selective Acknowledgment. Provided both
>   parties support SACK this option can be enabled to reduce the amount of data
>   sent over a congested network. Instead of acknowledging all received
>   packets, the receiver only asks for the retransmission of packets it doesn’t
>   receive (based on the segment numbers) or packets which are corrupted.

Connection establishment

The ﬁrst phase of a TCP session is the establishment of the connection. This
requires a three way handshake, ensuring that both sides of the connection have
an unambiguous understanding of the parameters to be used in the connection. The
operation can be described as follows:

1. The local system sends the remote system an initial sequence number using a
SYN packet

2. The remote system responds with an ACK of the initial sequence number and the
initial sequence number of the remote system response in a SYN packet

3. The local system responds with an ACK of this remote sequence number

The connection is then opened. In other words the local machine tells the other
machine what sequence number its stream will begin with. The remote machine
acknowledges receipt of this information, and tells the local machine what
sequence number its acknowledgment packets will begin with. The local machine
acknowledges receipt of this information.

Volume transfer

Once the connection has been established, TCP then endeavours to establish the
point of dynamic equilibrium between maximum network efﬁciency and packet loss.
This is why when you start to download a file over an Internet connection the
download rate always starts off quickly, then steadily gets slower until it
settles on a rate which may fluctuate a little. This equilibrium is very
important. Increasing data transfer leads to network congestion which leads to
data loss. Lost packets will require retransmission of data which will further
increase congestion thus leading to a declining spiral. The speed of the
connection will be that which guarantees the reliable delivery of data.

During the connection establishment, the receiving machine will advertise to the
sender the size of its receiving buffer — this is the amount of data the
receiving machine can store while data is being processed. The sending machine
will ensure that the amount of data in transit over the network at any point
does not exceed this value, ensuring that no data is being sent across the
network that may be automatically discarded.

The sending machine must also keep in its send buffer a copy of all sent data
until it has been successfully received by the target machine, in case it needs
to be retransmitted. This buffer cannot be exceeded either. The size of the send
and receive buffers on both machines are used to calculate what is called a
“sliding window" to determine the amount of data that can be placed on the
network at any time. This is known as the congestion window (cwnd). The size of
the congestion window is the optimum amount of data that can be put on the
network at any time. The point at which packet loss occurs as the congestion
window grows too large is known as the threshold (the ssthresh value). In TCP
parlance, therefore, the value of ssthresh should always be greater than the
cwnd value. In the event of packet loss, both parties will lower the data
throughput rate to the agreed minimum supported by both parties, then gradually
increase it again until packet loss occurs again. This process continues until
the session is closed. This is known as slow start. In TCP parlance, the size of
the cwnd value is reduced, then gradually increased (one segment per RTT [round
trip time] interval) until it exceeds the ssthresh value.

Packet loss

TCP can detect packet loss in two ways. First, if a single packet is lost within
a sequence of packets, the receiving machine will generate duplicate ACKs for
each packet that arrives after that packet, letting the sender know that it
needs to resend the lost packet. Secondly, if a packet is lost at the end of the
stream, there are no subsequent packets for the receiver to acknowledge with
duplicate ACKs. But if the sender does not receive any ACK for that packet once
its timer has expired it will assume packet loss and will retransmit that
packet.

A single duplicate ACK is not a reliable indicator of packet loss. If a single
packet has been routed slightly differently to the rest of the stream with a
slightly higher delay it may arrive out of sequence. When the receiver receives
the packet, it will acknowledge with an ACK to let the sender know it has
received the packet. Three duplicate ACKs in succession will trigger the sender
to resend the lost packet.

Another signal of packet loss is a complete cessation of ACK packets to the
sender. The sender will not wait indeﬁnitely for an ACK packet, after a
predeﬁned delay it will assume packet loss automatically and resend the
packet(s). this delay is governed by the Retransmission timer. This timer is
constantly refreshed to take into account differences in network load. If the
retransmission timer triggers too early it will send duplicate information into
the network unnecessarily. If it triggers too slowly it will slow down the flow
of data.

Assisting TCP performance

Although I stated above that TCP is an end-to-end protocol which ensures the
transmission of packets from host to target, it is possible for network elements
in between to assist in optimising performance.

[image]

One function the network can employ is RED, or Random Early Detection. RED
permits a network router to discard packets even if there is space on the
network. While this may sound inefficient there is a good reason why this may be
desirable.

RED uses a weighted average queue length to predict the likelihood of packet
loss. In other words, as the queue length increases (the time it takes for the
router to receive an ACK from the router it has forwarded a packet to), the
likelihood of packet loss increases.

At this stage it is probably worth reminding you that packet loss is ultimately
due to the TTL value (Time To Live) in the IP header. To avoid a situation where
packets circle the network forever, all packets are set with a limit on the
number of routers they can pass through before the packet is discarded
automatically. As the packet passes through a router, the router decreases the
TTL value in the IP header by 1. When a router receives a packet with a TTL
value of 1 in the IP header it discards it automatically. If this concept seems
unfamiliar read the Introduction to TCP/IP for more information.

RED aims to avoid a situation where all TCP streams experience congestion at the
same time. A small ﬂow of packets will be allowed to pass through, but a large
stream of packets may be subjected to control if the network is already under
heavy load. RED also aims to avoid a situation where there is a total loss of
ACK packets reaching the sender resulting a connection termination, so it may
favourably allow an ACK packet to reach it destination at the expense of another
packet of “less” importance.

RED is a useful performance enhancement. Although TCP is an end-to-end protocol
in that it uses error checking and packet loss detection measures to ensure the
reliable transmission of data, it makes no assumptions at all about the status
of the network. TCP uses packet loss as an indicator that it should slow data
throughput and begin again gradually incrementing throughput until packet loss
occurs again. Whilst all of the above sounds ﬁendishly clever, it is important
to note that there is no real intelligence as such — a limit is reached, it
slows down, and begins again and thus the process repeats itself.

It is important to note that RED will not necessarily discard the randomly
selected packet. It is RED’s function to signal the sender that there is a
potential for queue exhaustion. RED signals this likelihood of congestion by
adding a Congestion Experienced (CE) ﬂag in the Type of Service (ToS) ﬁeld of
the 1P header.

Conclusion

TCP is not a predictive protocol. It is an adaptive protocol that attempts to
operate the network at the point of greatest efﬁciency.

Part 2
======

Encryption, Certificates and Authentication
===========================================

Cryptography is a cornerstone of the modern electronic security technologies
used today to protect valuable information — whether it is corporate information
held on intranets, or whether it is credit card information being transferred
across the Internet. The term cryptography is derived from the Greek kryptés
meaning hidden, and graphein meaning to write.

Cryptography enables two users (the sender and the receiver) to convert
information (or plaintext) into a cipher (scrambled text) by means of a key, The
information is unreadable to anyone who is not in possession of this key. The
information is then said to be encrypted. The sender of the information must
share this key in a secure manner so only those who are authorised to view the
information can do so.

Encryption not only guarantees privacy, but it also proves that the information
originated from a known source - provided that the key has not been compromised.
This guarantee of authenticity is important in today‘s reliance on secure data
transfer. Anyone who gains access to the key can not only decrypt the cipher
into plaintext and read the information, but could also use the key to
impersonate the originator.

Systems of cryptography also include techniques and mechanisms for verifying
that the originators of encrypted messages are authentic.

This guide will look at the reasons why encryption is used, the different
technologies behind encryption and authentication, and will also look in some
detail at the use of digital certiﬁcates in both of these processes.

It is important to note that no system is infallible. What cryptography seeks to
do is not to provide perfect or risk-free security, but rather seeks to make the
unauthorised access to data so complicated, so costly that the cost incurred in
breaking security is more than the potential value that might be gained from the
information accessed.

Security

Cryptography seeks to provide four basic functions:

Confidentiality — assurance that only authorised users can read or use
conﬁdential information, and that to anyone who does manage to eavesdrop on the
transmission medium, the information will be unreadable.

Authentication — veriﬁcation of the identity of the entities that communicate
over the network.

Integrity — veriﬁcation that the original contents of information have not been
altered or corrupted, either deliberately or accidentally.

Nonrepudiation — assurance that a party involved in communication cannot falsely
deny that a part of the communication actually occurred.

Mathematics to the rescue

To make information secure, the plaintext is converted to ciphertext by applying
an encryption algorithm. The strength of the encryption depends on the strength
of the algorithm.

An encryption algorithm is basically a very difﬁcult mathematical problem — a
problem so difficult that it is even difﬁcult for a computer to resolve.

What does it mean for a mathematical problem to be difﬁcult? To answer that let
me ﬁrst explain what is meant by an algorithm.

An algorithm explains the steps to take to resolve a mathematical problem. A
simple algorithm which everyone knows is addition. Take the values a and b,
apply the addition algorithm to give the output a+b

A problem is “difficult” if the fastest algorithm to solve the problem takes a
long time relative to the input size.

Factoring 15 to give 3x5 is a simple problem. The problem is more difﬁcult if
the numbers involved are signiﬁcantly larger but the algorithm remains the same.
Addition and factorisation are example of polynomial algorithms — the time taken
to resolve the problem is proportional to the size of the numbers involved.

Modern encryption methods rely on exponential algorithms — as the size of the
numbers involved increases, the time taken to resolve the problem increases
disproportionately.

To encrypt information, its value is adjusted by a pre-deﬁned amount by applying
an algorithm to it. The amount by which the information is adjusted is speciﬁed
by a key. To decrypt the encrypted information the same key is required.

Types of encryption

There are two main categories into which an encryption system can fall:

>   Symmetric

>   Asymmetric (aka public-key)

Symmetric encryption involves using the same key for both encryption and
decryption. Symmetric keys are also known as shared secret keys as the same key
is shared by the sender and the receiver, but it is kept secret from any other
parties. The diagram below illustrates a basic symmetric key encryption:

[image]

Symmetric encryption is used by TLS (Transport Layer Security, aka SSL) and
IPSec. This method of encryption has drawbacks. Since the same key is used for
both encryption and decryption, if anyone were to steal the key they would be
able to decipher encoded messages.

Asymmetric, or public-key encryption uses different keys for the encryption and
decryption of information. This requires the use of a public key (which is
available to other entities on the network) and a private key (which is known
only to the owner). The public key is used to encrypt information, but only the
corresponding private key can be used to decrypt the information.

To use an analogy think of a strongbox with a double-acting deadbolt on it. The
deadbolt can only be locked with one key. Once locked it cannot be unlocked with
the same key. To unlock the box you require a second key. This key can only
unlock the box, it cannot lock it. Neither key can be recreated from the other.

You take the locking key and copy it 1000 times and freely distribute it. If
anyone wants to put anything in the strongbox they can lock it with their key.
But you are the only one who can unlock it. The only drawback is that you cannot
be sure who put the contents in the box as 1000 people have the same key.
Security has been gained at the price of veriﬁcation.

Asymmetric encryption is used by VPN (Virtual Private Network) solutions.

Public key encryption is considerably slower than symmetric key encryption due
to the higher computational overhead — the processor has to do much more work to
decrypt the information.

When talking about the efﬁciency of a cryptographic system, there are three
distinct factors to take into account:

Computational overheads — how much computation is required to perform the public
and private key transformations

Key size — how many bits are required to store the key pairs and any system
parameters

Bandwidth — how many bits must be communicated to transfer an encrypted message
or a signature

Secure Key Exchange

In order for symmetric key encryption to work, the secret key must be shared
securely to prevent it falling into the wrong hands. Two of the most common key
exchange algorithms are:

>   Difﬁe-Hellman Key Agreement

>   RSA Key Exchange Process

Both methods make it difficult for an intruder who intercepts network traffic to
guess or calculate the key required to decrypt the information being
transmitted. The Diffie-Hellman Agreement is largely accepted to be the more
secure of the two.

Diffie-Hellman Key Agreement

This form of key exchange actually uses public-key encryption technology. Public
key encryption was ﬁrst proposed in 1975 by Stanford University researchers
Whitﬁeld Difﬁe and Martin Hellman.

The basic process is shown below:

[image]

To preserve security, the key itself is never transmitted. The client only needs
to prove that it has the correct key. The procedure is as follows:

1. the client indicates to the server that it wants to connect.

2. the sewer then sends a random number (the challenge) to the client.

3. the client then performs a computation using its key and the random number,
and sends the result (the response) back to the server.

4. the server performs the same computation using the same random number and its
copy of the key.

5. if the keys match, the result of the computation will match, and the client
will be authenticated and accepted.

Message Digests

In order to ensure that the decrypted information you receive has not been
altered en route or become encrypted, message digest functions (aka hash
functions) are used. These verify the integrity of the data. They are commonly
128 to 160 bits in length and provide a digital identiﬁer for each ﬁle or
document.

Message digest functions are mathematical functions that process information to
generate a different value for each unique document. Identical documents have
the same digest, but if even one bit in the document is changed the digest
changes.

[image]

The two most common forms of message digest in use today are MD5 and SHA. SHA is
considered the stronger of the two as it uses a longer key length — the longer
the key length the higher the degree of security.

Digital Signatures

Just as a handwritten signature is used to identify an individual for legal
proceedings or ﬁnancial transactions, so digital signatures are used to identify
electronic entities - to prove that the source from which the data is coming is
really that which it claims to be, to provide authentication.

One possible method for creating a digital signature is to encrypt all data with
the originator's private key. But this is impractical for several reasons:

>   Ciphertext is considerably larger than plaintext and places excessive
>   demands on network resources.

>   Public key encryption is slow and places a high computational load on
>   processors. Transmitting large amounts of Ciphertext can be used by hackers
>   to break the encryption.

The most common form of digital signature is to sign the message digest function
with the originator’s private key. This produces only a small amount of
Ciphertext, does not require large amounts of data to be transmitted over the
network and does not burden the processor.

Who do you trust?

By themselves, private and public keys cannot provide proof that they belong to
an alleged owner. There has to be a way to verify the identity of the owner of a
key set. On a private network this is relatively easy. But for open networks
such as the Internet trust relationships need to be veriﬁed by a third party who
can provide assurance that key sets do correspond with certain entities.

This role is performed by Certification Authorities (CAs). A CA (such as
VeriSign) issues digital certiﬁcates. These certiﬁcates are used to sign public
and private encryption keys to prove that their identity is genuine.

If the CA is trusted, then any key that has been signed with a certiﬁcate issued
by that CA is also trusted.

Digital certiﬁcates function similarly to identiﬁcation cards such as passports:

>   It contains personal information to help trace the owner

>   It contains the information that is required to identify and contact the
>   issuing authority

>   It is designed to be tamper resistant and difficult to counterfeit

>   It is issued by an authority that can revoke the certificate at any time

>   It can be checked for revocation by contacting the issuing authority

The current industry standard for digital certiﬁcates is X.509 version 3. The
contents of a certiﬁcate is described in Appendix B.

Certificate servers

For use on an internal network you can use a certiﬁcate server and generate
digital certiﬁcates internally. Windows 2000 server has a certiﬁcate server
built in.

It is relatively easy to configure Internet Explorer to trust certiﬁcates issued
by your own server. Open Internet Explorer. From the Tools menu select Internet
Options.

Click on the Content tab

[image]

Click on the Certificates button

[image]

Click one of the following tabbed categories for the type of certiﬁcates you
want to install or remove:

-   Personal. Certiﬁcates in the Personal category have an associated private
    key. Information signed by using personal certiﬁcates is identified by the
    user's private key data. By default, internet Explorer places all
    certiﬁcates that will identify the user (with a private key) in the Personal
    category.

-   Other People. Certiﬁcates in the Other People category use public key
    cryptography to authenticate identity. based on a matching private key that
    is used to sign the information. By default, this category includes all
    certiﬁcates that are not in the Personal category (the user does not have a
    private key) and are not from CAs.

-   Intermediate Certification Authorities. This category contains all
    certiﬁcates for CAs that are not root certiﬁcates.

-   Trusted Root Certification Authorities. This category includes only
    self-signed certiﬁcates in the root store. When a CA's root certiﬁcate is
    listed in this category, you are trusting content from sites, people, and
    publishers with credentials issued by the CA.

-   Trusted Publishers. This category contains only certiﬁcates from trusted
    publishers whose content can be downloaded without user intervention, unless
    downloading active content is disabled in the settings for a speciﬁc
    security zone, Downloading active content is not enabled by default. For
    each available security zone, users can choose an appropriate set of ActiveX
    security preferences.

Depending on the version of SSL or TSL that the certiﬁcate was created using,
you may need to configure Explorer’s security settings:

In the Internet Options window, click on the Advanced tab. Scroll down to the
Security section and ensure that the necessary protocols are enabled:

[image]

Cryptanalysis attacks

Cryptanalysis is the practice of examining encrypted information to ascertain
the key to decrypt it

— to break the code.

Symmetric key encryption is subject to key search attacks (also called brute
force attacks). In these attacks, the attacker tries each possible key until the
right key is found to decrypt the message. Most attacks are successful before
all possible keys are tried.

In general, you can minimize the risk of key search attacks by choosing shorter
key lifetimes and longer key lengths. A shorter key lifetime means that each key
encrypts less information, which reduces the potential damage if one of the keys
is compromised.

Longer key lengths decrease the possibility of successful attacks by increasing
the number of combinations that are possible. For example, for a 40-bit key,
there are 24° possible values. By using a personal computer that can try 1
million keys per second, an attacker can try all possible keys in about 13 days.
However, an 128-bit key has 2‘25 possible values. If you could use a computer
that would allow you to try 100 billion keys 3 second and you used 10 million of
these computers, it would take about 1013 years to try every possible 128-bit
key value.

Given a key of the same length, public key cryptography generally is more
susceptible to attack than symmetric key cryptography, particularly to factoring
attacks. In a factoring attack, the attacker tries all of the combinations of
numbers that can be used with the algorithm to decrypt ciphertext. Factoring
attacks are similar to keysearch attacks, but the number of possible factors
varies with each algorithm and with the length of the public key and private key
that are used. In general, for a given key length, a factoring attack on a
public key requires fewer attempts to be successful than a key search attack on
a symmetric key.

Neither of these two types of attack are used commonly. The most common form of
attack is a plaintext attack. Hackers can collect amounts of ciphertext and
compare it with plaintext to try to determine the encryption key in use.
Standard email headers are sent as plaintext, for example.

Key Lifetimes

Key length is only one factor in the strength of both symmetric and asymmetric
systems. The longer a key is used the more susceptible to attack it is. Keys
should be changed at regular intervals and should always be changed immediately
if there is any suspicion that the key has been compromised.

Appendix A - Authentication protocols

PAP (Password Authentication Protocol)

A clear—text authentication scheme used in Point-to-Point Protocol (PPP)
connections Password Authentication Protocol (PAP) is not a secure form of
authentication because the user’s credentials are passed over the link in
unencrypted form. For this reason, Challenge Handshake Authentication Protocol
(CHAP) or some other authentication protocol is preferable if the remote client
supports it. If the password of a remote client using PAP has been compromised,
the authentication server can be attacked using replay attacks or remote client
impersonation.

How It Works

PAP uses a two-way handshake to perform authentication. Once the PPP link is
established using the Link Control Protocol (LCP), the PPP client sends a
username and password to the PPP server. The server uses its own authentication
scheme and user database to authenticate the user, and if the authentication is
successful, the server sends an acknowledgment to the client.

PAP is typically used only if the remote access server and the remote client
cannot negotiate any higher form of authentication. The remote client initiates
the PAP session when it attempts to connect to the PPP server or router. PAP
merely identiﬁes the client to the PPP server; the server then authenticates the
client based on whatever authentication scheme and user database are implemented
on the server.

EAP (Extensible Authentication Protocol)

The Extensible Authentication Protocol (EAP) is an extension to the
Point-to-Point Protocol (PPP). EAP was developed in response to an increasing
demand for remote access user authentication that uses other security devices.
EAP provides a standard mechanism for support of additional authentication
methods within PPP. By using EAP, support for a number of authentication schemes
may be added, including token cards, one-time passwords, public key
authentication using smart cards, certiﬁcates and others. EAP, in conjunction
with strong EAP authentication methods, is a critical technology component for
secure virtual private network (VPN) connections because it offers more security
against brute-force or dictionary attacks and password guessing than other
authentication methods, such as CHAP.

To find out if EAP authentication methods are being used, see your system
administrator.

CHAP (Challenge Handshake Authentication Protocol)

An encrypted authentication scheme in which the unencrypted password is not
transmitted over the network. CHAP is more secure than PAP because CHAP encrypts
the transmitted password, while PAP does not. SPAP and MS-CHAP are
vendor-speciﬁc implementations of the CHAP standard.

How It Works

A typical CHAP session during the PPP authentication process works something
like this:

>   1. A client connects to a network access server (NAS) and requests
>   authentication.

>   2. The sewer challenges the client by sending a session ID and an arbitrary
>   string.

>   3. The client uses the M05 one-way hashing algorithm and sends the sewer the
>   username, along with an encrypted form of the server’s challenge, session
>   ID, and client password.

>   4. A session is established between the client and the server.

To guard against replay attacks, the challenge string is chosen arbitrarily for
each authentication attempt. To protect against remote client impersonation,
CHAP sends repeated, random interval challenges to the client to maintain the
session.

MS CHAP (Microsoft’s implementation of CHAP)

MS CHAP negotiates a secure form of encrypted authentication by using Message
Digest 5 (MD5), an industry-standard hashing scheme. CHAP uses
challenge-response with one-way MD5 hashing on the response. In this way, you
can prove to the server that you know the password without actually sending the
password over the network.

By supporting CHAP and MD5, Windows 2000 is able to securely connect to almost
all other PPP servers. When you connect to other remote access servers or
clients, Windows 2000 remote access may negotiate plaintext authentication if
the other product does not support encrypted authentication.

Where possible, MS-CHAP is consistent with standard CHAP. lts response packet is
in a format speciﬁcally designed for Windows NT and Windows 2000, and Windows 95
and later, networking products. In addition, MS-CHAP does not require the use of
plaintext or reversibly encrypted passwords.

A system administrator can define authentication retry and password changing
rules for the users connecting to your server. A version of MS-CHAP is available
speciﬁcally for connecting to a server running Windows 95. You must use this
version if your connection is to a server running Windows 95.

Note

MS-CHAP v2 is a mutual authentication protocol, which means that both the client
and the server prove their identities. If your connection is configured to use
MS-CHAP v2 as its only authentication method, and the server that you are
connecting to does not provide proof of its identity, your connection
disconnects. Previously, servers could skip authentication and simply accept the
call. This change ensures that you can configure a your connection can be
configured to connect to the expected server.

MS CHAP 2

A new version of the Microsoft Challenge Handshake Authentication Protocol
(MS-CHAP v2) is available. This new protocol provides mutual authentication,
stronger initial data encryption keys, and different encryption keys for sending
and receiving. To minimize the risk of password compromise during MS—CHAP
exchanges, MS—CHAP v2 drops support for the MS—CHAP password change and does not
transmit the encoded password.

For VPN connections, Windows 2000 Server offers MS-CHAP v2 before offering the
legacy MS-CHAP. Windows 2000 dial-up and VPN connections can use MS-CHAP v2.
Windows NT 4.0 and Windows 98 computers can use only MS-CHAP v2 authentication
for VPN connections.

Kerberos

A method of securely authenticating users' requests for access to services on a
network. It was developed by the Massachusetts Institute of Technology (MIT),
which based it on the Data Encryption Standard (DES).

Kerberos is the primary security protocol for Microsoft Windows 2000 domains and
is used by domain controllers to verify the identity of the user and the
integrity of data during a session. Kerberos has also been implemented on
several UNIX platforms including OpenBSD.

How It Works

Kerberos uses a ticket-based method for granting a user access to a network
service. When a Kerberos-enabled client wants to request a network service (such
as network logon) from a Kerberos-enabled server, the client must ﬁrst contact
an authentication server (A8) to receive a ticket and an encryption key. The
encryption key, called the session key, is used to unlock communication between
the client and the server and thereby authenticate that communication. The
initial ticket, often called the ticket-granting ticket (TGT), contains a copy
of the session key and an identity, which is a randomly generated number. The AS
passes the TGT and the identity back to the client, which stores the ticket in
its ticket cache. When the client wants to access a particular service, it sends
the ticket to a ticket-granting server (TGS). (The TGS and AS are usually the
same machine.) The TGS gives the client a ticket that securely identiﬁes the
client to the service it is requesting. Finally, the client presents the ticket
to the network service it is trying to access and is granted access to the
resource as many times as desired until the ticket expires. When the client
sends a ticket, the ticket is always accompanied by an authenticator message
that is encrypted with the session key. This authenticator includes a time
stamp, which is used to ensure that the ticket is legitimate.

In the Windows 2000 implementation of Kerberos. each domain controller has the
Kerberos v5 services running on it, and a Kerberos client is built into each
server and workstation running Windows 2000. The Kerberos services maintain
encrypted user passwords and identities in Active Directory. When a user logs on
to a domain controller, the initial Kerberos authentication enables the user to
access available resources anywhere in the enterprise because authentication
credentials issued by the Kerberos services of one domain are accepted by all
domains within a domain tree or a domain forest.

The Kerberos service issues an initial ticket for the logon domain when a user
logs on to a Windows 2000 workstation. Any server running Windows 2000 can then
validate the client’s ticket without having to contact the domain Kerberos
service. It can do this because servers running Windows 2000 share the
encryption key that the Kerberos service uses to encrypt tickets. This
encryption key is called the server key.

Appendix B — Description of Digital Certificates

[table]

Introduction to IP Version 6
============================

Due to concerns over the depletion of the existing pool of IP addresses and the
desire to provide additional functionality for modern devices, an upgrade of the
current version of the Internet Protocol (IPv4) is in the process of
standardisation — IP Next Generation (an9) or IP Version 6 (IPv6).

This document will explain the problems of IPv4 and how they are addressed in
IPv6. I will also look at the new IPv6 header and the various signalling
protocols used by routers to handle IPv6 traffic. ‘

Limitations of IPv4

>   IPv4 addresses have become relatively scarce, forcing some organisations to
>   use NAT (Network Address Translation) to map multiple private addresses to a
>   single public IP address. While NAT promotes the use of private address
>   space it can interfere with certain higher layer protocols such as IPSec.

>   The increase in the number of Internet-connected devices ensures that the
>   public IPv4 address space will eventually be depleted.

>   The growth in the size of the Internet has led to a corresponding increase
>   in the size of Internet routing tables. There are routinely 85,000 routes in
>   the routing tables of backbone Internet routers. As the size of routing
>   tables increases so performance is decreased and journey times increase.

>   IP conﬁguration either has to be done manually or with automatic
>   conﬁguration protocols such as DHCP (Dynamic Host Conﬁguration Protocol). As
>   more and more devices connect to the Internet conﬁguration needs to be
>   simpler.

>   The current implementation of IP has no support for security.

>   While IPv4 headers contain ToS (Type of Service) settings to prioritise data
>   and ensure the delivery of data in real time, this has limited
>   functionality.

Features of IPv6

New header format

The IPv6 header has a new format that is designed to keep overhead to a minimum.
This is achieved by moving non-essential and optional ﬁelds to extension headers
that are placed after the IPv6 header. This means headers are more efﬁciently
processed by routers. IPv4 and IPv6 headers are not interoperable. A host or
router must use an implementation of both IPv4 and IPv6 in order to recognise
and process both header formats. IPv6 addresses are four times as large as IPv4
addresses. but the header is only twice as large.

Large address space

IPv6 has 128-bit source and destination IP addresses. 128 bits can express 3.4 x
1035 possible combinations. This large address space allows for multiple levels
of subnetting and means that individual subnets within an organisation can be
assigned an IP address from the Internet authority, thus techniques such as NAT
will no longer be necessary.

Efficient and hierarchical addressing and routing infrastructure

IPv6 addresses are designed to create an efficient and hierarchical routing
infrastructure meaning that Internet backbone routers have smaller routing
tables.

Stateless and stateful address configuration

To simplify host conﬁguration, IPv6 supports both stateful conﬁguration (in the
presence of a DHCP server) and stateless configuration (in the absence of a DHCP
server). Stateless hosts configure themselves for the duration of the
conﬁguration using network preﬁxes advertised by the router. Even in the absence
of a router hosts can conﬁgure themselves automatically without manual
conﬁguration.

Built-in security

IPv6 supports optional packet encryption using iPSec.

Better support for 008

New ﬁelds in the IPv6 header deﬁne how trafﬁc is handled and identiﬁed. Trafﬁc
identiﬁcation using a Flow Label ﬁeld in the header allows router to identify
and provide special handling for packets belonging to a ﬂow (a series of packets
between a source and a destination). Because this information is contained in
the header, QoS settings can be used even if the payload of the packet is
encrypted.

New protocol for neighbouring node interaction

The Neighbour Discovery protocol is a new ICMP (Internet Control Message
Protocol) message that manages the interaction of neighbouring nodes on the same
link. Neighbour Discovery replaces the broadcast-based Address Resolution
Protocol (ARP).

Extensibility

IPv6 can be easily extended for new features by adding extension headers after
the IPv6 header. Unlike options in the IPv4 header which is limited to 40 bytes,
the size of the IPv6 extension headers is only constrained by the size of the
IPv6 packet.

IPv6 Addressing

The most obvious distinguishing feature of IPv6 is its use of much larger
addresses. The size of an address is 128 bits - four times larger than the IPv4
address. A 32-bit address space allows for 232 or 4,294,967,296 possible
addresses. A 128-bit address space allows for 2\^28 addresses
(340,282,366,920,938,463,463,374,607,431,768,211,456). It is hard to imagine a
scenario where this many addresses would not be enough!

Syntax

IPv4 addresses are represented in dotted-decimal format. This 32-bit address is
divided along 8-bit boundaries. Each set of 8 bits is converted to its decimal
equivalent and separated by dots. For IPv6, the 128-bit address is divided along
16-bit boundaries, and each 16-bit block is converted to a 4-digit hexadecimal
number and separated by colons. The resulting representation is called
colon-hexadecimal:

21DA:D3:O:2F3B:2AA:FF:FE28:905A

Some types of addresses contain long sequences of zeros. To further simplify the
representation of IPv6 addresses, a 16-bit sequence containing all zeros is
omitted.

IPv6 addresses are separated into network preﬁx and host number in the same way
as IPv4 addresses.

There are three types of IPv6 address:

Unicast - a unicast address identiﬁes a single interface on the network

Multicast — a multicast address identiﬁes multiple interfaces on the network

Anycast — an anycast address identiﬁes all of the interfaces on the immediate
network (immediacy being determined by routing distance)

Aggre gatable Global Unicast Addresses

As the name implies, Aggregatable global unicast addresses are designed to be
summarised to produce an efﬁcient routing hierarchy. A router running IPv6 only
needs to look at the ﬁrst few bits of the target address to know where to send
the packet. This reduces the amount of processing done by the router, reduces
delay and therefore overall data throughput.

The structure of an aggregatable unicast address is as follows:

[image]

TLA ID

The TOP-Level Aggregation Identiﬁer is a 13-bit ﬁeld. This identiﬁes the highest
level in the routing hierarchy. These IDs are administered by the Internet
authority and allocated to local lSPs. This ﬁeld size allows for up to 8,192 TLA
le.

Res

Bits that are reserved for future use I expanding the size of either the TLA ID
or the NLA ID.

NLA ID

The Next-Level Aggregation Identifier is used to identify a speciﬁc customer
site. The ﬁeld is 24- bits. Together the TLA ID and the NLA ID form the public
part of the IP address.

SLA ID

The Site-Level Aggregation Identiﬁer is used by an individual organisation to
identify subnets within its site. The field is 16-bits, allowing for the
creation of up to 65,536 subnets or multiple levels of addressing hierarchy.

Interface ID

The Interface ID identiﬁes the interface (network card) on a specific subnet.

IPv6 Header

The IPv6 header is a ﬁxed size of 40 bytes:

[image]

Before we look at the changes in header structure in IPv6, let’s look brieﬂy at
the current header structure used in IPv4

[image]

Version Indicates the version of IP used

Internet Header Length Indicates the total length of the header. The minimum
size of the IPv4 header is 20 bytes. Options can extend this by a number of
additional 4-byte blocks, with padding being used if necessary to fill unused
space.

Type of Service Indicates the desired service expected by this packet for
delivery through routers across the network. This ﬁeld is 8—bits and contains
parameters for precedence, delay, throughput and reliability.

Total Length Indicates the total length of the whole packet (header and
payload). The size of this ﬁeld is 16-bits, which can indicate a maximum of
65,535 bytes.

Identification Identiﬁes the speciﬁc packet in a stream of packets. If a packet
is fragmented during transmission, all of the fragments retain the original
identiﬁcation value so that they can be grouped for reassembly.

Flags Identiﬁes flags for the fragmentation process. The size of this ﬁeld is 3
bits, however, only 2 bits are deﬁned for use. There are two ﬂags — one to
indicate whether the packet might be fragmented and another to indicate whether
more fragments follow the current fragment.

Fragment Offset Indicates the position of the fragment relative to the original
packet. ‘

Time to Live Indicates the maximum number of links on which a packet can travel
before being discarded. As a packet passes through a router, the TTL value is
decreased by one. When a router receives a packet with a TTL value of 1 it is
discarded and an ICMP Time

Expired message is sent back to the sending node.

Protocol Identiﬁes the upper layer protocol. The size of field is 8 bits. For
example, TCP uses a protocol of 6, UDP 17 and ICMP 1.

Header Checksum Provides a checksum on the IP header. The size of this ﬁeld is
16 bits. The payload is not included in the checksum. Each node that receives
the packet veriﬁes checksum integrity and discards the packet if veriﬁcation
fails. Because the router must decrease the TTL value, the header checksum is
recalculated with each hop.

Source Address Stores the IP address of the originating host. Destination
Address Stores the IP address of the destination host. Options Stores one or
more lP options. The size of this ﬁeld is a multiple of 32 bits. If the IP
option or options do not use all 32 bits, padding is used so that the IP header
is a number of 4-byte blocks that can be indicated by the Internet Header Length
field.

The IPv6 Header:

[image]

Version 4 bits are used to indicate the version of IP used (set to 6)

Traffic Class Indicates the class or priority of the packet. This ﬁeld provides
similar functionality to the va4 Type of Service ﬁeld.

Flow Label Indicates that this packet belongs to a speciﬁc sequence of packets
between a source and destination, requiring special handling by intermediate
routers. The Flow Label is used for non-default quality of service connections,
suCh as those needed by real—time data (voice and video). For default router
handling, the Flow Label is set to O.

Payload Length Indicates the length of the total packet. The size of this field
is 16 bits providing a maximum of 65,535 bytes. For payload lengths greater than
65,535 bytes the Payload Length field is set to 0 and the Jumbo Payload option
is used in the Extensions header.

Next Header Indicates either the first extension header (if present) or the
protocol in the upper layer (such as TCP, UDP or lCMP). The size of the field is
8 bits. When indicating an upper layer protocol, the same values as IPv4 are
used.

Hop Limit Indicates the maximum number of links over which the packet can travel
before being discarded. The Hop Limit is similar to the IPv4 TTL ﬁeld.

Source Address Stores the va6 address of the originating host.

Destination Address Stores the va6 address of the destination host. If a routing
extension header is present, the Destination Address might be set to the next
router interface.

Extension Headers

ln IPv4 the IP header contains all of the options. As a packet passes through a
router, all of these options must be processed, causing performance degradation.
With IPv6 all options have been moved to extension headers. The only extension
header that must be processed by the router is the Hop-by—Hop Options extension
header. This increases processing speed and forwarding performance.

Currently these are the extension headers supported by va6:

>   Hop-by-Hop Options

>   Destination Options

>   Routing header

>   Fragment header

>   Authentication header

>   Encapsulating Security Payload (ESP) header

Extension headers are processed in the order in which they are present,
therefore there are rules to the order in which they can be placed. Because the
Hop-by-Hop Options extension header must be processed, it must be placed first.

Extension headers must fall on a 64-bit (8 byte) boundary Extension headers of
variable length must use padding as required to ensure this.

The Next Header ﬁeld in the IP header indicates whether an extension is present
or not. Each extension header also has its own Next Header value to indicate
whether another extension is present:

[image]

Hop-by-Hop Options header

The Hop-by-Hop Options header consists of a Next Header field, a Header
Extension Length ﬁeld, and an Options field that contains one or more options.

The options contain information on how the packet should be handled by the
router: The Jumbo Payload option is used to indicate a payload greater than
65,535 bytes. The option is 32-bits, therefore with the Jumbo Payload option,
payload sizes of up to 4,294,967,295 bytes can be indicated. An IPv6 packet with
a payload greater than 65,535 bytes is known as a Jumbogram.

The Router Alert option is used to indicate to the router that the contents of
the packet require additional processing.

Destination Options header

The Destination Options header is used to specify packet delivery parameters for
either intermediate destinations or the ﬁnal destination. It is used in two
ways:

If a Routing header is present, it specifies delivery or processing options at
each intermediate destination It speciﬁes delivery or processing options at the
ﬁnal destination

Routing header

Much like va4, va6 source nodes can use the Routing header to specify loose
routing. Alternatively speciﬁc routing information with the addresses of each
intermediate host can be included. As the packet arrives at each intermediate
host on the route, the Routing header is processed and the address of the next
intermediate host becomes the destination address in the IP header.

Fragment header

The Fragment header is used for fragmentation and reassembly services. In va6,
only source nodes can fragment payloads. If the payload submitted by the upper
layer protocol is larger than the link MTU (Maximum Transmission Unit), the IP
fragments the payload at the source and includes reassembly information. When an
va6 packet is fragmented, it is initially divided into unfragmentable and
fragmentable parts:

The unfragmentable part of the original IPv6 packet must be processed by each
intermediate node between source and destination. This part therefore contains
the IP header, Hop-by-Hop Options header, Destination Options header and the
Routing header.

The fragmentable part of the original IPv6 packet must only be processed by the
final destination. This part consists of the Authentication header, the ESP
header, the Destination Options header for the final destination and the upper
layer PDU.

The fragmentation process is illustrated below:

[image]

Authentication header

(see Jimbo’s Guide to Encryption, Authentication and Digital Certiﬁcates for
more information on this subject)

The Authentication header provides data authentication (veriﬁcation of the node
that sent the packet), data integrity (veriﬁcation that the data was not
modified in transit — either intentionally or unintentionally) and anti-relay
protection (assurance that captured packets cannot be retransmitted and accepted
as valid). The Authentication header (AH) is part of the IPv6 security
architecture and is already used by VPN protocols such as IPSec.

The AH does not provide data confidentiality by encrypting the contents of the
payload. To do this ‘ it can be used in conjunction with the Encapsulating
Security Payload (ESP). See Jimbo's Guide to Virtual Private Networking for more
information on this subject.

Encapsulating Security Payload header and trailer

The ESP header and trailer provide data confidentiality, data authentication and
data integrity services. The ESP header contains the Security Parameters index
(SPI) ﬁeld that identiﬁes the iPSec session information, as well as a Sequence
Number field that provides anti-relay protection.

Details on how both the AH and the ESP work are beyond the scope of this
document.

Signalling

IPv6 uses a revised version of the Internet Control Messaging Protocol (ICMPv6).
There are two types of ICMP message:

>   Error messages

>   Informational messages

Error messages are used to report errors in the forwarding or delivery of lP
packets by either the destination node or an intermediate node. Error messages
include Destination Unreachable, Packet Too Big, Time exceeded and Parameter
Problem.

Informational messages are used to provide diagnostic functions and additional
host functionality.

Destination Unreachable messages

Destination Unreachable messages can have a value between 0 and 4:

>   0 No route matching the destination was found in the table

>   1 The communication with the destination is prohibited by administrative
>   policy. (This

>   is typically sent when the packet is blocked by a ﬁrewall)

>   2 The address is beyond the scope of the source address

>   3 The destination address is unreachable. This is typically sent because of
>   an

>   inability to resolve the destination’s link layer address

>   4 The destination port was unreachable. This is typically sent when a packet
>   containing a UDP message arrived at the destination but there were no
>   applications listening on the destination UDP port

Packet Too Big error message

An ICMP Packet Too Big message is sent when the packet cannot be forwarded
because the link MTU is smaller than the size of the packet. IPv6 requires that
the link layer support a minimum packet size of 1280 bytes. Link layers that do
not support this must provide a link layer fragmentation and reassembly scheme
that is transparent to IP.

Time Exceeded error message

An ICMP Time Exceeded message is sent by a router when the Hop Limit field in
the IP header is zero.

Parameter Problem error message

An ICMP Parameter Problem message is sent either by a router or the destination
host. This occurs when an error is encountered in either the IP header or an
extension header, preventing IP from performing further processing.

Informational messages provide diagnostic capabilities to aid troubleshooting.

Echo Request

An ICMP Echo Request message is sent to a destination to solicit an immediate
Echo Reply message. The Echo Request/ Echo Reply message facility provides a
simple diagnostics function to aid in the troubleshooting of a variety of
reachability and routing problems.

Path MTU Discovery

The path MTU (Maximum Transmission Unit) is the lowest common denominator in any
IP link. Packets with a maximum size of the path MTU do not need to be
fragmented and will be successfully forwarded by all routers on the path. To
discover the path MTU, the sending node uses the receipt of ICMP Packet Too Big
messages.

The path MTU is discovered through the following process:

>   1. The sending node assumes that the path MTU is the link MTU of the
>   interface on which the traffic is being forwarded. (In other words lP knows
>   the MTU of the network segment it is on, and assumes that this is the MTU of
>   the entire path to the destination)

>   2. The sending node sends IP packets at the path MTU size

>   3. If a router on the path is unable to forward the packet over a link with
>   an MTU that is smaller than the size of the packet. it discards the packet
>   and sends an iCMP Packet Too Big message back to the sending node. The lCMP
>   message contains the MTU of the link on which the forwarding failed.

>   4. The sending node sets the path MTU for packets being sent to the
>   destination to the value of the MTU in the lCMP message.

This process may need to be repeated several times.

Multicast Listener Query

MLQ is a service used by routers to establish what interfaces are to be
addressed by multicast messages.

Neighbour Discovery

Neighbour Discovery (ND) is a set of messages used by both hosts and routers. It
is used by hosts to:

Discover neighbouring routers

Discover addresses, address preﬁxes, and other conﬁguration parameters

ND is used by routers to:

>   Advertise their presence, host conﬁguration parameters, and on-link prefixes

>   Inform hosts of a better next-hop address to forward packets for a speciﬁc
>   destination

ND is used by nodes to:

>   Resolve the link layer address of a neighbouring node to which an IP packet
>   is being

>   forwarded and determine when the link layer address of a neighbouring node
>   has

>   Changed

>   Determine whether a neighbour is still reachable

Address Autoconfiguration

One of the most useful aspects of IPv6 is its ability to conﬁgure itself, even
without the use of a

stateful configuration protocol such as DHCP. By default, an IPv6 host can
configure a link address for each interface. By using router discovery, a host
can also determine the address of routers, other conﬁguration parameters,
additional addresses and link prefixes.

The address autoconﬁguration process for the physical interface of an IPv6 node
is as follows:

1.  A tentative link address is derived based on the link preﬁx

2.  Using duplicate address detection to verify the uniqueness of the tentative
    link address,a

>   Neighbour Solicitation message is sent with the Target Address ﬁeld that is
>   set to the tentative link address

1.  If a Neighbour Advertisement message sent in response to the Neighbour
    Solicitation message is received, this indicates that another node on the
    local link is using the tentative link address. At this point the process
    stops and manual configuration must be done on the node.

2.  If no Advertisement message is received, the tentative link address is
    assumed to be unique and valid. The local address is therefore initialised
    for the interface.

The next step of the conﬁguration process then continues:

1.  The host sends up to 3 Router Solicitation messages (by default)

2.  if no router Advertisement messages are received, then the host uses a
    stateful address configuration protocol to obtain addresses and other
    configuration parameters

3.  If a Router Advertisement message is received, the Hop Limit, Reachable
    Time, Retrans Timer and the MTU are set

4.  4. Duplicate address detection is then used to verify the uniqueness of the
    Tentative address

The process is illustrated below:

[image]

Voice over IP (VOIP)
====================

VOIP is the transmission of voice calls over data networks such as the Internet.
This technology has many benefits — imagine being able to make a long—distance
call to the other side of the world for the price of a call to a local ISP. Not
many people know that VOIP is already used by many telephone companies to handle
their long-distance communications between regional offices.

The conventional PSTN (Public Switched Telephone Network) relies on circuit
switching. Connections are established between the two parties, and the
connection remains open until one party closes it by hanging up. A typical
telephone call may happen as follows:

1.  You pick up the receiver and listen for a dial tone. This lets you know that

>   you have a connection to the local office of your telephone carrier.

1.  You dial the number of the party you wish to talk to.

2.  The call is routed through the switch at your local carrier to the party you
    are calling.

3.  A connection is made between your telephone and the other party's line,
    opening the circuit.

4.  You talk for a period of time and then hang up the receiver.

5.  When you hang up, the circuit is closed, freeing your line.

Let's say that you talk for 10 minutes. During this time, the circuit is
continuously open between the two phones. Telephone conversations over the
traditional PSTN are transmitted at a ﬁxed rate of about 64 kilobits per second
(Kbps), or 1,024 bits per second (bps), in each direction, for a total
transmission rate of 128 Kbps. Since there are 8 kilobits (Kb) in a kilobyte
(KB), this translates to a transmission of 16 KB each second the circuit is
open, and 960 KB every minute

it's open. 80 in a 10-minute conversation, the total transmission is 9600 KB,
which is roughly equal to 9.4 megabytes (MB).

Because during a typical telephone conversation only one party is speaking at a
time, half of this transmitted data is wasted. If we could remove this “dead
air", then the transmitted data could be cut down to 4.7 MB

Data networks do not use circuit switched connections. When you retrieve a web
page, the connection with the web server is dropped once the page has been
successfully transferred. Instead, data networks use packet switching.

>   Note — if you are making a dial—up connection to the internet, you still pay
>   for the time you are connected and in this sense the connection is
>   circuit-switched, but the connection is only maintained with the lSP’s
>   portal to the Internet, not with the web servers you may connect to
>   subsequently.

Packet switching is very efficient: it minimises the time that a connection is
maintained between two systems, which reduces the load on the network. It also
frees up the two computers communicating with each other so that they can accept
information from other computers as well.

VOIP uses this technology to provide several advantages over traditional
circuit- switching. Using PSTN, that 10-minute phone call consumed 10 full
minutes of transmission time at a cost of 128 Kbps. With VOIP, that same call
may have occupied only 3.5 minutes of transmission time at a cost of 64 Kbps,
leaving another 64 Kbps free for that 3.5 minutes, plus an additional 128 Kbps
for the remaining 6.5 minutes. Based on this simple estimate, another three or
four calls could easily fit into the space used by a single call under the
conventional system. And this example doesn't even factor in the use of data
compression, which further reduces the size of each call.

At a basic level, IP telephony and conferencing technologies are built on simple
concepts. A personal computer (or other device) is used to capture audio and
video signals from the user (for example, by using a microphone attached to a
sound card, and a video camera attached to a video capture card). This
information is compressed and sent to the intended recipient(s) over the LAN or
the Internet. At the receiving end, the signals are restored to their original
form and played back.

On a more complicated level, using VOIP, a company’s PBX becomes a gateway to
the IP network (the internet, for example). in this system a typical call may
happen as follows:

1.  You pick up the receiver, which sends a signal to the PBX.

2.  The PBX receives the signal and sends a dial tone. This lets you know that
    you have a connection to the PBX.

3.  You dial the number of the party you wish to talk to. This number is then
    temporarily stored by the PBX.

4.  Once you have entered the number, the PBX checks it to ensure that it is in
    a valid format.

5.  The PBX determines whom to map the number to. in mapping, a telephone number
    is mapped to the lP address of another device called the IP host. The IP
    host is typically another digital PBX that is connected directly to the
    phone system of the number you dialled. In some cases, particularly if the
    party you are calling is using a computer-based VOIP client, the lP host is
    the system you wish to connect with.

6.  A session is established between your company's PBX and the other party‘s IP
    host. This means that each system knows to expect packets of data from the
    other system. Each system must use the same protocol to communicate. The
    systems will implement two channels, one for each direction, as part of the
    session.

7.  You talk for a period of time. During the conversation, your company's PBX
    and the other party's IP host transmit packets back and forth when there is
    data to be sent. The PBX at your end keeps the circuit open between itself
    and your phone extension while it forwards packets to and from the IP host
    at the other end.

8.  You finish talking and hang up the receiver.

9.  When you hang up, the circuit is closed between your phone and the PBX,
    freeing your line.

10. The PBX sends a signal to the IP host of the party you called that it is
    terminating the session.

11. The IP host terminates the session at its end, too.

12. 0nce the session is terminated, the PBX removes the number-to-lP-host
    mapping from memory.

In order for telephone systems to be able to communicate with each other a data
network they need to share a common protocol.

VOIP Protocols

VOIP currently uses two main protocols — H232 and SIP. Both protocols are
responsible for three main areas of communication: establishing the call,
handling changes to the call and ending the call. These responsibilities fall
into two categories: signalling and session management. Signalling allows
information to be carried over the network. Session management provides the
ability to control the attributes of a call and also define codecs
(compressor-decompressor) which are used to convert audio signals into a
compressed digital signal for transmission and back again for audio replay.

H.232

The first protocol is H.323. H232 originated as an ITU (International
Telecommunications Union) protocol and is actually a suite of protocols which
provide support for a number of specific applications such as videoconferencing,
data sharing and IP telephony. It was intended as an “all-in-one" solution to
ensure that systems from different vendors worked together. However it is also
relatively slow and cumbersome due to the large amount of redundant data
transmitted. It is also very inflexible in terms of upgrades. In a field as
untested and ever-changing as IP telephony a more ﬂexible protocol is required.

I won’t look at this protocol in any detail, but more information can be found
at

http://www.protocols.com

**Configuring H323 settings on Windows XP**

Open the Control Panel

Double click on Phone and Modem Options

Click on the Advanced tab

[image]

Select **Microsoft H.323 Telephony Service Provider** and click **Configure**

[image]

Here you can specify whether you wish to use a gateway or proxy server and enter
the name of the server. You can also specify what port number to wait or
incoming calls on.

An H.323 gateway connects the IP network with a switched service network, such
as the public switched network (PSTN). A proxy server acts as a firewall or
security barrier between your intranet and the Internet, keeping other people on
the Internet from gaining access to confidential information on your internal
network or your computer. Your telephony system administrator can provide the
correct name or IP address to enter here.

Initiating a VOIP call

You can use the Windows Phone Dialer application to initiate a VOIP call.

You can run Phone Dialer by typing “dialer.exe” in the Run dialogue box on the

Start Menu:

[image]

To initiate the call open the Phone menu and select Dial

[image]

Select the option to make an Internet call and enter the recipient’s IP address:

[image]

If you will be calling this number on a regular basis you can select the option
to add the number to your speed dial list

Alternatively you can save a contact's IP address in the Windows Address Book.

Open the Address Book and create a new contact. Click on the Business tab.

You will see a field for IP Phone:

[image]

Receiving VOIP calls

In order to receive a VOIP call the Phone Dialer application must be running. It
can be minimised to the system tray for convenience.

[image]

Session Initiation Protocol (SIP)

The second protocol, a potential successor to H.323, is the Session Initiation
Protocol (SIP).

Currently VOIP does not offer sufﬁcient voice quality to be seriously considered
as a viable alternative to more conventional telephony solutions, like the PSTN.

The Internet Engineering Task Force (IETF) is committed to making high-quality
voice over IP a reality. The Session Initiation Protocol is at the heart of this
project, and is modelled on other well-established Internet protocols such as
SMTP (Simple Mail Transfer Protocol) and HTTP (Hypertext Transfer Protocol). It
reuses existing technology not only to make it interoperable with existing
protocols such as email, but also to provide easy extensibility in the future.

SIP has a number of advantages over H232. It has the ability to:

>   Determine the location of the target endpoint—SIP supports address
>   resolution, name mapping, and call redirection.

>   Determine the media capabilities of the target endpoint—Via Session
>   Description Protocol (SDP), SIP determines the “lowest level" of common
>   services between the endpoints. Conferences are established using only the
>   media capabilities that can be supported by all endpoints.

>   Determine the availability of the target endpoint—If a call cannot be
>   completed because the target end point is unavailable, SIP determines
>   whether the called party is already on the phone or did not answer in the
>   allotted number of rings. It then returns a message indicating why the
>   target endpoint was unavailable.

>   Establish a session between the originating and target endpoint—If the call
>   can be completed, SIP establishes a session between the endpoints. SIP also
>   supports mid-call changes, such as the addition of another endpoint to the
>   conference or the changing of a media characteristic or codec.

>   Handle the transfer and termination of calls—SIP supports the transfer of
>   calls from one end point to another. During a call transfer, SIP simply
>   establishes a session between the transferee and a new endpoint (speciﬁed by
>   the transferring party) and terminates the session between the transferee
>   and the transferring party. At the end of a call, SIP terminates the
>   sessions between all parties.

SIP provides support for the following call features:

>   Call Hold

>   Consultation Hold

>   Unattended Transfer

>   Call Fwd Unconditional

>   Call Fwd on Busy

>   Call Fwd on No Answer

>   3-Way Conference

>   Single Line Extension

>   Find-Me

>   Incoming Call Screening

>   Outgoing Call Screening

>   Secondary Number — In

>   Secondary Number — Out

>   Do Not Disturb

>   Call Waiting

SIP Protocols

SIP is actually a suite of protocols, much in the same way that TCP is actually
comprised of several protocols all working together.

SIP has two main components: the SIP User Agent and the SIP Network Sewer. The
User Agent can be a lightweight piece of software, suitable for embedding in a
PDA or Smartphone device. The main function of the SIP Server is to provide name
resolution and location information.

The User Agent is effectively the end system component for the call and the SIP
Server is the network device that handles the signalling associated with
multiple calls. The User Agent itself has a client element, the User Agent
Client (UAC) and a server element, the User Agent Server (UAS.) The client
element initiates the calls and the server element answers the calls. This
allows peer-to-peer calls to be made using a client-server protocol.

The SIP Server element also provides for more than one type of server. There are
effectively three forms of server that can exist in the network — the SIP
stateful proxy server, the SIP stateless proxy server and the SIP redirect
server. The main function of the SIP sewers is to provide name resolution and
user location, since the caller is unlikely to know the IP address or host name
of the called party. What will be available is perhaps an email—like address or
a

telephone number associated with the called party. Using this information, the
caller’s user agent can identify with a specific server to "resolve" the address
information. It is likely that this will involve many servers in the network. In
this respect, identity resolution will be similar to the DNS system. Indeed SIP
borrows substantially from SMTP and its routing system.

A SIP proxy server receives requests, determines where to send these, and passes
them onto the next server (using next hop routing principles). There can be many
server hops in the network.

The difference between a stateful and stateless proxy server is that a stateful
proxy server remembers the incoming requests it receives, along with the
responses it sends back and the outgoing requests it sends on. A stateless proxy
server forgets all information once it has sent on a request. This allows a
stateful proxy server to fork requests to try multiple possible user locations
in parallel and only send the best responses back (in other words, if there are
multiple server entries for the same user, it will try all of them and then use
the address which the user is actually using). Stateless proxy servers are most
likely to be the fast, backbone of the SIP infrastructure. Stateful proxy
servers are then most likely to be the local devices close to the User Agents,
controlling domains of users and becoming the prime platform for the application
services.

A redirect server receives requests, but rather than passing these onto the next
server it sends a response to the caller indicating the address for the called
user. This provides the address for the caller to contact the called party at
the next server directly.

Networks can also implement a SIP Registrar. This device accepts registration
requests from users and maintains a Location Server database much like the

HLR in a GSM network.

The Session Description Protocol is responsible for negotiating a mutually—
acceptable call framework between the two parties. It decides the media to use
(codec and sampling rate), the media destination (IP address and port number),
the session name and the time the session is active for.

SIP and SDP are signalling protocols that enable one party to place a call to
another party and to negotiate the parameters of a multimedia session. The
actual audio, video or other multimedia content is exchanged between session
participants using an appropriate transport protocol. In many cases, the
transport protocol to use is the Real-time Transport Protocol (RTP). RTP is used
to packetise various digital media streams, such as voice, text and video.

Real Time Streaming Protocol is responsible for ensuring that packets in real
time media streams arrive consecutively.

How does it work?

SIP is a simple, ASCII-based protocol that uses requests and responses to
establish communication among the various components in the network and to
ultimately establish a conference between two or more endpoints.

A SIP request message is comprised of three elements:

>   Request line

>   Header

>   Message body

A SIP response message also consists of three elements:

>   Status line

>   Header

>   Message body

Users in a SIP network are identified by unique SIP addresses. A SIP address is
similar to an email address and is in the format of sip:<userlD@gateway.com>.
This address is also known as a **Universal Resource Indicator** (URI).

When a user initiates a call, a SIP request is sent to a SIP server (either a
proxy or a redirect server) by the SIP User Agent. The request includes the
address of the caller (in the From header field) and the address of the intended
callee (in the To header ﬁeld).

The SIP server will then try to resolve the address, much in the same way that a
web server or email address is resolved by DNS. The location of the end user
will be registered with the SIP server on the target user’s network. Because
over time a user may move between end systems, the server may have multiple
addresses for the user. If a Stateful Proxy SIP Server is being used, this
server will try each of the addresses in turn until it locates the user, and
return this address. If a Stateless Proxy Server or a Redirect Server is being
used, all of the addresses will be returned in the header of the response
message.

The SIP server then returns this information to the SIP User Agent. The User
Agent then sends an INVITE request to the server, which forwards it to the
callee. The caller and callee then exchange messages indicating their
capabilities in terms of available services, transmission speed and quality of
service\*. The requests and responses involved in these exchanges are forwarded
by the server. Once a mutually-acceptable framework has been agreed upon, a
session is opened between the two parties.

\* SIP does not support quality of service. A minimum required service level can
be speciﬁed and the call will only proceed if this level can be guaranteed.

**Interoperability**

H232 and SIP are not directly compatible with each other, but both protocols can
exist in the same network if a device that supports both protocols is available.
For example, a User Agent could communicate with the Server via H232, and the
server could then communicate with other parties via SIP.

**Usage scenarios**

Voice calls can be made over an IP-based network in a number of ways:

>   From PC to PC

>   From PC to PSTN phone

>   From PSTN phone to PC

>   From IP phone to IP phone

**Alternative uses**

Because SIP is largely responsible for signalling, the payload it carries does
not have to be voice traffic. It can be used to handle a number of applications
such as instant messaging, video conferencing and even network games.

One of SIP‘s advantages is its unique ability to return different media types
within a single session. For example, a customer could call a travel agent, view
video clips of possible holiday destinations, complete an on-Iine booking form
and order currency - all within the same communication session.

It is important to remember that SIP does NOT transport any data. It is a
signalling protocol responsible for maintaining the connection and handling the
addition of other users or services to an existing connection. It can be used to
transport digitised voice information, but it does not handle multimedia data.
In this application, data will be transferred via conventional TCP and may not
follow the same path as a SIP packet. Where the network supports it, RTP will be
used

in preference to TCP.

**Examples of operation**

[image]

The above illustration shows a successful attempt by user Alice to establish a
session with user Bob, whose URI is bgb\@biloxi.cgm. Alice’s UAC is configured
to communicate with a proxy server (the outbound server) in its domain and
begins by sending an INVITE message to the proxy server that indicates it
desires to invite Bob’s UAS into a session (1): the server acknowledges the
request (2). Although Bob’s UAS is identified by its URI, the outbound proxy
server needs to account for the possibility that Bob is not currently available
or that Bob has moved. Accordingly, the outbound proxy server should forward the
INVITE request to the proxy server that is responsible for the domain
biloxi.com. The outbound proxy thus consults a local DNS server to obtain the IP
address of the biloxi.com proxy server (3), by asking for the DNS SRV resource
record that contains information on the proxy server for biloxi.com.

The DNS server responds (4) With the IP address of the biloxi.com proxy server
(the inbound server). Alice’s proxy server can now forward the INVITE message to
the inbound proxy server (5), which acknowledges the message (6). The inbound
proxy server now consults a location server to determine Bob’s location (7), and
the location server responds with Bob's location, indicating the Bob is signed
in, and therefore available for SIP messages (8).

The proxy server can now send the INVITE message on to Bob (9). A ringing
response is sent back from Bon to Alice (10,11,12) while the UAS at Bob is
alerting the local media application (for example, telephony). When the media
application accepts the call, Bob’s UAS sends back an OK response to Alice
(13,14,15).

Finally, Alice’s UAC sends an acknowledgement message to Bob’s UAS to confirm
the reception of the final response (16). In this example, the ACK is sent
directly from Alice to Bob, bypassing the two proxies. This occurs because the
endpoints have learned each other’s address from the INVITE/200 (OK) exchange,
which was not known when the initial INVITE was sent. The media session has now
begun, and Alice and Bob can exchange data over one or more RTP connections.

[image]

The next example (illustrated above) makes use of two message types that are not
yet part of the SIP standard but are documented in RFC 2848 and are likely to be
incorporated in a later revision of SIP. These message types support telephony
applications. Suppose that in the preceding example, Alice was informed that Bob
was not available. Alice’s UAC can then issue a SUBSCRIBE message (1),
indicating that it wants to be informed when Bob is available.

This request is forwarded through the two proxies in our example to a PINT
(PSTN-internet Networking) server (2,3). A PINT server acts as a gateway between
an IP network from which comes a request to place a telephone call and a
telephone network that executes the call by connecting to the destination
telephone. In this example, we assume that the PINT server logic is collocated
with the location service. It could also be the case that Bob is attached to the
Internet rather than a PSTN, in which case the equivalent of PINT logic is
needed to handle SUBSCRIBE requests. In this example, we assume the latter and
assume that the PINT functionality is implemented in the location service. In
any case, the location service authorises subscription by returning an OK
message

(4), which is passed back to Alice (5,6). The location service then immediately
sends a NOTIFY message with Bob’s current status of not signed in (7,8,9), which
Alice’s UAC acknowledges (10,11,12).

[image]

The above illustration continues the example. Bob signs on by sending a REGISTER
message to the proxy in its domain (1). The proxy updates the database at the
location service to reflect registration (2). The update is conﬁrmed by the
proxy (3), which confirms the registration to Bob (4). The PINT functionality
learns of Bob’s new status from the location server (here we assume that they
are collocated) and sends a NOTIFY message containing Bob’s new status (5),
which is forwarded to Alice (6,7). Alice’s UAC acknowledges receipt of the
notification (8,9,10).

SIP Messages

As was mentioned, SIP is a text-based protocol with a syntax similar to that of
HTTP. There are two different types of SIP messages, requests and responses. The
format difference between the two types of messages is seen in the ﬁrst line.
The first line of a request has a method, defining the nature of the request and
a Request URI, indicating where the request should be sent. The ﬁrst line of a
response has a response code. All messages include a header, consisting of a
number of lines, each line beginning with a header label. A message can also
contain a body such as an SDP media description.

For SIP requests, RFC 3261 defines the following methods:

>   REGISTER: used by a user agent to notify a SIP configuration of its current
>   IP address and the URls for which it would like to receive calls

>   INVITE: used to establish a media session between user agents

>   ACK: conﬁrms reliable message exchanges

>   CANCEL: terminates a pending request, but does not undo a completed

>   call

>   BYE: terminates a session between two users in a conference

>   OPTIONS: solicits information about the capabilities of the callee, but does
>   not set up a call

For example, the header of message (1) in figure 2 might look like the
following:

[image]

The first line contains the method name (INVITE), a SIP URI, and the version
number of SIP that is used. The lines that follow are a list of header fields.
This example contains the minimum required set.

The Via headers show the path the request has taken in the SIP configuration
(source and intervening proxies), and are used to route responses back along the
same path. As the INVITE message leaves, there is only one header inserted by
Alice. The line contains the IP address (1226.17.91), port number (5060), and
transport protocol (UDP) that Alice wants Bob to use in his response.

The Max-Forwards header limits the number of hops a request can make on the way
to its destination. It consists of an integer that is decremented by one by each
proxy that forwards the request. If the Max-Forwards value reaches 0 before the
request reaches its destination, it is rejected with a 483 (Too many hops) error
response.

The To header field contains a display name (Bob) and a SIP or SIPS URI
(sip:bob\@biloxi.com) toward which the request was originally directed. The From
header field also contains a display name (Alice) and a SIP or SIPS URI
(sip:alice\@atlanta.com) that indicate the originator of the request. This
header field also has a tag parameter that contains a random string (1928301774)
that was added to the URI by the UAC. It is used to identify the session.

The Call-ID header field contains a globally unique identifier for this call,
generated by the combination of a random string and the hostname or IP address.
The combination of the To tag, From tag and Call-ID completely deﬁnes a
peer-to-peer SIP relationship between Alice and Bob and is referred to as a
dialogue.

The CSeq or Command Sequence header field contains an integer and a method name.
The CSeq number is initialised at the start of a call (314159 in this example),
incremented for each new request within a dialogue, and is a traditional
sequence number. The CSeq is used to distinguish a retransmission from a new
request.

The Contact header field contains a SlP URI for direct communication between
user agents. Whereas the Via header ﬁeld tells other elements where to send the
response, the Contact header field tells other elements where to send future
requests for this dialogue.

The Content-Type header field indicates the type of the message body. The
Content-Length header field gives the length in octets of the message body.

The SIP response types defined in RFC 3261 are in the following categories:

>   Provisional (1xx): the request was received and is being processed

>   Success (2xx): the action was successfully received, understood and accepted

>   Redirection (3xx): further action needs to be taken in order to complete the
>   request

>   Client Error (4xx): the request contains bad syntax or cannot be fulfilled
>   at this server

>   Server Error (5xx): the server failed to fulfil an apparently valid request

>   Global Failure (6xx): the request cannot be fulfilled at any server

For example, the header of message (13) in figure 2 might look like the
following:

[image]

The first line contains the version number of SIP that is used and the response
code and name. The lines that follow are a list of header ﬁelds. The Via, To,
From, Call-ID and CSeq header fields are copied from the INVITE request. (There
are three Via header field values — one added by Alice’s SIP UAC, one added by
the atlanta.com proxy, and one added by the biloxi.com proxy.) Bob's SIP phone
has added a tag parameter to the To header field. This tag is incorporated by
both endpoints into the dialogue and is included in all future requests and
responses in this call.

More information

<http://www.cs.columbia.edu/~hgs/sip>

<http://www.sipcenter.com>

<http://www.sipforum.com>

<http://www.protocols.com>

Further Reading
===============

[Where Wizards Stay Up Late](https://amzn.to/2Fgsun2)

[The Road Ahead](https://amzn.to/2DiAgtT)

[Idea Man](https://amzn.to/2z4ajM0)

[Being Digital](https://amzn.to/2ODaKlb)
